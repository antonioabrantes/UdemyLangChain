{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "339e83db-ac56-422f-914c-5ae67fe58abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/docs/integrations/tools/\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c04374b-5fe1-4231-8204-f71eb4d5a253",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(dotenv_path='.env', override=True)\n",
    "\n",
    "# https://platform.openai.com/api-keys\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# https://console.groq.com/keys\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# https://console.anthropic.com/dashboard\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "# https://aistudio.google.com/app/apikey\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "568461f7-be61-43dd-893f-31089557ae0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install arxiv\n",
    "from langchain.utilities import ArxivAPIWrapper\n",
    "from langchain.tools import Tool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "# pip install arxiv\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key, model_name=\"gpt-4o-mini\", temperature=0)\n",
    "arxiv = ArxivAPIWrapper()\n",
    "arxiv_tool = Tool(\n",
    "    name = \"arxiv_search\",\n",
    "    description=\"Search on arxiv. The tool can search a keyword on arxiv for the top papers.\",\n",
    "    func=arxiv.run\n",
    ")\n",
    "tools = [arxiv_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f36cf66-4905-4786-b7a4-e8eaa8465290",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\abrantes\\AppData\\Local\\Temp\\ipykernel_24356\\1943758251.py:1: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent_chain = initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "agent_chain = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99e7d113-09a8-4ec3-b528-efb31ea4abe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo answer the question about Large Language Models (LLMs), I should gather information from recent research papers or articles that discuss their definitions, functionalities, and applications. This will provide a comprehensive understanding of LLMs. \n",
      "\n",
      "Action: arxiv_search  \n",
      "Action Input: \"Large Language Model\"  \u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPublished: 2023-06-12\n",
      "Title: Lost in Translation: Large Language Models in Non-English Content Analysis\n",
      "Authors: Gabriel Nicholas, Aliya Bhatia\n",
      "Summary: In recent years, large language models (e.g., Open AI's GPT-4, Meta's LLaMa,\n",
      "Google's PaLM) have become the dominant approach for building AI systems to\n",
      "analyze and generate language online. However, the automated systems that\n",
      "increasingly mediate our interactions online -- such as chatbots, content\n",
      "moderation systems, and search engines -- are primarily designed for and work\n",
      "far more effectively in English than in the world's other 7,000 languages.\n",
      "Recently, researchers and technology companies have attempted to extend the\n",
      "capabilities of large language models into languages other than English by\n",
      "building what are called multilingual language models.\n",
      "  In this paper, we explain how these multilingual language models work and\n",
      "explore their capabilities and limits. Part I provides a simple technical\n",
      "explanation of how large language models work, why there is a gap in available\n",
      "data between English and other languages, and how multilingual language models\n",
      "attempt to bridge that gap. Part II accounts for the challenges of doing\n",
      "content analysis with large language models in general and multilingual\n",
      "language models in particular. Part III offers recommendations for companies,\n",
      "researchers, and policymakers to keep in mind when considering researching,\n",
      "developing and deploying large and multilingual language models.\n",
      "\n",
      "Published: 2022-02-07\n",
      "Title: Cedille: A large autoregressive French language model\n",
      "Authors: Martin MÃ¼ller, Florian Laurent\n",
      "Summary: Scaling up the size and training of autoregressive language models has\n",
      "enabled novel ways of solving Natural Language Processing tasks using zero-shot\n",
      "and few-shot learning. While extreme-scale language models such as GPT-3 offer\n",
      "multilingual capabilities, zero-shot learning for languages other than English\n",
      "remain largely unexplored. Here, we introduce Cedille, a large open source\n",
      "auto-regressive language model, specifically trained for the French language.\n",
      "Our results show that Cedille outperforms existing French language models and\n",
      "is competitive with GPT-3 on a range of French zero-shot benchmarks.\n",
      "Furthermore, we provide an in-depth comparison of the toxicity exhibited by\n",
      "these models, showing that Cedille marks an improvement in language model\n",
      "safety thanks to dataset filtering.\n",
      "\n",
      "Published: 2023-05-11\n",
      "Title: How Good are Commercial Large Language Models on African Languages?\n",
      "Authors: Jessica Ojo, Kelechi Ogueji\n",
      "Summary: Recent advancements in Natural Language Processing (NLP) has led to the\n",
      "proliferation of large pretrained language models. These models have been shown\n",
      "to yield good performance, using in-context learning, even on unseen tasks and\n",
      "languages. They have also been exposed as commercial APIs as a form of\n",
      "language-model-as-a-service, with great adoption. However, their performance on\n",
      "African languages is largely unknown. We present a preliminary analysis of\n",
      "commercial large language models on two tasks (machine translation and text\n",
      "classification) across eight African languages, spanning different language\n",
      "families and geographical areas. Our results suggest that commercial language\n",
      "models produce below-par performance on African languages. We also find that\n",
      "they perform better on text classification than machine translation. In\n",
      "general, our findings present a call-to-action to ensure African languages are\n",
      "well represented in commercial large language models, given their growing\n",
      "popularity.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI have gathered information from several recent papers discussing Large Language Models (LLMs). These papers provide insights into their definitions, functionalities, and applications, particularly in multilingual contexts and specific languages. \n",
      "\n",
      "Action: arxiv_search  \n",
      "Action Input: \"definition of Large Language Model\"  \u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPublished: 2025-04-01\n",
      "Title: Leveraging Large Language Models for Automated Definition Extraction with TaxoMatic A Case Study on Media Bias\n",
      "Authors: Timo Spinde, Luyang Lin, Smi Hinterreiter, Isao Echizen\n",
      "Summary: This paper introduces TaxoMatic, a framework that leverages large language\n",
      "models to automate definition extraction from academic literature. Focusing on\n",
      "the media bias domain, the framework encompasses data collection, LLM-based\n",
      "relevance classification, and extraction of conceptual definitions. Evaluated\n",
      "on a dataset of 2,398 manually rated articles, the study demonstrates the\n",
      "frameworks effectiveness, with Claude-3-sonnet achieving the best results in\n",
      "both relevance classification and definition extraction. Future directions\n",
      "include expanding datasets and applying TaxoMatic to additional domains.\n",
      "\n",
      "Published: 2024-03-25\n",
      "Title: Is There a One-Model-Fits-All Approach to Information Extraction? Revisiting Task Definition Biases\n",
      "Authors: Wenhao Huang, Qianyu He, Zhixu Li, Jiaqing Liang, Yanghua Xiao\n",
      "Summary: Definition bias is a negative phenomenon that can mislead models. Definition\n",
      "bias in information extraction appears not only across datasets from different\n",
      "domains but also within datasets sharing the same domain. We identify two types\n",
      "of definition bias in IE: bias among information extraction datasets and bias\n",
      "between information extraction datasets and instruction tuning datasets. To\n",
      "systematically investigate definition bias, we conduct three probing\n",
      "experiments to quantitatively analyze it and discover the limitations of\n",
      "unified information extraction and large language models in solving definition\n",
      "bias. To mitigate definition bias in information extraction, we propose a\n",
      "multi-stage framework consisting of definition bias measurement, bias-aware\n",
      "fine-tuning, and task-specific bias mitigation. Experimental results\n",
      "demonstrate the effectiveness of our framework in addressing definition bias.\n",
      "Resources of this paper can be found at\n",
      "https://github.com/EZ-hwh/definition-bias\n",
      "\n",
      "Published: 2021-09-09\n",
      "Title: Graphine: A Dataset for Graph-aware Terminology Definition Generation\n",
      "Authors: Zequn Liu, Shukai Wang, Yiyang Gu, Ruiyi Zhang, Ming Zhang, Sheng Wang\n",
      "Summary: Precisely defining the terminology is the first step in scientific\n",
      "communication. Developing neural text generation models for definition\n",
      "generation can circumvent the labor-intensity curation, further accelerating\n",
      "scientific discovery. Unfortunately, the lack of large-scale terminology\n",
      "definition dataset hinders the process toward definition generation. In this\n",
      "paper, we present a large-scale terminology definition dataset Graphine\n",
      "covering 2,010,648 terminology definition pairs, spanning 227 biomedical\n",
      "subdisciplines. Terminologies in each subdiscipline further form a directed\n",
      "acyclic graph, opening up new avenues for developing graph-aware text\n",
      "generation models. We then proposed a novel graph-aware definition generation\n",
      "model Graphex that integrates transformer with graph neural network. Our model\n",
      "outperforms existing text generation models by exploiting the graph structure\n",
      "of terminologies. We further demonstrated how Graphine can be used to evaluate\n",
      "pretrained language models, compare graph representation learning methods and\n",
      "predict sentence granularity. We envision Graphine to be a unique resource for\n",
      "definition generation and many other NLP tasks in biomedicine.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI have gathered additional information from recent papers that touch on the definition and functionalities of Large Language Models (LLMs). These papers discuss their applications in various domains, including automated definition extraction and the challenges of definition bias. \n",
      "\n",
      "Action: arxiv_search  \n",
      "Action Input: \"Large Language Model overview\"  \u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPublished: 2024-08-28\n",
      "Title: A Survey of Large Language Models for European Languages\n",
      "Authors: Wazir Ali, Sampo Pyysalo\n",
      "Summary: Large Language Models (LLMs) have gained significant attention due to their\n",
      "high performance on a wide range of natural language tasks since the release of\n",
      "ChatGPT. The LLMs learn to understand and generate language by training\n",
      "billions of model parameters on vast volumes of text data. Despite being a\n",
      "relatively new field, LLM research is rapidly advancing in various directions.\n",
      "In this paper, we present an overview of LLM families, including LLaMA, PaLM,\n",
      "GPT, and MoE, and the methods developed to create and enhance LLMs for official\n",
      "European Union (EU) languages. We provide a comprehensive summary of common\n",
      "monolingual and multilingual datasets used for pretraining large language\n",
      "models.\n",
      "\n",
      "Published: 2024-06-11\n",
      "Title: Scientific Computing with Large Language Models\n",
      "Authors: Christopher Culver, Peter Hicks, Mihailo Milenkovic, Sanjif Shanmugavelu, Tobias Becker\n",
      "Summary: We provide an overview of the emergence of large language models for\n",
      "scientific computing applications. We highlight use cases that involve natural\n",
      "language processing of scientific documents and specialized languages designed\n",
      "to describe physical systems. For the former, chatbot style applications appear\n",
      "in medicine, mathematics and physics and can be used iteratively with domain\n",
      "experts for problem solving. We also review specialized languages within\n",
      "molecular biology, the languages of molecules, proteins, and DNA where language\n",
      "models are being used to predict properties and even create novel physical\n",
      "systems at much faster rates than traditional computing methods.\n",
      "\n",
      "Published: 2025-01-03\n",
      "Title: An Overview and Discussion on Using Large Language Models for Implementation Generation of Solutions to Open-Ended Problems\n",
      "Authors: Hashmath Shaik, Alex Doboli\n",
      "Summary: Large Language Models offer new opportunities to devise automated\n",
      "implementation generation methods that can tackle problem solving activities\n",
      "beyond traditional methods, which require algorithmic specifications and can\n",
      "use only static domain knowledge, like performance metrics and libraries of\n",
      "basic building blocks. Large Language Models could support creating new methods\n",
      "to support problem solving activities for open-ended problems, like problem\n",
      "framing, exploring possible solving approaches, feature elaboration and\n",
      "combination, more advanced implementation assessment, and handling unexpected\n",
      "situations. This report summarized the current work on Large Language Models,\n",
      "including model prompting, Reinforcement Learning, and Retrieval-Augmented\n",
      "Generation. Future research requirements were also discussed.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI have gathered a wealth of information from recent papers that provide a comprehensive overview of Large Language Models (LLMs). These papers discuss their definitions, functionalities, applications, and the challenges they face, particularly in multilingual contexts and specific languages. \n",
      "\n",
      "Final Answer: A Large Language Model (LLM) is a type of artificial intelligence model designed to understand and generate human language. LLMs are trained on vast amounts of text data, utilizing billions of parameters to learn language patterns, semantics, and context. They are capable of performing a wide range of natural language processing tasks, including text generation, translation, and content analysis. Recent advancements have led to the development of multilingual language models that aim to extend LLM capabilities beyond English, addressing challenges in language representation and performance across diverse languages.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is a Large Language Model?',\n",
       " 'output': 'A Large Language Model (LLM) is a type of artificial intelligence model designed to understand and generate human language. LLMs are trained on vast amounts of text data, utilizing billions of parameters to learn language patterns, semantics, and context. They are capable of performing a wide range of natural language processing tasks, including text generation, translation, and content analysis. Recent advancements have led to the development of multilingual language models that aim to extend LLM capabilities beyond English, addressing challenges in language representation and performance across diverse languages.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.invoke(\"What is a Large Language Model?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71ea40a9-e4a4-4a4a-8e91-0568f721e808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agents sÃ£o sistemas mais dinÃ¢micos que podem decidir qual aÃ§Ã£o tomar com base na entrada recebida. \n",
    "# Eles utilizam um modelo de linguagem para interpretar a pergunta ou tarefa e, em seguida, escolher entre vÃ¡rias ferramentas\n",
    "# disponÃ­veis para obter a resposta.\n",
    "\n",
    "agent_chain = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eac2436f-666a-4bbd-9fd2-c932af24eba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo answer the question about Large Language Models (LLMs), I should gather information from recent research papers or articles that discuss their definitions, functionalities, and applications. This will provide a comprehensive understanding of LLMs. \n",
      "\n",
      "Action: arxiv_search(\"Large Language Model\") \n",
      "Action Input: \"Large Language Model\" \u001b[0m\n",
      "Observation: arxiv_search(\"Large Language Model\") is not a valid tool, try one of [arxiv_search].\n",
      "\u001b[32;1m\u001b[1;3mI need to perform a search on arXiv to find relevant papers that discuss Large Language Models. \n",
      "\n",
      "Action: arxiv_search(\"Large Language Model\") \n",
      "Action Input: \"Large Language Model\" \u001b[0m\n",
      "Observation: arxiv_search(\"Large Language Model\") is not a valid tool, try one of [arxiv_search].\n",
      "\u001b[32;1m\u001b[1;3mIt seems there was an error in my previous attempts to search for information. I will try again to find relevant papers on Large Language Models.\n",
      "\n",
      "Action: arxiv_search(\"Large Language Model\") \n",
      "Action Input: \"Large Language Model\" \u001b[0m\n",
      "Observation: arxiv_search(\"Large Language Model\") is not a valid tool, try one of [arxiv_search].\n",
      "\u001b[32;1m\u001b[1;3mIt appears that I am unable to perform the search on arXiv due to a repeated error. However, I can still provide a general understanding of Large Language Models based on my training data.\n",
      "\n",
      "Large Language Models (LLMs) are a type of artificial intelligence model designed to understand and generate human language. They are built using deep learning techniques, particularly transformer architectures, which allow them to process and generate text based on patterns learned from vast amounts of data. LLMs are capable of performing a variety of language tasks, including translation, summarization, question answering, and text generation.\n",
      "\n",
      "These models are trained on diverse datasets that include books, articles, and websites, enabling them to capture a wide range of linguistic nuances and contextual information. Some well-known examples of LLMs include OpenAI's GPT-3 and Google's BERT.\n",
      "\n",
      "Thought: I now know the final answer\n",
      "Final Answer: A Large Language Model (LLM) is an AI model designed to understand and generate human language, built using deep learning techniques, particularly transformer architectures, and trained on vast datasets to perform various language tasks.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is a Large Language Model?',\n",
       " 'output': 'A Large Language Model (LLM) is an AI model designed to understand and generate human language, built using deep learning techniques, particularly transformer architectures, and trained on vast datasets to perform various language tasks.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.invoke(\"What is a Large Language Model?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77d97359-8814-435a-9b70-02cb0d4ca5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CÃ©sar Lattes', 'Pion', 'C. F. Powell', 'Giuseppe Occhialini', 'State University of Campinas', 'Subatomic particle', 'Lattes', 'JosÃ© Leite Lopes', 'Lattes Platform', 'Thermo-dielectric effect']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import wikipedia\n",
    "import urllib3\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "def patched_beautifulsoup(html):\n",
    "    return BeautifulSoup(html, features=\"lxml\")\n",
    "wikipedia.BeautifulSoup = patched_beautifulsoup\n",
    "\n",
    "result = wikipedia.search(\"Cesar Lattes\")\n",
    "print(result) # retorna lista de tÃ­tulos relacionados com essa pesquisa, note que nÃ£o usamos o agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e335ce5-2cf0-4354-8bfa-70405559625a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'title': 'LangChain', 'summary': \"LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\\n\\n\", 'source': 'https://en.wikipedia.org/wiki/LangChain'}, page_content='LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain\\'s use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\\n\\n\\n== History ==\\nLangChain was launched in October 2022 as an open source project by Harrison Chase, while working at machine learning startup Robust Intelligence. The project quickly garnered popularity, with improvements from hundreds of contributors on GitHub, trending discussions on Twitter, lively activity on the project\\'s Discord server, many YouTube tutorials, and meetups in San Francisco and London. In April 2023, LangChain had incorporated and the new startup raised over $20 million in funding at a valuation of at least $200 million from venture firm Sequoia Capital, a week after announcing a $10 million seed investment from Benchmark.\\nIn the third quarter of 2023, the LangChain Expression Language (LCEL) was introduced, which provides a declarative way to define chains of actions.\\nIn October 2023 LangChain introduced LangServe, a deployment tool to host LCEL code as a production-ready API.\\n\\n\\n== Capabilities ==\\nLangChain\\'s developers highlight the framework\\'s applicability to use-cases including chatbots, retrieval-augmented generation,  document summarization, and synthetic data generation.\\nAs of March 2023, LangChain included integrations with systems including Amazon, Google, and Microsoft Azure cloud storage; API wrappers for news, movie information, and weather; Bash for summarization, syntax and semantics checking, and execution of shell scripts; multiple web scraping subsystems and templates; few-shot learning prompt generation support; finding and summarizing \"todo\" tasks in code; Google Drive documents, spreadsheets, and presentations summarization, extraction, and creation; Google Search and Microsoft Bing web search; OpenAI, Anthropic, and Hugging Face language models; iFixit repair guides and wikis search and summarization; MapReduce for question answering, combining documents, and question generation; N-gram overlap scoring; PyPDF, pdfminer, fitz, and pymupdf for PDF file text extraction and manipulation; Python and JavaScript code generation, analysis, and debugging; Milvus vector database to store and retrieve vector embeddings; Weaviate vector database to cache embedding and data objects; Redis cache database storage; Python RequestsWrapper and other methods for API requests; SQL and NoSQL databases including JSON support; Streamlit, including for logging; text mapping for k-nearest neighbors search; time zone conversion and calendar operations; tracing and recording stack symbols in threaded and asynchronous subprocess runs; and the Wolfram Alpha website and SDK. As of April 2023, it can read from more than 50 document types and data sources.\\n\\n\\n== LangChain tools ==\\n\\n\\n== References ==\\n\\n\\n== External links ==\\n\\nOfficial website\\nDiscord server support hub\\nLangchain-ai on GitHub'), Document(metadata={'title': 'Intelligent agent', 'summary': 'In artificial intelligence, an intelligent agent is an entity that perceives its environment, takes actions autonomously to achieve goals, and may improve its performance through machine learning or by acquiring knowledge. Leading AI textbooks define artificial intelligence as the \"study and design of intelligent agents,\" emphasizing that goal-directed behavior is central to intelligence.\\nA specialized subset of intelligent agents, agentic AI (also known as an AI agent or simply agent), expands this concept by proactively pursuing goals, making decisions, and taking actions over extended periods, thereby exemplifying a novel form of digital agency.\\nIntelligent agents can range from simple to highly complex. A basic thermostat or control system is considered an intelligent agent, as is a human being, or any other system that meets the same criteriaâsuch as a firm, a state, or a biome.\\nIntelligent agents operate based on an objective function, which encapsulates their goals. They are designed to create and execute plans that maximize the expected value of this function upon completion. For example, a reinforcement learning agent has a reward function, which allows programmers to shape its desired behavior. Similarly, an evolutionary algorithm\\'s behavior is guided by a fitness function.\\nIntelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, and the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations.\\nIntelligent agents are often described schematically as abstract functional systems similar to computer programs. To distinguish theoretical models from real-world implementations, abstract descriptions of intelligent agents are called abstract intelligent agents. Intelligent agents are also closely related to software agentsâautonomous computer programs that carry out tasks on behalf of users. They are also referred to using a term borrowed from economics: a \"rational agent\".\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Intelligent_agent'}, page_content='In artificial intelligence, an intelligent agent is an entity that perceives its environment, takes actions autonomously to achieve goals, and may improve its performance through machine learning or by acquiring knowledge. Leading AI textbooks define artificial intelligence as the \"study and design of intelligent agents,\" emphasizing that goal-directed behavior is central to intelligence.\\nA specialized subset of intelligent agents, agentic AI (also known as an AI agent or simply agent), expands this concept by proactively pursuing goals, making decisions, and taking actions over extended periods, thereby exemplifying a novel form of digital agency.\\nIntelligent agents can range from simple to highly complex. A basic thermostat or control system is considered an intelligent agent, as is a human being, or any other system that meets the same criteriaâsuch as a firm, a state, or a biome.\\nIntelligent agents operate based on an objective function, which encapsulates their goals. They are designed to create and execute plans that maximize the expected value of this function upon completion. For example, a reinforcement learning agent has a reward function, which allows programmers to shape its desired behavior. Similarly, an evolutionary algorithm\\'s behavior is guided by a fitness function.\\nIntelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, and the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations.\\nIntelligent agents are often described schematically as abstract functional systems similar to computer programs. To distinguish theoretical models from real-world implementations, abstract descriptions of intelligent agents are called abstract intelligent agents. Intelligent agents are also closely related to software agentsâautonomous computer programs that carry out tasks on behalf of users. They are also referred to using a term borrowed from economics: a \"rational agent\".\\n\\n\\n== Intelligent agents as the foundation of AI ==\\n\\nThe concept of intelligent agents provides a foundational lens through which to define and understand artificial intelligence. For instance, the influential textbook Artificial Intelligence: A Modern Approach (Russell & Norvig) describes:\\n\\nAgent: Anything that perceives its environment (using sensors) and acts upon it (using actuators). E.g., a robot with cameras and wheels, or a software program that reads data and makes recommendations.\\nRational Agent: An agent that strives to achieve the *best possible outcome* based on its knowledge and past experiences. \"Best\" is defined by a performance measure â a way of evaluating how well the agent is doing.\\nArtificial Intelligence (as a field): The study and creation of these rational agents.\\nOther researchers and definitions build upon this foundation. Padgham & Winikoff emphasize that intelligent agents should react to changes in their environment in a timely way, proactively pursue goals, and be flexible and robust (able to handle unexpected situations). Some also suggest that ideal agents should be \"rational\" in the economic sense (making optimal choices) and capable of complex reasoning, like having beliefs, desires, and intentions (BDI model). Kaplan and Haenlein offer a similar definition, focusing on a system\\'s ability to understand external data, learn from that data, and use what is learned to achieve goals through flexible adaptation.\\nDefining AI in terms of intelligent agents offers several key advantages:\\n\\nAvoids Philosophical Debates: It sidesteps arguments about whether AI is \"truly\" intelligent or conscious, like those raised by the Turing test or Searle\\'s Chinese Room. It focuses on behavior and goal achievement, not on replicating human thought.\\nObjective Testing: It provides a clear, scientific way to evaluate AI systems. Researchers can compare differen'), Document(metadata={'title': 'Model Context Protocol', 'summary': 'The Model Context Protocol (MCP) is an open standard, open-source framework introduced by Anthropic in November 2024 to standardize the way artificial intelligence (AI) models like large language models (LLMs) integrate and share data with external tools, systems, and data sources. Technology writers have dubbed MCP âthe USB-C of AI appsâ, underscoring its goal of serving as a universal connector between language-model agents and external software. Designed to standardize context exchange between AI assistants and software environments, MCP provides a model-agnostic universal interface for reading files, executing functions, and handling contextual prompts. It was officially announced and open-sourced by Anthropic in November 2024, with subsequent adoption by major AI providers including OpenAI and Google DeepMind.', 'source': 'https://en.wikipedia.org/wiki/Model_Context_Protocol'}, page_content='The Model Context Protocol (MCP) is an open standard, open-source framework introduced by Anthropic in November 2024 to standardize the way artificial intelligence (AI) models like large language models (LLMs) integrate and share data with external tools, systems, and data sources. Technology writers have dubbed MCP âthe USB-C of AI appsâ, underscoring its goal of serving as a universal connector between language-model agents and external software. Designed to standardize context exchange between AI assistants and software environments, MCP provides a model-agnostic universal interface for reading files, executing functions, and handling contextual prompts. It was officially announced and open-sourced by Anthropic in November 2024, with subsequent adoption by major AI providers including OpenAI and Google DeepMind.\\n\\n\\n== Background ==\\nThe protocol was announced in November 2024 as an open standard for connecting AI assistants to data systems such as content repositories, business management tools, and development environments. It addresses the challenge of information silos and legacy systems that constrain even the most sophisticated AI models.\\nAnthropic introduced MCP to address the growing complexity of integrating LLMs with third-party systems. Before MCP, developers often had to build custom connectors for each data source or tool, resulting in what Anthropic described as an \"NÃM\" data integration problem.\\nEarlier stop-gap approaches - such as OpenAIâs 2023 âfunction-callingâ API and the ChatGPT plug-in framework - solved similar problems but required vendor-specific connectors. MCPâs authors note that the protocol deliberately re-uses the message-flow ideas of the Language Server Protocol (LSP) and is transported over JSON-RPC 2.0.\\nMCP was designed as a response to this challenge, offering a universal protocol for interfacing any AI assistant with any structured tool or data layer. The protocol was released with software development kits (SDK) in multiple programming languages, including Python, TypeScript, Java, and C#.\\n\\n\\n== Features ==\\n\\nMCP defines a set of specifications for:\\n\\nData ingestion and transformation\\nContextual metadata tagging\\nModel interoperability across platforms\\nSecure, two-way connections between data sources and AI-powered tools\\nThe protocol enables developers to either expose their data through MCP servers or build AI applications (MCP clients) that connect to these servers. Key components include:\\n\\nProtocol specification and SDKs\\nLocal MCP server support in Claude Desktop apps\\nOpen-source repository of MCP servers\\n\\n\\n== Applications ==\\n  \\nMCP has been applied across a range of use cases in software development, business process automation, and natural language automation:  \\n\\nDesktop assistants: The Claude Desktop app runs local MCP servers to allow the assistant to read files or interact with system tools securely.\\nEnterprise assistants: Companies like Block use MCP to allow internal assistants to retrieve information from proprietary documents, customer relationship management (CRM) systems, and company knowledge bases.\\nMulti-tool agents: MCP supports agentic AI workflows involving multiple tools (e.g., document lookup + messaging APIs), enabling chain-of-thought reasoning over distributed resources.\\nNatural language data access: Applications like AI2SQL leverage MCP to connect models with SQL databases, enabling plain-language information retrieval.\\nSoftware development: Integrated development environments (IDEs) such as Zed, platforms like Replit, and code intelligence tools such as Sourcegraph integrated MCP to give coding assistants access to real-time code context, useful in vibe coding.\\nWeb application development: Wix.com uses MCP servers within its website building platform to enable AI tools to access current website data and make live edits to websites.\\n\\n\\n== Implementation ==\\nAnthropic maintains an open-source repository of reference MCP server implementations for popular enterprise systems'), Document(metadata={'title': 'Milvus (vector database)', 'summary': 'Milvus is a distributed vector database developed by Zilliz. It is available as both open-source software and a cloud service.\\nMilvus is an open-source project under LF AI & Data Foundation distributed under the Apache License 2.0.', 'source': 'https://en.wikipedia.org/wiki/Milvus_(vector_database)'}, page_content='Milvus is a distributed vector database developed by Zilliz. It is available as both open-source software and a cloud service.\\nMilvus is an open-source project under LF AI & Data Foundation distributed under the Apache License 2.0.\\n\\n\\n== History ==\\nMilvus has been developed by Zilliz since 2017.\\nMilvus joined Linux Foundation as an incubation project in January 2020 and became a graduate in June 2021. The details about its architecture and possible applications were presented on ACM SIGMOD Conference in 2021\\nMilvus 2.0, a major redesign of the whole product with a new architecture, was released in January 2022.\\n\\n\\n== Features ==\\n\\n\\n=== Similarity search ===\\nMajor similarity search related features that are available in the active 2.4.x Milvus branch:\\n\\nIn-memory, on-disk and GPU indices,\\nSingle query, batch query and range query search,\\nSupport of sparse vectors, binary vectors, JSON and arrays,\\nFP32, FP16 and BF16 data types,\\nEuclidean distance, inner product distance and cosine distance support for floating-point data,\\nHamming distance and jaccard distance for binary data,\\nSupport of graph indices (including HNSW), Inverted-lists based indices and a brute-force search.\\nSupport of vector quantization for lossy input data compression, including product quantization (PQ) and scalar quantization (SQ), that trades stored data size for accuracy,\\nRe-ranking.\\nMilvus similarity search engine relies on heavily-modified forks of third-party open-source similarity search libraries, such as Faiss, DiskANN and hnswlib.\\nMilvus includes optimizations for I/O data layout, specific to graph search indices.\\n\\n\\n=== Database ===\\nAs a database, Milvus provides the following features:\\n\\nColumn-oriented database\\nFour supported data consistency levels, including strong consistency and eventual consistency.\\nData sharding\\nStreaming data ingestion, which allows to process and ingest data in real-time as it arrives\\nA dynamic schema, which allows inserting the data without a predefined schema\\nIndependent storage and compute layers\\nMulti-tenancy scenarios (database-oriented, collection-oriented, partition-oriented)\\nMemory-mapped data storage\\nRole-based access control\\nMulti-vector and hybrid search\\n\\n\\n=== Deployment options ===\\nMilvus can be deployed as an embedded database, standalone server, or distributed cluster. Zillis Cloud offers a fully managed version.\\n\\n\\n=== GPU support ===\\nMilvus provides GPU accelerated index building and search using Nvidia CUDA technology via Nvidia RAFT library, including a recent GPU-based graph indexing algorithm Nvidia CAGRA\\n\\n\\n=== Integration ===\\nMilvus provides official SDK clients for Java, NodeJS, Python and Go. An additional C# SDK client was contributed by Microsoft. The database can integrate with Prometheus and Grafana for monitoring and alerts, frameworks Haystack and LangChain, IBM Watsonx, and OpenAI models.\\n\\n\\n== See also ==\\n\\nNearest neighbor search\\nSimilarity search\\nVector database\\nVector embedding\\nVector quantization\\n\\n\\n== References =='), Document(metadata={'title': 'FAISS', 'summary': 'FAISS (Facebook AI Similarity Search) is an open-source library for similarity search and clustering of vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning. \\nFAISS is written in C++ with complete wrappers for Python and C. Some of the most useful algorithms are implemented on the GPU using CUDA.', 'source': 'https://en.wikipedia.org/wiki/FAISS'}, page_content='FAISS (Facebook AI Similarity Search) is an open-source library for similarity search and clustering of vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning. \\nFAISS is written in C++ with complete wrappers for Python and C. Some of the most useful algorithms are implemented on the GPU using CUDA.\\n\\n\\n== Features ==\\nFAISS is organized as a toolbox that contains a variety of indexing methods that commonly involve a chain of components (preprocessing, compression, non-exhaustive search, etc.). The scope of the library is intentionally limited to focus on ANNS algorithmic implementation and to avoid facilities related to database functionality, distributed computing or feature extraction algorithms.\\nFAISS is designed with the following assumptions:\\n\\nPrimary data type for vector representation is FP32. The support of other floating-point formats, such as BF16 and FP16, is provided.\\nPrefer batches of input queries over a single input query for the search.\\nEmphasize on allowing users to write a fast prototyping code using its Python wrappers.\\nThe code should be as open as possible, so that users can access all the implementation details of the indexes.\\nThe following major categories of indexing methods are supported:\\n\\nBrute-force search\\nInverted-lists based indices\\nGraph indices, including  (Hierarchical navigable small world) HNSW and Navigating Spread-out Graph (NSG)\\nLocality-sensitive hashing (LSH)\\nThe following families of vector quantization methods are supported:\\n\\nBinary Quantization\\nScalar Quantization (SQ)\\nProduct Quantization (PQ), including Polysemous Codes, Optimized Product Quantization (OPQ) and Quicker ADC (PQFastScan)\\nAdditive Quantization (AQ), including Residual Quantization (RQ) and Local Search Quantization (LSQ)\\nNeural Quantization, including QINCO\\nFAISS focuses on euclidean distance and inner product distance for floating-point data. The limited support of other distances (manhattan distance, Lp distance, etc.) is also available.\\nFAISS code supports multithreading via OpenMP, utilizes BLAS via OpenBLAS or Intel MKL, and also uses custom SIMD kernels for x86 and ARM Neon CPUs.\\nBesides the similarity search, FAISS provides the following useful facilities:\\n\\nk-means clustering\\nRandom-matrix rotations for spreading the variance over all the dimensions without changing the measured distances\\nPrincipal component analysis\\nData deduplication, which is especially useful for image datasets.\\nFAISS has a standalone Vector Codec functionality for the lossy compression of vectors, allowing to trade the representation accuracy for the binary size.\\n\\n\\n== Applications ==\\nTypical FAISS applications include  recommender systems, data mining, text retrieval and content moderation.\\nFAISS was reported to index 1.5 trillion 144-dimensional vectors for internal Meta Platforms applications.\\nFAISS is used in vector databases as a core component of a search engine (OpenSearch, Milvus, Vearch).\\nFAISS is often considered as a baseline in similarity search benchmarks.\\nFAISS has an integration with Haystack, LangChain frameworks.\\nVarious advanced code snippets for FAISS can be found on its snippets wiki page and case studies wiki page.\\n\\n\\n== See also ==\\n\\nNearest neighbor search\\nSimilarity search\\nVector database\\nVector quantization\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nOfficial website\\nfaiss on GitHub\\nOfficial FAISS wiki\\nGuidelines to choose a FAISS index\\nAutofaiss - automatically create Faiss knn indices with the most optimal similarity search parameters'), Document(metadata={'title': 'DataStax', 'summary': 'DataStax, Inc. is a real-time data for AI company based in Santa Clara, California. Its product Astra DB is a cloud database-as-a-service based on Apache Cassandra. DataStax also offers DataStax Enterprise (DSE), an on-premises database built on Apache Cassandra, and Astra Streaming, a messaging and event streaming cloud service based on Apache Pulsar. As of June 2022, the company has roughly 800 customers distributed in over 50 countries.', 'source': 'https://en.wikipedia.org/wiki/DataStax'}, page_content=\"DataStax, Inc. is a real-time data for AI company based in Santa Clara, California. Its product Astra DB is a cloud database-as-a-service based on Apache Cassandra. DataStax also offers DataStax Enterprise (DSE), an on-premises database built on Apache Cassandra, and Astra Streaming, a messaging and event streaming cloud service based on Apache Pulsar. As of June 2022, the company has roughly 800 customers distributed in over 50 countries.\\n\\n\\n== History ==\\nDataStax was built on the open source NoSQL database Apache Cassandra. Cassandra was initially developed internally at Facebook to handle large data sets across multiple servers, and was released as an Apache open source project in 2008. In 2010, Jonathan Ellis and Matt Pfeil left Rackspace, where they had worked with Cassandra, to launch Riptano in Austin, Texas. Ellis and Pfeil later renamed the company DataStax, and moved its headquarters to Santa Clara, California.\\nThe company went on to create its own enterprise version of Cassandra, a NoSQL database called DataStax Enterprise (DSE).\\nIn 2019, Chet Kapoor was named the company's new CEO, taking over from Billy Bosworth.\\n\\nIn May 2020, DataStax released Astra DB, a DBaaS for Cassandra applications. In November 2020, DataStax released K8ssandra, an open source distribution of Cassandra on Kubernetes. In December 2020, DataStax released Stargate, an open source data API gateway.\\nAfter acquiring streaming event vendor Kesque in January 2021, the company launched Luna Streaming, a data streaming platform for Apache Pulsar. DataStax then rebuilt the Kesque technology into Astra Streaming. The Astra Streaming cloud service became generally available on June 29, 2022. With the release, the company added API-level support for messaging tools Apache Kafka, RabbitMQ and Java Message Service, in addition to Apache Pulsar. Astra Streaming can connect to a larger data platform by utilizing DataStax's Astra DB cloud service.\\nStarting in 2023, DataStax began incorporating artificial intelligence and machine learning into its platform. In January 2023, the company acquired Kaskada, developer of a platform that helps organizations use data for AI applications. DataStax made the formerly proprietary Kaskada technology open source, and integrated it into its Luna ML service, which was launched on May 4, 2023. With the acquisition, former Kaskada CEO Davor Bonaci was named DataStax chief technology officer and executive vice president.\\nOn May 24, 2023, DataStax announced that it would be partnering with ThirdAI to bring large language models to DSE and AstraDB, to help developers develop generative AI applications.\\nIn June 2023, the company announced the development of a GPT-based schema translator in its Astra Streaming cloud service. The Astra Streaming GPT Schema Translator uses generative AI to automatically generate schema mappings, to enable data integration and interoperability between multiple systems and data sources.\\nOn July 18, 2023, the company announced a partnership with Google to make semantic search available in its Astra DB cloud database for developers building generative AI applications.\\nOn September 13, 2023, DataStax launched the LangStream open source project, which works with Astra DB and supports vector databases including Milvus and Pinecone. LangStream enables developers to better work with streaming data sources, using Apache Kafka technology and generative AI to help build event-driven architectures.\\nIn November 2023, DataStax announced RAGStack, a simplified commercial offering for RAG (retrieval-augmented generation) based on LangChain and Astra DB vector search.\\nOn February 25, 2025, IBM announced its intention to acquire DataStax.\\n\\n\\n== Products ==\\n\\n\\n=== Astra DB ===\\nAstra DB is available on cloud services such as Microsoft Azure, Amazon Web Services, and Google Cloud Platform. In February 2021, DataStax announced the serverless version of Astra DB, offering developers pay-as-you-go data.\\nIn March 2022, DataStax \"), Document(metadata={'title': 'Retrieval-augmented generation', 'summary': 'Retrieval-augmented generation (RAG) is a technique that enables large language models (LLMs) to retrieve and incorporate new information. With RAG, LLMs do not respond to user queries until they refer to a specified set of documents. These documents supplement information from the LLM\\'s pre-existing training data. This allows LLMs to use domain-specific and/or updated information that is not available in the training data. For example, this helps LLM-based chatbots access internal company data or generate responses based on authoritative sources.\\nRAG improves large language models (LLMs) by incorporating information retrieval before generating responses. Unlike traditional LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources. According to Ars Technica, \"RAG is a way of improving LLM performance, in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts.\" This method helps reduce AI hallucinations, which have caused chatbots to describe policies that don\\'t exist, or recommend nonexistent legal cases to lawyers that are looking for citations to support their arguments.\\nRAG also reduces the need to retrain LLMs with new data, saving on computational and financial costs. Beyond efficiency gains, RAG also allows LLMs to include sources in their responses, so users can verify the cited sources. This provides greater transparency, as users can cross-check retrieved content to ensure accuracy and relevance.\\nThe term RAG was first introduced in a 2020 research paper from Meta.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Retrieval-augmented_generation'}, page_content='Retrieval-augmented generation (RAG) is a technique that enables large language models (LLMs) to retrieve and incorporate new information. With RAG, LLMs do not respond to user queries until they refer to a specified set of documents. These documents supplement information from the LLM\\'s pre-existing training data. This allows LLMs to use domain-specific and/or updated information that is not available in the training data. For example, this helps LLM-based chatbots access internal company data or generate responses based on authoritative sources.\\nRAG improves large language models (LLMs) by incorporating information retrieval before generating responses. Unlike traditional LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources. According to Ars Technica, \"RAG is a way of improving LLM performance, in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts.\" This method helps reduce AI hallucinations, which have caused chatbots to describe policies that don\\'t exist, or recommend nonexistent legal cases to lawyers that are looking for citations to support their arguments.\\nRAG also reduces the need to retrain LLMs with new data, saving on computational and financial costs. Beyond efficiency gains, RAG also allows LLMs to include sources in their responses, so users can verify the cited sources. This provides greater transparency, as users can cross-check retrieved content to ensure accuracy and relevance.\\nThe term RAG was first introduced in a 2020 research paper from Meta.\\n\\n\\n== RAG and LLM Limitations ==\\nLLMs can provide incorrect information. For example, when Google first demonstrated its LLM tool \"Google Bard\", the LLM provided incorrect information about the James Webb Space Telescope. This error contributed to a $100 billion decline in the companyâs stock value. RAG is used to prevent these errors, but it does not solve all the problems. For example, LLMs can generate misinformation even when pulling from factually correct sources if they misinterpret the context. MIT Technology Review gives the example of an AI-generated response stating, \"The United States has had one Muslim president, Barack Hussein Obama.\" The model retrieved this from an academic book rhetorically titled Barack Hussein Obama: Americaâs First Muslim President? The LLM did not \"know\" or \"understand\" the context of the title, generating a false statement.\\nLLMs with RAG are programmed to prioritize new information. This technique has been called \"prompt stuffing.\" Without \"prompt stuffing\", the LLM\\'s input is generated by a user; with prompt stuff, additional relevant context is added to this input to guide the modelâs response. This approach provides the LLM with key information early in the prompt, encouraging it to prioritize the supplied data over pre-existing training knowledge.\\n\\n\\n== Process ==\\nRetrieval-Augmented Generation (RAG) enhances large language models (LLMs) by incorporating an information-retrieval mechanism that allows models to access and utilize additional data beyond their original training set. AWS states, \"RAG allows LLMs to retrieve relevant information from external data sources to generate more accurate and contextually relevant responses\" (indexing). This approach reduces reliance on static datasets, which can quickly become outdated. When a user submits a query, RAG uses a document retriever to search for relevant content from available sources before incorporating the retrieved information into the modelâs response (retrieval). Ars Technica notes that \"when new information becomes available, rather than having to retrain the model, all thatâs needed is to augment the modelâs external knowledge base with the updated information\" (augmentation). By dynamically integrating relevant data, RAG enables LLMs to generate more informed and contextually grounded responses (generation). IBM states that \"in the genera'), Document(metadata={'title': 'Sentence embedding', 'summary': \"In natural language processing, a sentence embedding is a representation of a sentence as a vector of numbers which encodes meaningful semantic information.\\nState of the art embeddings are based on the learned hidden layer representation of dedicated sentence transformer models. BERT pioneered an approach involving the use of a dedicated [CLS] token prepended to the beginning of each sentence inputted into the model; the final hidden state vector of this token encodes information about the sentence and can be fine-tuned for use in sentence classification tasks. In practice however, BERT's sentence embedding with the [CLS] token achieves poor performance, often worse than simply averaging non-contextual word embeddings. SBERT later achieved superior sentence embedding performance by fine tuning BERT's [CLS] token embeddings through the usage of a siamese neural network architecture on the SNLI dataset. \\nOther approaches are loosely based on the idea of distributional semantics applied to sentences. Skip-Thought trains an encoder-decoder structure for the task of neighboring sentences predictions; this has been shown to achieve worse performance than approaches such as InferSent or SBERT. \\nAn alternative direction is to aggregate word embeddings, such as those returned by Word2vec, into sentence embeddings. The most straightforward approach is to simply compute the average of word vectors, known as continuous bag-of-words (CBOW). However, more elaborate solutions based on word vector quantization have also been proposed. One such approach is the vector of locally aggregated word embeddings (VLAWE), which demonstrated performance improvements in downstream text classification tasks.\", 'source': 'https://en.wikipedia.org/wiki/Sentence_embedding'}, page_content=\"In natural language processing, a sentence embedding is a representation of a sentence as a vector of numbers which encodes meaningful semantic information.\\nState of the art embeddings are based on the learned hidden layer representation of dedicated sentence transformer models. BERT pioneered an approach involving the use of a dedicated [CLS] token prepended to the beginning of each sentence inputted into the model; the final hidden state vector of this token encodes information about the sentence and can be fine-tuned for use in sentence classification tasks. In practice however, BERT's sentence embedding with the [CLS] token achieves poor performance, often worse than simply averaging non-contextual word embeddings. SBERT later achieved superior sentence embedding performance by fine tuning BERT's [CLS] token embeddings through the usage of a siamese neural network architecture on the SNLI dataset. \\nOther approaches are loosely based on the idea of distributional semantics applied to sentences. Skip-Thought trains an encoder-decoder structure for the task of neighboring sentences predictions; this has been shown to achieve worse performance than approaches such as InferSent or SBERT. \\nAn alternative direction is to aggregate word embeddings, such as those returned by Word2vec, into sentence embeddings. The most straightforward approach is to simply compute the average of word vectors, known as continuous bag-of-words (CBOW). However, more elaborate solutions based on word vector quantization have also been proposed. One such approach is the vector of locally aggregated word embeddings (VLAWE), which demonstrated performance improvements in downstream text classification tasks.\\n\\n\\n== Applications ==\\nIn recent years, sentence embedding has seen a growing level of interest due to its applications in natural language queryable knowledge bases through the usage of vector indexing for semantic search. LangChain for instance utilizes sentence transformers for purposes of indexing documents. In particular, an indexing is generated by generating embeddings for chunks of documents and storing (document chunk, embedding) tuples. Then given a query in natural language, the embedding for the query can be generated. A top k similarity search algorithm is then used between the query embedding and the document chunk embeddings to retrieve the most relevant document chunks as context information for question answering tasks. This approach is also known formally as retrieval-augmented generation\\nThough not as predominant as BERTScore, sentence embeddings are commonly used for sentence similarity evaluation which sees common use for the task of optimizing a Large language model's generation parameters is often performed via comparing candidate sentences against reference sentences. By using the cosine-similarity of the sentence embeddings of candidate and reference sentences as the evaluation function, a grid-search algorithm can be utilized to automate hyperparameter optimization .\\n\\n\\n== Evaluation ==\\nA way of testing sentence encodings is to apply them on Sentences Involving Compositional Knowledge (SICK) corpus\\nfor both entailment (SICK-E) and relatedness (SICK-R).\\nIn  the best results are obtained using a BiLSTM network trained on the Stanford Natural Language Inference (SNLI) Corpus. The Pearson correlation coefficient for SICK-R is 0.885 and the result for SICK-E is 86.3. A slight improvement over previous scores is presented in: SICK-R: 0.888 and SICK-E: 87.8 using a concatenation of bidirectional Gated recurrent unit.\\n\\n\\n== See also ==\\nDistributional semantics\\nWord embedding\\n\\n\\n== External links ==\\n\\nInferSent sentence embeddings and training code\\nUniversal Sentence Encoder\\nLearning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning\\n\\n\\n== References ==\"), Document(metadata={'title': 'Markov chain', 'summary': 'In probability theory and statistics, a Markov chain or Markov process is a stochastic process describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. Informally, this may be thought of as, \"What happens next depends only on the state of affairs now.\" A countably infinite sequence, in which the chain moves state at discrete time steps, gives a discrete-time Markov chain (DTMC). A continuous-time process is called a continuous-time Markov chain (CTMC). Markov processes are named in honor of the Russian mathematician Andrey Markov.\\nMarkov chains have many applications as statistical models of real-world processes. They provide the basis for general stochastic simulation methods known as Markov chain Monte Carlo, which are used for simulating sampling from complex probability distributions, and have found application in areas including Bayesian statistics, biology, chemistry, economics, finance, information theory, physics, signal processing, and speech processing.\\nThe adjectives Markovian and Markov are used to describe something that is related to a Markov process.', 'source': 'https://en.wikipedia.org/wiki/Markov_chain'}, page_content='In probability theory and statistics, a Markov chain or Markov process is a stochastic process describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. Informally, this may be thought of as, \"What happens next depends only on the state of affairs now.\" A countably infinite sequence, in which the chain moves state at discrete time steps, gives a discrete-time Markov chain (DTMC). A continuous-time process is called a continuous-time Markov chain (CTMC). Markov processes are named in honor of the Russian mathematician Andrey Markov.\\nMarkov chains have many applications as statistical models of real-world processes. They provide the basis for general stochastic simulation methods known as Markov chain Monte Carlo, which are used for simulating sampling from complex probability distributions, and have found application in areas including Bayesian statistics, biology, chemistry, economics, finance, information theory, physics, signal processing, and speech processing.\\nThe adjectives Markovian and Markov are used to describe something that is related to a Markov process.\\n\\n\\n== Principles ==\\n\\n\\n=== Definition ===\\nA Markov process is a stochastic process that satisfies the Markov property (sometimes characterized as \"memorylessness\"). In simpler terms, it is a process for which predictions can be made regarding future outcomes based solely on its present state andâmost importantlyâsuch predictions are just as good as the ones that could be made knowing the process\\'s full history. In other words, conditional on the present state of the system, its future and past states are independent.\\nA Markov chain is a type of Markov process that has either a discrete state space or a discrete index set (often representing time), but the precise definition of a Markov chain varies. For example, it is common to define a Markov chain as a Markov process in either discrete or continuous time with a countable state space (thus regardless of the nature of time), but it is also common to define a Markov chain as having discrete time in either countable or continuous state space (thus regardless of the state space).\\n\\n\\n=== Types of Markov chains ===\\nThe system\\'s state space and time parameter index need to be specified. The following table gives an overview of the different instances of Markov processes for different levels of state space generality and for discrete time v. continuous time:\\n\\nNote that there is no definitive agreement in the literature on the use of some of the terms that signify special cases of Markov processes. Usually the term \"Markov chain\" is reserved for a process with a discrete set of times, that is, a discrete-time Markov chain (DTMC), but a few authors use the term \"Markov process\" to refer to a continuous-time Markov chain (CTMC) without explicit mention. In addition, there are other extensions of Markov processes that are referred to as such but do not necessarily fall within any of these four categories (see Markov model). Moreover, the time index need not necessarily be real-valued; like with the state space, there are conceivable processes that move through index sets with other mathematical constructs. Notice that the general state space continuous-time Markov chain is general to such a degree that it has no designated term.\\nWhile the time parameter is usually discrete, the state space of a Markov chain does not have any generally agreed-on restrictions: the term may refer to a process on an arbitrary state space. However, many applications of Markov chains employ finite or countably infinite state spaces, which have a more straightforward statistical analysis. Besides time-index and state-space parameters, there are many other variations, extensions and generalizations (see Variations). For simplicity, most of this article concentrates on the discrete-time, discrete state-space case, unless mentioned otherwise.\\n\\n\\n=== Transitions ===\\nThe changes '), Document(metadata={'title': 'List of supermarket chains in Germany', 'summary': 'This is a list of current German supermarket chains.', 'source': 'https://en.wikipedia.org/wiki/List_of_supermarket_chains_in_Germany'}, page_content='This is a list of current German supermarket chains.\\n\\n\\n== German supermarket chains ==\\n\\n\\n== See also ==\\n\\nList of supermarket chains\\nList of supermarket chains in Europe\\nIndian grocery online in Germany\\n\\n\\n== References =='), Document(metadata={'title': 'List of The Late Late Show with Craig Ferguson episodes', 'summary': \"The Late Late Show with Craig Ferguson is an American late-night talk show that aired weeknights at 12:37 am (Los Angeles time) on CBS in the United States from January 3, 2005, to December 19, 2014. The hour-long show was hosted by Scottish American comedian, actor and author Craig Ferguson, with his animatronic robot skeleton sidekick Geoff Peterson (voiced by Josh Robert Thompson), and featuring Secretariat, a pantomime horse. The show's writers and other staff appeared in skits and as themselves occasionally, with show producer Michael Naidus becoming a regular. 2,058 episodes were produced.\", 'source': 'https://en.wikipedia.org/wiki/List_of_The_Late_Late_Show_with_Craig_Ferguson_episodes'}, page_content=\"The Late Late Show with Craig Ferguson is an American late-night talk show that aired weeknights at 12:37 am (Los Angeles time) on CBS in the United States from January 3, 2005, to December 19, 2014. The hour-long show was hosted by Scottish American comedian, actor and author Craig Ferguson, with his animatronic robot skeleton sidekick Geoff Peterson (voiced by Josh Robert Thompson), and featuring Secretariat, a pantomime horse. The show's writers and other staff appeared in skits and as themselves occasionally, with show producer Michael Naidus becoming a regular. 2,058 episodes were produced.\\n\\n\\n== 2005 ==\\n\\n\\n=== January ===\\n\\n\\n=== February ===\\n\\n\\n=== March ===\\n\\n\\n=== April ===\\n\\n\\n=== May ===\\n\\n\\n=== June ===\\n\\n\\n=== July ===\\n\\n\\n=== August ===\\n\\n\\n=== September ===\\n\\n\\n=== October ===\\n\\n\\n=== November ===\\n\\n\\n=== December ===\\n\\n\\n== 2006 ==\\n\\n\\n=== January ===\\n\\n\\n=== February ===\\n\\n\\n=== March ===\\n\\n\\n=== April ===\\n\\n\\n=== May ===\\n\\n\\n=== June ===\\n\\n\\n=== July ===\\n\\n\\n=== August ===\\n\\n\\n=== September ===\\n\\n\\n=== October ===\\n\\n\\n=== November ===\\n\\n\\n=== December ===\\n\\n\\n== 2007 ==\\n\\n\\n=== January ===\\n\\n\\n=== February ===\\n\\n\\n=== March ===\\n\\n\\n=== April ===\\n\\n\\n=== May ===\\n\\n\\n=== June ===\\n\\n\\n=== July ===\\n\\n\\n=== August ===\\n\\n\\n=== September ===\\n\\n\\n=== October ===\\n\\n\\n=== November ===\\n\\nThere were no other episodes produced in November as well as all of December due to the 2007â08 Writers Guild of America strike.\\n\\n\\n== 2008 ==\\n\\n\\n=== January ===\\n\\n\\n=== February ===\\n\\n\\n=== March ===\\n\\n\\n=== April ===\\n\\n\\n=== May ===\\n\\n\\n=== June ===\\n\\n\\n=== July ===\\n\\n\\n=== August ===\\n\\n\\n=== September ===\\n\\n\\n=== October ===\\n\\n\\n=== November ===\\n\\n\\n=== December ===\\n\\n\\n== 2009 ==\\n\\n\\n=== January ===\\n\\n\\n=== February ===\\n\\n\\n=== March ===\\n\\n\\n=== April ===\\n\\n\\n=== May ===\\n\\n\\n=== June ===\\n\\n\\n=== July ===\\n\\n\\n=== August ===\\n\\n\\n=== September ===\\n\\n\\n=== October ===\\n\\n\\n=== November ===\\n\\n\\n=== December ===\\n\\n\\n== 2010 ==\\n\\n\\n=== January ===\\n\\n\\n=== February ===\\n\\n\\n=== March ===\\n\\n\\n=== April ===\\n\\n\\n=== May ===\\n\\n\\n=== June ===\\n\\n\\n=== July ===\\n\\n\\n=== August ===\\n\\n\\n=== September ===\\n\\n\\n=== October ===\\n\\n\\n=== November ===\\n\\n\\n=== December ===\\n\\n\\n== 2011 ==\\n\\n\\n=== January ===\\n\\n\\n=== February ===\\n\\n\\n=== March ===\\n\\n\\n=== April ===\\n\\n\\n=== May ===\\n\\n\\n=== June ===\\n\\n\\n=== July ===\\n\\n\\n=== August ===\\n\\n\\n=== September ===\\n\\n\\n=== October ===\\n\\n\\n=== November ===\\n\\n\\n=== December ===\\n\\n\\n== 2012 ==\\n\\n\\n=== January ===\\n\\n\\n=== February ===\\n\\n\\n=== March ===\\n\\n\\n=== April ===\\n\\n\\n=== May ===\\n\\n\\n=== June ===\\n\\n\\n=== July ===\\n\\n\\n=== August ===\\n\\n\\n=== September ===\\n\\n\\n=== October ===\\n\\n\\n=== November ===\\n\\n\\n=== December ===\\n\\n\\n== 2013 ==\\n\\n\\n=== January ===\\n\\n\\n=== February ===\\n\\n\\n=== March ===\\n\\n\\n=== April ===\\n\\n\\n=== May ===\\n\\n\\n=== June ===\\n\\n\\n=== July ===\\n\\n\\n=== August ===\\n\\n\\n=== September ===\\n\\n\\n=== October ===\\n\\n\\n=== November ===\\n\\n\\n=== December ===\\n\\n\\n== 2014 ==\\n\\n\\n=== January ===\\n\\n\\n=== February ===\\n\\n\\n=== March ===\\n\\n\\n=== April ===\\n\\n\\n=== May ===\\n\\n\\n=== June ===\\n\\n\\n=== July ===\\n\\n\\n=== August ===\\n\\n\\n=== September ===\\n\\n\\n=== October ===\\n\\n\\n=== November ===\\n\\n\\n=== December ===\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nCraig Ferguson on Twitter\\nThe Late Late Show with Craig Ferguson at IMDb\"), Document(metadata={'title': 'North Franklin, Maine', 'summary': 'North Franklin is an unorganized territory in Franklin County, Maine, United States. The population was 41 at the 2020 census. The territory consists of 13 townships and three gores, namely Lang, Davis, Tim Pond, Stetsontown, Jim Pond, Alder Stream, Seven Ponds, Kibby, Chain of Ponds, Massachusetts Gore, Coburn Gore, Skinner, Merrill Strip, Lowelltown, Gorham Gore, and Beattie.', 'source': 'https://en.wikipedia.org/wiki/North_Franklin,_Maine'}, page_content='North Franklin is an unorganized territory in Franklin County, Maine, United States. The population was 41 at the 2020 census. The territory consists of 13 townships and three gores, namely Lang, Davis, Tim Pond, Stetsontown, Jim Pond, Alder Stream, Seven Ponds, Kibby, Chain of Ponds, Massachusetts Gore, Coburn Gore, Skinner, Merrill Strip, Lowelltown, Gorham Gore, and Beattie.\\n\\n\\n== Geography ==\\nAccording to the United States Census Bureau, the unorganized territory has a total area of 508.1 square miles (1,315.9 km2), of which 499.3 square miles (1,293.2 km2)  is land and 8.8 square miles (22.7 km2)  (1.72%) is water.\\nThe territory includes all areas north and west of Eustis, Coplin Plantation, Dallas Plantation and the town of Rangeley. Most of this area has never been organized. The only exception is the former Lang Plantation, which surrendered its organization in 1935.\\n\\n\\n== Demographics ==\\n\\nAs of the census of 2000, there were 41 people, 17 households, and 12 families residing in the unorganized territory. The population density was 0.1 inhabitants per square mile (0.039/km2). There were 281 housing units at an average density of 0.6 per square mile (0.23/km2). The racial makeup of the unorganized territory was 92.68% White and 7.32% Black or African American.\\nThere were 17 households, out of which 29.4% had children under the age of 18 living with them, 58.8% were married couples living together, 5.9% had a female householder with no husband present, and 29.4% were non-families. 29.4% of all households were made up of individuals, and 23.5% had someone living alone who was 65 years of age or older. The average household size was 2.41 and the average family size was 2.83.\\nIn the unorganized territory the population was spread out, with 26.8% under the age of 18, 2.4% from 18 to 24, 29.3% from 25 to 44, 26.8% from 45 to 64, and 14.6% who were 65 years of age or older. The median age was 41 years. For every 100 females, there were 105.0 males. For every 100 females age 18 and over, there were 114.3 males.\\nThe median income for a household in the unorganized territory was $26,250, and the median income for a family was $28,750. Males had a median income of $46,250 versus $0 for females. The per capita income for the unorganized territory was $15,670. There were 20.0% of families and 10.0% of the population living below the poverty line, including no under eighteens and none of those over 64.\\n\\n\\n== See also ==\\nCoburn GoreâWoburn Border Crossing\\n\\n\\n== References =='), Document(metadata={'title': 'Cultural impact of Harry Styles', 'summary': 'English singer Harry Styles has made a significant impact on popular culture with his music, artistry, fashion, identity, tours, and commercial achievements worldwide. He is regarded as one of the most influential men in music and fashion, and a subject of widespread public interest with a vast fanbase. He is considered to be among the most successful solo artists who have emerged from a band.\\nHis musical career began in 2010 as part of One Direction, a boy band formed on the British music competition series The X Factor, after each member had been eliminated from the solo contest. They became one of the best-selling boy bands of all time before going on an indefinite hiatus in 2016. Styles has steadily amassed fame, success, and public interest throughout his career, with several media publications considering him a pop icon, a fashion icon, and a global icon, and one of the most successful artists of the 21st century.\\nStyles has been credited for the rise in vinyl sales and has had simultaneous achievements in physical sales, digital sales, streaming, record charts, and touring. His three studio albums were commercially successful and debuted atop the U.S. Billboard 200 chart. Styles has inspired several musicians and novelists, with Rolling Stone UK calling him the \"new King of Pop\" in 2022. He has often been described as a successor to artists such as Elton John, David Bowie, Elvis Presley, Freddie Mercury, and Michael Jackson, and one of the culture-defining figures shaping masculinity, femininity, and gender fluidity through his fashion and lifestyle.', 'source': 'https://en.wikipedia.org/wiki/Cultural_impact_of_Harry_Styles'}, page_content='English singer Harry Styles has made a significant impact on popular culture with his music, artistry, fashion, identity, tours, and commercial achievements worldwide. He is regarded as one of the most influential men in music and fashion, and a subject of widespread public interest with a vast fanbase. He is considered to be among the most successful solo artists who have emerged from a band.\\nHis musical career began in 2010 as part of One Direction, a boy band formed on the British music competition series The X Factor, after each member had been eliminated from the solo contest. They became one of the best-selling boy bands of all time before going on an indefinite hiatus in 2016. Styles has steadily amassed fame, success, and public interest throughout his career, with several media publications considering him a pop icon, a fashion icon, and a global icon, and one of the most successful artists of the 21st century.\\nStyles has been credited for the rise in vinyl sales and has had simultaneous achievements in physical sales, digital sales, streaming, record charts, and touring. His three studio albums were commercially successful and debuted atop the U.S. Billboard 200 chart. Styles has inspired several musicians and novelists, with Rolling Stone UK calling him the \"new King of Pop\" in 2022. He has often been described as a successor to artists such as Elton John, David Bowie, Elvis Presley, Freddie Mercury, and Michael Jackson, and one of the culture-defining figures shaping masculinity, femininity, and gender fluidity through his fashion and lifestyle.\\n\\n\\n== Fame and stardom ==\\nHarry Styles has been labelled one of the greatest and most influential artists of the 21st century. He is one of the richest musicians in the United Kingdom, with an estimated net worth of Â£175 million as of 2024, through his music catalogue and film appearances. Styles has released three studio albums, supported by fourteen singles and one promotional single. All three albums were commercially successful and well-received by music critics, with all of them debuting atop the U.S. Billboard 200 chart and reaching the top two on the UK Albums Chart.\\nStyles has been referred to as a pop icon, a fashion icon, and a global icon. He is considered to be among the most successful solo artists to have emerged from a band. Ben Beaumont-Thomas of The Guardian wrote that Styles is \"a star who has negotiated one of the most difficult transitions in music â from boyband to solo artist â with more panache than even successful forebears such as Justin Timberlake and Robbie Williams\". He has often been described as a successor to artists such as Elton John, David Bowie, Elvis Presley, Freddie Mercury, Michael Jackson, Prince, and Little Richard. Rolling Stone UK named Styles the \"new King of Pop\" in 2022. He is considered to be one of the culture-defining figures shaping masculinity, femininity, gender fluidity, and individuality through his fashion and lifestyle.\\n\\n\\n=== Identity ===\\nJournalists deemed Styles a cultural touchstone in redefining traditional understandings of masculinity and the perception of male musicians. Anne T. Donahue of The Guardian described him as the harbinger of the \"art-throb\" persona, subverting expectations of machismo and hyper-masculinity promoted by other popular male musicians like Justin Bieber and Drake. Columnist Sarah Ditum of The Times labelled Styles \"the gold standard of modern sensitivity\" and said he \"reinvented the playboy\" through his chivalry and respectful nature. Guy Pewsey, writing for Grazia, thought that his \"enthusiasm for frills and flamboyance is far from superficial â it\\'s changing what it means to be a man\". Jack Michael Taylor of the University of Bangor stated \"Styles is integrated within popular culture as a queer icon latched to the music industry and can be used as a vessel to bring forth the media\\'s perpetuations of being genderqueer and what it means to be a \\'man\\'\". Hannah Banks of the University of the S'), Document(metadata={'title': 'List of Jewish Academy Award winners and nominees', 'summary': 'This is a list of Jewish winners and nominees of Academy Awards. It includes ethnic Jews and those who converted to Judaism.', 'source': 'https://en.wikipedia.org/wiki/List_of_Jewish_Academy_Award_winners_and_nominees'}, page_content='This is a list of Jewish winners and nominees of Academy Awards. It includes ethnic Jews and those who converted to Judaism.\\n\\n\\n== Best Actor in a Leading Role ==\\n\\n\\n== Best Actress in a Leading Role ==\\n\\n\\n== Best Actor in a Supporting Role ==\\n\\n\\n== Best Actress in a Supporting Role ==\\n\\n\\n== Best Animated Feature ==\\n\\n\\n== Best Assistant Director ==\\n\\nNote: Defunct category.\\n\\n\\n== Best Cinematography ==\\n\\n\\n== Best Costume Design ==\\n\\n\\n== Best Dance Direction ==\\n\\nNote: Defunct category.\\n\\n\\n== Best Director ==\\n\\n\\n== Best Documentary Feature ==\\n\\n\\n== Best Documentary (Short Subject) ==\\n\\n\\n== Best Film Editing ==\\n\\n\\n== Best International Feature Film ==\\nThe Academy Award for Best International Feature Film is awarded to countries, not individuals. This list contains Jewish directors of nominated films, who typically accept the award on behalf of their country.\\n\\n\\n== Best Makeup and Hairstyling ==\\n\\n\\n== Best Music, Original Score ==\\n\\n\\n== Best Music, Original Song ==\\n\\n\\n== Best Picture ==\\n\\n\\n== Best Production Design ==\\n\\n\\n== Best Animated Short Film ==\\n\\n\\n== Best Live Action Short Film ==\\n\\n\\n== Best Sound ==\\n\\n\\n== Best Story ==\\n\\nNote: Defunct category.\\n\\n\\n== Best Visual Effects ==\\n\\n\\n== Best Writing (Adapted Screenplay) ==\\n\\n\\n== Best Writing (Original Screenplay) ==\\n\\n\\n== Special awards ==\\n\\n\\n== References =='), Document(metadata={'title': '228 Hand-in-Hand rally', 'summary': \"The 228 Hand-in-Hand rally was a demonstration in the form of a human chain held in Taiwan on February 28, 2004, the 57th anniversary of the February 28 Incident. Approximately two million (estimation ranged from 1.2 to 2.3 million depending on the reporting media) Taiwanese formed a 500-kilometer (310 mi) long human chain from Keelung to Pingtung County to commemorate the incident, to call for peace, and to protest the deployment of missiles by the People's Republic of China aimed at Taiwan along the coast. The rally was the largest in Taiwanese history.\", 'source': 'https://en.wikipedia.org/wiki/228_Hand-in-Hand_rally'}, page_content='The 228 Hand-in-Hand rally was a demonstration in the form of a human chain held in Taiwan on February 28, 2004, the 57th anniversary of the February 28 Incident. Approximately two million (estimation ranged from 1.2 to 2.3 million depending on the reporting media) Taiwanese formed a 500-kilometer (310 mi) long human chain from Keelung to Pingtung County to commemorate the incident, to call for peace, and to protest the deployment of missiles by the People\\'s Republic of China aimed at Taiwan along the coast. The rally was the largest in Taiwanese history.\\n\\n\\n== Origin ==\\nThe demonstration was inspired by Baltic Way, the human chain comprising two million that was organized in the Baltic states in 1989. Although billed as \"non-political\", the event was organized by the Pan-Green Coalition and took place only a few weeks before the 2004 Taiwanese presidential election. Some of the symbolism of the demonstration, particularly at the point in the event where participants \"turn away from China\" veered clearly toward support of Taiwanese independence, and hence was not attended by members of the Pan-Blue Coalition who countered the demonstration with an island-wide rally of their own days later.\\n\\n\\n== Human chain ==\\nThe human chain started at the northern tip of Taiwan in Hoping Island near Keelung, stretched 500 kilometers through 18 Taiwanese cities and counties, and ended in Pingtung County.\\nIn Taipei, people linked hands in front of the Presidential Office Building.\\nPresident Chen Shui-bian and former president Lee Teng-hui joined hands in Miaoli County.\\n\\n\\n== See also ==\\nCatalan Way\\nWild Lily student movement\\nSunflower Student Movement\\n\\n\\n== References ==\\n\\n\\n== External links ==\\n\\nHand-in-Hand Taiwan Alliance â official site'), Document(metadata={'title': 'List of The Legend of Qin episodes', 'summary': 'Below is a list of the Episodes for the Chinese CG animated TV series, The Legend of Qin. See the List of The Legend of Qin Characters for their roles and alternative names.', 'source': 'https://en.wikipedia.org/wiki/List_of_The_Legend_of_Qin_episodes'}, page_content='Below is a list of the Episodes for the Chinese CG animated TV series, The Legend of Qin. See the List of The Legend of Qin Characters for their roles and alternative names.\\n\\n\\n== Series overview ==\\n\\n\\n== Season 1: 2007 ==\\nThe title for the season is: ç¾æ­¥é£å Bai Bu Fei Jian (The Hundred Pace Flying Sword), which is also\\nGe Nie\\'s signature move.  It consists of only 10 episodes.\\n\\n\\n== Season 2: 2008 ==\\nThe title for the season is Night Ends - Daybreakï¼Tianmingï¼. It consists of 18 episodes.\\n\\n\\n== Season 3: 2009 ==\\nThe title for the season is: è¯¸å­ç¾å®¶ Zhu Zi Bai Jia (Masters of the Hundreds of Schools), the word \"Hundred\" here means many instead of an accurate number. The expression is the classical description for the diverse intellectual life of the Warring States period of Chinese history.\\nThe third season was finished in 2009 and aired in 2010, it consists of 34 episodes.\\n\\n\\n== Season 4: 2012 ==\\nThe title for the season is ä¸éé¿å (wÃ nlÇ chÃ¡ngchÃ©ng) and it consists of 37 episodes.\\n\\n\\n== Season 5: 2014 ==\\n\\n\\n== Specials ==\\nBirdsong in Hollow Valley: 2014\\nThis special consists of three episodes which tells the story of Baifeng and his friend Moya who were assassins for General Ji Wuye, but who turn against him after Baifeng falls for a female assassin, Nong Yu.\\n\\nUnder Luosheng Tang\\nThis special tells of a young Taoist who joins the Yin-Yang School to find his sister.\\n\\nLady Xiang Descends\\nThis special is a story about a wounded man who finds himself caught up in the lives of twin sisters and the man that they both fell in love with.\\n\\n\\n== See also ==\\nList of The Legend of Qin characters\\nThe Legend of Qin animated film\\n\\n\\n== References =='), Document(metadata={'title': \"List of Dragons' Den (British TV programme) offers Series 1-10\", 'summary': \"The following is a list of offers made on the British reality television series Dragons' Den in Series 1â10, originally aired during 2005â2012. 104 episodes of Dragons' Den were broadcast consisting of at least 754 pitches. A total of 129 pitches were successful, with 26 offers from the dragons rejected by the entrepreneurs and 599 failing to receive an offer of investment.\\n\\n\", 'source': 'https://en.wikipedia.org/wiki/List_of_Dragons%27_Den_(British_TV_programme)_offers_Series_1-10'}, page_content=\"The following is a list of offers made on the British reality television series Dragons' Den in Series 1â10, originally aired during 2005â2012. 104 episodes of Dragons' Den were broadcast consisting of at least 754 pitches. A total of 129 pitches were successful, with 26 offers from the dragons rejected by the entrepreneurs and 599 failing to receive an offer of investment.\\n\\n\\n== Series overview ==\\n\\n\\n== Successful pitches ==\\n\\n\\n=== Series 1 ===\\n\\n\\n=== Series 2 ===\\n\\n\\n=== Series 3 ===\\n\\n\\n=== Series 4 ===\\n\\n\\n=== Series 5 ===\\n\\n\\n=== Series 6 ===\\n\\n\\n=== Series 7 ===\\n\\n\\n=== Series 8 ===\\n\\n\\n=== Series 9 ===\\n\\n\\n=== Series 10 ===\\n\\n\\n== Rejected offers ==\\n\\n\\n=== Series 1 ===\\n\\n\\n=== Series 2 ===\\n\\n\\n=== Series 3 ===\\n\\n\\n=== Series 4 ===\\n\\n\\n=== Series 5 ===\\n\\n\\n=== Series 6 ===\\n\\n\\n=== Series 7 ===\\n\\n\\n=== Series 9 ===\\n\\n\\n=== Series 10 ===\\n\\n\\n== Failed pitches ==\\n\\n\\n=== Series 1 ===\\n\\n\\n=== Series 2 ===\\n\\n\\n=== Series 3 ===\\n\\n\\n=== Series 4 ===\\n\\n\\n=== Series 5 ===\\n\\n\\n=== Series 6 ===\\n\\n\\n=== Series 7 ===\\n\\n\\n=== Series 8 ===\\n\\n\\n=== Series 9 ===\\n\\n\\n=== Series 10 ===\\n\\n\\n== References ==\"), Document(metadata={'title': 'Itaconic acid', 'summary': 'Itaconic acid is an organic compound with the formula CH2=C(CO2H)CH2CO2H. With two carboxyl groups, it is classified as a dicarboxylic acid.  It is a non-toxic white solid that is soluble in water and several organic solvents.  It plays several roles in biology.', 'source': 'https://en.wikipedia.org/wiki/Itaconic_acid'}, page_content=\"Itaconic acid is an organic compound with the formula CH2=C(CO2H)CH2CO2H. With two carboxyl groups, it is classified as a dicarboxylic acid.  It is a non-toxic white solid that is soluble in water and several organic solvents.  It plays several roles in biology.\\n\\n\\n== Reactions ==\\nUpon heating, itaconic acid converts to its anhydride.\\nAs a dicarboxylic acid, itaconic acid has two pKa's.  At pH levels above 7, itaconic acid exists as its double negatively charged form, termed itaconate. \\nAs an Î±,Î²-unsaturated carbonyl compound, itaconic acid is a good Michael acceptor.  Thus, nucleophiles add across the C=C bond. \\n\\nCH2=C(CO2H)CH2CO2H  +  R2P(O)H   â   R2P(O)CH2âCHCH2(CO2H)2 (R = organic group).\\nThis reaction is the means by which the fire retarding chemical 9,10-Dihydro-9-oxa-10-phosphaphenanthrene-10-oxide can be incorporated into polymers.\\n\\n\\n== Production ==\\nIn 1836, Samuel Baup discovered itaconic acid as a by-product in a dry distillation of citric acid. In the late 1920s, itaconic acid was isolated from a fungus in the Aspergillus genus of fungi The dry distillation forms itaconic anhydride, which then is hydrolyzed. Since the 1960s, however, it has been produced commercially by fermenting glucose, molasses, or another abundant carbon sources by a fungus such as Aspergillus itaconicus, Aspergillus terreus, or Ustilago maydis have also been investigated. One generally accepted route by which fungi make itaconate is through the tricarboxylic acid cycle pathway. This pathway forms cis-aconitate which is converted to itaconate by cis-aconitate-decarboxylase. Animal cells also make itaconate by an enzyme-catalyzed reaction from cis-aconitate, an intermediate metabolite in the tricarboxylic acid cycle, (i.e., TCA cycle). The itaconate-producing reaction is stimulated when the TCA cycle is suppressed. \\nUstilago maydis  makes itaconic acid from trans-aconitate, catalyzed by aconitate delta-isomerase. The trans-aconitate product is decarboxylated to itaconate by trans-aconitate decarboxylase (i.e., TAD1, an enzyme found in Ustilago maydis) Itaconate has also been obtained by fermenting the fungi Yarrowia lipolytica with glucose, various species of Candida fungi with glucose, Ustilago vetiveriae fungus with glycerol, and various species of Aspergillus niger fungi with glucose, sorbitol, or sorbitol plus xylose mixture. Fermenting Escherichia coli bacteria with glucose, xylose, glycerol, or starch and Corynebacterium glutamicum bacteria with glucose or urea also affords itaconic acid.  Ustilago maydis has, however, been genetically engineered to increase its itaconic acid production,\\n\\n\\n== History ==\\nIn the 1930s itaconate was shown to have bactericidal actions.  In 2011, Strelko et al. reported that itaconate was produced by two mammalian immortalized cell lines, cultured mouse VM-M3 brain tumor cells and RAW 264.7 mouse macrophages, and by macrophages isolated from mice. This group also showed that stimulation of mouse macrophages with the bacterial toxin, lipopolysaccharide (i.e., LPS, also termed endotoxin), increased their production and secretion of itaconate. In 2013, Michelucci et al. revealed the biosynthesis pathway that makes itaconate in mammals. These publications were followed by numerous others focused on the biology of itaconate and certain itaconate-like compounds as regulars of various cellular responses in animals and possibly humans.\\n\\n\\n== Biology of Itaconate ==\\nBiological studies focus on itaconate's physiological and pathological functions. \\n\\n\\n=== Cells making itaconate ===\\nThe major cell types that normally make itaconate in response to stressful conditions are hematological cells such as the macrophages located in various tissues and the monocytes located in the bone marrow and blood. These cells are phagocytes, i.e., cells that ingulf microorganisms, dead or seriously injured cells, and foreign particles all of which cause inflammatory responses. Itaconate is also produced by certain myeloid-derived suppressor \"), Document(metadata={'title': 'List of German films of the 1990s', 'summary': 'This is a list of the most notable films produced in Cinema of Germany in the 1990s.\\nFor an alphabetical list of articles on German films see Category:1990s German films.', 'source': 'https://en.wikipedia.org/wiki/List_of_German_films_of_the_1990s'}, page_content='This is a list of the most notable films produced in Cinema of Germany in the 1990s.\\nFor an alphabetical list of articles on German films see Category:1990s German films.\\n\\n\\n== 1990 ==\\n\\n\\n== 1991 ==\\n\\n\\n== 1992 ==\\n\\n\\n== 1993 ==\\n\\n\\n== 1994 ==\\n\\n\\n== 1995 ==\\n\\n\\n== 1996 ==\\n\\n\\n== 1997 ==\\n\\n\\n== 1998 ==\\n\\n\\n== 1999 ==\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nGerman film at the Internet Movie Database (maintains separate lists for West Germany and East Germany)'), Document(metadata={'title': 'List of festivals in the Philippines', 'summary': 'The origin of most early festivals, locally known as \"fiestas\", are rooted in Christianity, dating back to the Spanish colonial period when the many communities (such as barrios and towns) of the predominantly Catholic Philippines almost always had a patron saint assigned to each of them. Originally encouraged by the Spanish to coincide with Christian holy days, early patronal festivals became vital instruments in spreading Christianity throughout the country.\\nFestivals in the Philippines can be religious, cultural, or both. Several of these are held to honor the local Roman Catholic patron saint, to commemorate local history and culture, to promote the community\\'s products, or to celebrate a bountiful harvest. They can be marked by Holy Masses, processions, parades, theatrical play and reenactments, religious or cultural rituals, trade fairs, exhibits, concerts, pageants and various games and contests. However, festivals in the country are not limited to Christian origins. Many festivals also focus on Islamic or indigenous concepts. There are more than 42,000 known major and minor festivals in the Philippines, the majority of which are in the barangay (village) level. Due to the thousands of town, city, provincial, national, and village fiestas in the country, the Philippines has traditionally been known as the Capital of the World\\'s Festivities. The majority of festivals in the Philippines may have their own peryas (trade fairs with temporary amusement parks).\\nSome festivals, such as Holy Week and Christmas, are declared as public holidays, and thus, are observed and celebrated nationwide.', 'source': 'https://en.wikipedia.org/wiki/List_of_festivals_in_the_Philippines'}, page_content='The origin of most early festivals, locally known as \"fiestas\", are rooted in Christianity, dating back to the Spanish colonial period when the many communities (such as barrios and towns) of the predominantly Catholic Philippines almost always had a patron saint assigned to each of them. Originally encouraged by the Spanish to coincide with Christian holy days, early patronal festivals became vital instruments in spreading Christianity throughout the country.\\nFestivals in the Philippines can be religious, cultural, or both. Several of these are held to honor the local Roman Catholic patron saint, to commemorate local history and culture, to promote the community\\'s products, or to celebrate a bountiful harvest. They can be marked by Holy Masses, processions, parades, theatrical play and reenactments, religious or cultural rituals, trade fairs, exhibits, concerts, pageants and various games and contests. However, festivals in the country are not limited to Christian origins. Many festivals also focus on Islamic or indigenous concepts. There are more than 42,000 known major and minor festivals in the Philippines, the majority of which are in the barangay (village) level. Due to the thousands of town, city, provincial, national, and village fiestas in the country, the Philippines has traditionally been known as the Capital of the World\\'s Festivities. The majority of festivals in the Philippines may have their own peryas (trade fairs with temporary amusement parks).\\nSome festivals, such as Holy Week and Christmas, are declared as public holidays, and thus, are observed and celebrated nationwide.\\n\\n\\n== List ==\\n\\nThe partial calendar list contains several of the oldest and larger religious and/or cultural festivals in the country. Each town, city, and village has a dedicated fiesta, resulting in thousands held throughout the year; a few are national in character. Some fiestas may contain multiple/conflicting dates and/or place entries.\\n\\n\\n=== January ===\\n\\nJanuary 20 Pana Pana Festival \\nTarlac city Tarlac\\n\\n\\n=== February ===\\n\\n\\n=== March ===\\n     Part of Holy Week\\n\\n\\n=== April ===\\n     Part of Holy Week\\n\\n\\n=== May ===\\n\\n\\n=== June ===\\n\\n\\n=== July ===\\n\\n\\n=== August ===\\n\\n\\n=== September ===\\n\\n\\n=== October ===\\n\\n\\n=== November ===\\n\\n\\n=== December ===\\n     Part of Christmas\\n\\n Bacolod, Philippines<ref name=\\n\\n\\n== See also ==\\n\\nChristmas in the Philippines\\nHoly Week in the Philippines\\nFilm festivals in the Philippines\\nMusic festivals in the Philippines\\nPublic holidays in the Philippines\\nPatronal festival\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nOfficial Philippine Tourism Website'), Document(metadata={'title': 'List of The Weekly with Charlie Pickering episodes', 'summary': 'The Weekly with Charlie Pickering is an Australian news satire series on the ABC. The series premiered on 22 April 2015, and Charlie Pickering as host with Tom Gleeson, Adam Briggs, Kitty Flanagan (2015â2018) in the cast, and Judith Lucy joined the series in 2019. The first season consisted of 20 episodes and concluded on 22 September 2015. The series was renewed for a second season on 18 September 2015, which premiered on 3 February 2016. The series was renewed for a third season with Adam Briggs joining the team and began airing from 1 February 2017. The fourth season premiered on 2 May 2018 at the later timeslot of 9:05pm to make room for the season return of Gruen at 8:30pm, and was signed on for 20 episodes. \\nFlanagan announced her departure from The Weekly With Charlie Pickering during the final episode of season four, but returned for The Yearly with Charlie Pickering special in December 2018. \\nIn 2019, the series was renewed for a fifth season with Judith Lucy announced as a new addition to the cast as a \"wellness expert\".\\nThe show was pre-recorded in front of an audience in ABC\\'s Ripponlea studio on the same day of its airing from 2015 to 2017. In 2018, the fourth season episodes were pre-recorded in front of an audience at the ABC Southbank Centre studios. In 2020, the show was filmed without a live audience due to COVID-19 pandemic restrictions and comedian Luke McGregor joined the show as a regular contributor. Judith Lucy did not return in 2021 and ZoÃ« Coombs Marr joined as a new cast member in season 7 with the running joke that she was fired from the show in episode one yet she kept returning to work for the show.', 'source': 'https://en.wikipedia.org/wiki/List_of_The_Weekly_with_Charlie_Pickering_episodes'}, page_content='The Weekly with Charlie Pickering is an Australian news satire series on the ABC. The series premiered on 22 April 2015, and Charlie Pickering as host with Tom Gleeson, Adam Briggs, Kitty Flanagan (2015â2018) in the cast, and Judith Lucy joined the series in 2019. The first season consisted of 20 episodes and concluded on 22 September 2015. The series was renewed for a second season on 18 September 2015, which premiered on 3 February 2016. The series was renewed for a third season with Adam Briggs joining the team and began airing from 1 February 2017. The fourth season premiered on 2 May 2018 at the later timeslot of 9:05pm to make room for the season return of Gruen at 8:30pm, and was signed on for 20 episodes. \\nFlanagan announced her departure from The Weekly With Charlie Pickering during the final episode of season four, but returned for The Yearly with Charlie Pickering special in December 2018. \\nIn 2019, the series was renewed for a fifth season with Judith Lucy announced as a new addition to the cast as a \"wellness expert\".\\nThe show was pre-recorded in front of an audience in ABC\\'s Ripponlea studio on the same day of its airing from 2015 to 2017. In 2018, the fourth season episodes were pre-recorded in front of an audience at the ABC Southbank Centre studios. In 2020, the show was filmed without a live audience due to COVID-19 pandemic restrictions and comedian Luke McGregor joined the show as a regular contributor. Judith Lucy did not return in 2021 and ZoÃ« Coombs Marr joined as a new cast member in season 7 with the running joke that she was fired from the show in episode one yet she kept returning to work for the show.\\n\\n\\n== Series overview ==\\n\\n\\n== Episodes ==\\n\\n\\n=== Season 1 (2015) ===\\n\\n\\n=== Season 2 (2016) ===\\n\\n\\n=== Season 3 (2017) ===\\n\\n\\n=== Season 4 (2018) ===\\n\\n\\n=== Season 5 (2019) ===\\n\\n\\n=== Season 6 (2020) ===\\n\\n\\n=== Season 7 (2021) ===\\n\\n\\n=== Season 8 (2022) ===\\n\\n\\n=== Season 9 (2023) ===\\n\\n\\n=== Season 10 (2024) ===\\n\\n\\n=== Season 11 (2025) ===\\n\\n\\n=== The Yearly with Charlie Pickering ===\\n\\n\\n== References =='), Document(metadata={'title': 'List of people who adopted matrilineal surnames', 'summary': \"This is a list of notable people who have changed, adopted or adjusted their surnames based on a mother's or grandmother's maiden name. Included are people who changed their legal names and people who created personal or professional pseudonyms. Under longstanding Western custom and law, children are customarily given the father's surname, except for children born outside marriage, who often carry their mother's family names. In mediaeval times where a great family died out in the male line, an alternative male heir to the estates was selected as one of the younger sons of a daughter, who was required by the bequest to adopt, by royal licence, in lieu of his patronymic, his maternal surname and coat of arms for himself and his descendants. This was also the origin of double-barrelled surnames, where the paternal surname was partially retained, or resurrected by a later generation. The compliance with the terms of the bequest was essential to avoid challenge by another potential heir in the lawcourts. In the 1970s some women began to adopt their mother's maiden name as their legal surnames. People in Sweden have recently begun adopting maternal line surnames in an effort to broaden the number of last names in the country. Such practices add considerable difficulties to the study of genealogy and family history.\", 'source': 'https://en.wikipedia.org/wiki/List_of_people_who_adopted_matrilineal_surnames'}, page_content='This is a list of notable people who have changed, adopted or adjusted their surnames based on a mother\\'s or grandmother\\'s maiden name. Included are people who changed their legal names and people who created personal or professional pseudonyms. Under longstanding Western custom and law, children are customarily given the father\\'s surname, except for children born outside marriage, who often carry their mother\\'s family names. In mediaeval times where a great family died out in the male line, an alternative male heir to the estates was selected as one of the younger sons of a daughter, who was required by the bequest to adopt, by royal licence, in lieu of his patronymic, his maternal surname and coat of arms for himself and his descendants. This was also the origin of double-barrelled surnames, where the paternal surname was partially retained, or resurrected by a later generation. The compliance with the terms of the bequest was essential to avoid challenge by another potential heir in the lawcourts. In the 1970s some women began to adopt their mother\\'s maiden name as their legal surnames. People in Sweden have recently begun adopting maternal line surnames in an effort to broaden the number of last names in the country. Such practices add considerable difficulties to the study of genealogy and family history.\\n\\n\\n== Stage names ==\\nMany actors and other entertainers elect to add or include their mothers\\' maiden names in their adopted stage names. The book How to be a Working Actor: The Insider\\'s Guide to Finding Jobs in Theater, Film, and Television advises aspiring performers to consider changing their names, noting that \"if [your birth name] is difficult to spell, pronounce, or remember, it may not be the name you want for your professional career.\" It goes on to suggest: \"If you want to retain a connection to your family, try using your mother\\'s maiden name or the name of a revered relative.\"\\nA person\\'s mother\\'s maiden name is used by many financial institutions as a key piece of information to validate a customer\\'s identity. In 2005, researchers showed that the common practice of using a mother\\'s maiden name as the basis for a stage name could be exploited to entice people to reveal that name and other details that could allow fraudsters to steal their identities. Researchers asked a random sample of people on London streets a series of questions, beginning with \"What is your name?\" They then engaged in conversation about theatre, asked people if they knew how actors choose their stage names, then told them that stage names were typically a combination of the name of a pet and the mother\\'s maiden name. Next the participants were asked what their stage names would be; 94% responded by revealing both their mother\\'s maiden name and a pet\\'s name.\\n\\n\\n== List ==\\n\\n\\n=== A ===\\nJanet Achurch, British actress, was born Janet Sharp; her mother died during childbirth and she was reared by her father William Prior Sharp, an insurance agent. She later adopted her maternal grandparents\\' surname (Achurch) as her professional name.\\nEdie Adams, American actress and singer, was born Edith Elizabeth Enke to Sheldon Alonzo Enke and his wife, Ada Dorothy (nÃ©e Adams), later adopting her mother\\'s maiden name as her professional name.\\nMaude Adams, American actress, was born Maude Ewing Kiskadden (or Maude Ewing Adams Kiskadden) and adopted her mother\\'s maiden name of Adams as her stage name.\\nAndrÃ¡s AdorjÃ¡n, Hungarian author and chess grandmaster; born as AndrÃ¡s Jocha (or Jocha AndrÃ¡s), he adopted his mother\\'s maiden name, AdorjÃ¡n, at the age of 18, in 1968.\\nTheodor Adorno, German sociologist, philosopher and musicologist known for his critical theory of society, was born Theodor Ludwig Wiesengrund to Oscar Alexander Wellington (1870â1946) and Maria Calvelli-Adorno della Piana (1865â1952). His mother wanted her son\\'s paternal surname to be supplemented by the addition of her own surname/maiden name, Adorno. His earliest publications carried the name Th'), Document(metadata={'title': 'Alliance of Women Film Journalists', 'summary': 'The Alliance of Women Film Journalists (AWFJ) is a non-profit organization founded in 2006. It is based in New York City and is dedicated to supporting work by and about women in the film industry. The AWFJ is composed of 84 professional female movie critics, journalists, and feature writers working in print, broadcast, and online media. The British Film Institute describes the AWFJ as an organization that collects articles by its (mainly U.S.-based) members, gives annual awards, and \"supports films by and about women\".', 'source': 'https://en.wikipedia.org/wiki/Alliance_of_Women_Film_Journalists'}, page_content='The Alliance of Women Film Journalists (AWFJ) is a non-profit organization founded in 2006. It is based in New York City and is dedicated to supporting work by and about women in the film industry. The AWFJ is composed of 84 professional female movie critics, journalists, and feature writers working in print, broadcast, and online media. The British Film Institute describes the AWFJ as an organization that collects articles by its (mainly U.S.-based) members, gives annual awards, and \"supports films by and about women\".\\n\\n\\n== EDA Awards ==\\n\\nBeginning in 2007, the group annually gives awards to the best (and worst) in film, as voted on by its members. These awards are called EDAs in honor of AWFJ founder Jennifer Merin\\'s mother, actress Eda Reiss Merin. EDA is also an acronym for Excellent Dynamic Activism. These awards have been reported on in recent years by a number of mainstream media sources including Time, USA Today, and Variety, and are also included in The New York Times\\' movie reviews awards lists. In 2007, AWFJ released a Top 100 Films List in response to the American Film Institute revision of their 100 Years, 100 Films list. In creating their list, the AWFJ was motivated by the fact that only 4.5 out of 400 films on AFI\\'s ballot were directed by women.\\nIn addition to awards for achievement granted regardless of gender (Best Film, Best Director, Best Original Screenplay, Best Screenplay Adapted, Best Documentary, Best Animated Film, Best Ensemble Cast, Best Editing, Best Cinematography, Best Film Music or Score, and Best Non-English-Language Film), there are also \"EDA Female Focus Awards\" (Best Woman Director, Best Woman Screenwriter, Kick Ass Award For Best Female Action Star, Best Animated Female, Best Breakthrough Performance, Best Newcomer, Women\\'s Image Award, Sexist Pig Award, Hanging in There Award for Persistence, Actress Defying Age and Agism, Lifetime Achievement Award, Award for Humanitarian Activism, Female Icon Award, and This Year\\'s Outstanding Achievement by a Woman in the Film Industry) and \"EDA Special Mention Awards\" (Hall of Shame Award, Actress Most in Need of a New Agent, Movie You Wanted to Love But Just Couldn\\'t, Unforgettable Moment Award, Best Depiction of Nudity, Sexuality, or Seduction, Most Egregious Age Difference Between Leading Man and Love Interest, Bravest Performance Award, Best Leap from Actress to Director Award, Cultural Crossover Award, Sequel or Remake That Shouldn\\'t Have Been Made Award, and Best of the Fests).\\n\\n\\n== EDA Best of Awards winners and nominees ==\\n\\n\\n== EDA Female Focus Awards winners and nominees ==\\n\\n\\n=== EDA Special Mention Awards ===\\n\\n\\n== Notes ==\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nOfficial website'), Document(metadata={'title': 'List of European Academy Award winners and nominees', 'summary': 'This is a list of European Academy Award winners and nominees, which includes people born and/or raised in Europe and people born outside of Europe who are citizens of European countries.', 'source': 'https://en.wikipedia.org/wiki/List_of_European_Academy_Award_winners_and_nominees'}, page_content='This is a list of European Academy Award winners and nominees, which includes people born and/or raised in Europe and people born outside of Europe who are citizens of European countries.\\n\\n\\n== Best Picture ==\\n\\n\\n== Best Director ==\\n\\n\\n== Best Actor in a Leading Role ==\\n\\n\\n== Best Actress in a Leading Role ==\\n\\n\\n== Best Actor in a Supporting Role ==\\n\\n\\n== Best Actress in a Supporting Role ==\\n\\n\\n== Best Original Screenplay ==\\n\\n\\n== Best Adapted Screenplay ==\\nDuring certain ceremonies the Oscars only gave out one screenplay awards. If a screenplay is original, it will be noted as such.\\n\\n\\n== Best Story ==\\n\\n\\n== Best Animated Feature ==\\n\\n\\n== Best Animated Short Film ==\\n\\n\\n== Best Cinematography ==\\n\\n\\n== Best Film Editing ==\\n\\n\\n== Best Makeup and Hairstyling ==\\n\\n\\n== Best Original Score ==\\n\\n\\n== Best International Feature Film ==\\n\\n\\n== References =='), Document(metadata={'title': 'Robert Lang Studios', 'summary': 'Robert Lang Studios is a recording studio in Shoreline, Washington, United States. Numerous bands have recorded at Robert Lang Studios since 1974 including Nirvana, Alice in Chains, Foo Fighters, Soundgarden, Dave Matthews Band, Death Cab for Cutie, Heart, Sir Mix-A-Lot, Peter Frampton, Candlebox, and Bush.', 'source': 'https://en.wikipedia.org/wiki/Robert_Lang_Studios'}, page_content='Robert Lang Studios is a recording studio in Shoreline, Washington, United States. Numerous bands have recorded at Robert Lang Studios since 1974 including Nirvana, Alice in Chains, Foo Fighters, Soundgarden, Dave Matthews Band, Death Cab for Cutie, Heart, Sir Mix-A-Lot, Peter Frampton, Candlebox, and Bush.\\n\\n\\n== History ==\\nRobert Lang quit his job as a Boeing hydrofoil TIG welder to pursue a career in recording, first establishing the studio in 1974 in the garage of a beach house in Shoreline, Washington, near Seattle. The studio remained garage-based for the first seven years, with early projects including Seattle\\'s Franklin High School jazz lab, which included a 15 year-old saxophonist by the name of Kenneth Gorelick (who would later achieve fame as Kenny G), as well as Albert Collins.\\nIn 1982, Lang purchased the property. He began gradually excavating the earth beneath the house over the course of the next several years to create new subterranean reinforced concrete rooms for the studio. In April 1992, Candlebox recorded two songs that would be included on their debut album at Robert Lang Studios, including their hit song \"Far Behind\" In 1993, 11 years after construction began, the studio\\'s new live room, complete with marble and granite floors and natural stone walls, was finally finished.\\nIn late January 1994, Nirvana recorded their last known studio recording, \"You Know You\\'re Right\" at Robert Lang Studios. In October of the same year, Dave Grohl, formerly of Nirvana, booked the studio, which was close to his house, for six days, for his own musical project. The resulting album was the self-titled debut album that resulted in Grohl\\'s forming Foo Fighters. \\nIn October 1998, frontman Layne Staley recorded vocals for two new Alice In Chains songs, \"Get Born Again\" and \"Died\". The songs, which were released on the Music Bank box set in 1999, would be the last the singer would record with the band.\\nIn 2006 Peter Frampton recorded the track \"Blowin\\' Smoke\" with Pearl Jam drummer Matt Cameron and guitarist Mike McCready for his album Fingerprints. In 2008, Death Cab for Cutie recorded their highest-charting album, Narrow Stairs at the studio.\\nIn 2014 Foo Fighters and Death Cab for Cutie\\'s Ben Gibbard returned to Robert Lang Studios for episode 7 of the HBO rockumentary mini-series Foo Fighters Sonic Highways. During the episode, Foo Fighters and Gibbard recorded the song \"Subterranean\".\\nArtists recording at Robert Lang Studios have included Heart, Sir Mix-A-Lot, Alice In Chains, Dave Matthews, Bush, Brandi Carlile, QueensrÃ¿che, Train, Portugal. The Man, and The Blood Brothers.\\n\\n\\n== Selected albums recorded at Robert Lang Studios ==\\nSir Mix-a-Lot - Seminar (1989)\\nMichael Gettel - Skywatching (1993)\\nFoo Fighters - Foo Fighters (1994)\\nPretty Girls Make Graves - Good Health (2001) \\nMastodon - Leviathan (2004)\\nDeath Cab for Cutie - Narrow Stairs (2008)\\nHockey Dad - Blend Inn (2018)\\n\\n\\n== See also ==\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nOfficial website')]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import WikipediaLoader\n",
    "loader = WikipediaLoader(\"LangChain\")\n",
    "documents = loader.load()\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7a7885d-25ea-472c-9199-a9b817000bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vamos explorar as vÃ¡rias facetas de sua trajetÃ³ria e entender como ele se tornou o primeiro presidente afro-americano. Uma histÃ³ria de resiliÃªncia e determinaÃ§Ã£o Barack Hussein Obama II nasceu em Honolulu, Havai, em 4 de agosto de 1961. Barack Obama nasceu em 4 de agosto de 1961, em Honolulu, Havai. Seu nome foi escolhido por seu pai, Barack Hussein Obama Sr., que era um economista queniano. O nome foi uma forma de honrar suas raÃ­zes africanas e a cultura Luo, que Ã© uma parte importante da identidade de Obama. George Washington foi o primeiro presidente dos EUA e exerceu seu mandato de 1789 a 1797. Quantos presidentes republicanos os EUA elegeram? Quantos presidentes democratas os EUA elegeram? Quantos presidentes dos Estados Unidos da AmÃ©rica nÃ£o terminaram o mandato? 9 presidentes nÃ£o concluÃ­ram seus mandatos. George Washington, o primeiro presidente dos Estados Unidos, Ã© o Ãºnico que nÃ£o pertence a um partido polÃ­tico. O primeiro presidente do partido Whig, William Henry Harrison, morreu apÃ³s servir apenas um mÃªs no cargo em 1841. Barack Hussein Obama se tornou largamente conhecido em seu discurso na convenÃ§Ã£o democrata de 2004, onde o jovem senador de Illinois apresentou sua histÃ³ria de vida e a trajetÃ³ria improvÃ¡vel que o levaria ao estrelato polÃ­tico aos EUA.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install -qU duckduckgo-search langchain-community\n",
    "# https://python.langchain.com/docs/integrations/tools/ddg/\n",
    "\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "search.invoke(\"Qual o primeiro nome do presidente Obama?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "339347f5-1b54-4b1d-a129-6f99149be6c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'MathResponse',\n",
       "  'args': {'steps': [{'explanation': 'Subtract 31 from both sides to isolate the term with x.',\n",
       "     'output': '8x = 2 - 31'},\n",
       "    {'explanation': 'Calculate the right side: 2 - 31 = -29.',\n",
       "     'output': '8x = -29'},\n",
       "    {'explanation': 'Divide both sides by 8 to solve for x.',\n",
       "     'output': 'x = -29 / 8'},\n",
       "    {'explanation': 'Simplify the fraction if possible.',\n",
       "     'output': 'x = -3.625'}],\n",
       "   'final_answer': 'x = -3.625'},\n",
       "  'id': 'call_AQVJVOtSygvwiE4yTq9vvaaj',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Step(BaseModel):\n",
    "    explanation: str\n",
    "    output: str\n",
    "\n",
    "class MathResponse(BaseModel):\n",
    "    steps: list[Step]\n",
    "    final_answer: str\n",
    "\n",
    "tools = [MathResponse]\n",
    "llm_math_agent = llm.bind_tools(tools)\n",
    "\n",
    "query = \"solve 8x + 31 = 2\"\n",
    "ai_msg = llm_math_agent.invoke(query)\n",
    "ai_msg.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac26f174-dce6-46df-94ef-67021b433554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'x = -3.625'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg.tool_calls[0]['args']['final_answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abcca6ef-1628-4e2c-b582-a4932849a7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: Subtract 31 from both sides to isolate the term with x.\n",
      "step 2: Calculate the right side: 2 - 31 = -29.\n",
      "step 3: Divide both sides by 8 to solve for x.\n",
      "step 4: Simplify the fraction if possible.\n",
      "final answer: x = -3.625\n"
     ]
    }
   ],
   "source": [
    "for istep, step in enumerate(ai_msg.tool_calls[0]['args']['steps']):\n",
    "    print(f\"step {istep+1}: {step['explanation']}\")\n",
    "\n",
    "print(f\"final answer: {ai_msg.tool_calls[0]['args']['final_answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3682d401-894a-4357-b60a-c02a79200a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'MathResponse',\n",
       "  'args': {'steps': [{'explanation': 'Rearranging the equation to standard form: x^2 + 3x - 5 = 0',\n",
       "     'output': 'x^2 + 3x - 5 = 0'},\n",
       "    {'explanation': 'Using the quadratic formula: x = (-b Â± â(bÂ² - 4ac)) / 2a, where a = 1, b = 3, c = -5',\n",
       "     'output': 'x = (-3 Â± â(3Â² - 4*1*(-5))) / (2*1)'},\n",
       "    {'explanation': 'Calculating the discriminant: 3Â² - 4*1*(-5) = 9 + 20 = 29',\n",
       "     'output': 'Discriminant = 29'},\n",
       "    {'explanation': 'Substituting the discriminant back into the quadratic formula: x = (-3 Â± â29) / 2',\n",
       "     'output': 'x = (-3 Â± â29) / 2'},\n",
       "    {'explanation': 'Calculating the two possible values for x: xâ = (-3 + â29) / 2 and xâ = (-3 - â29) / 2',\n",
       "     'output': 'xâ = (-3 + â29) / 2, xâ = (-3 - â29) / 2'}],\n",
       "   'final_answer': 'xâ = (-3 + â29) / 2, xâ = (-3 - â29) / 2'},\n",
       "  'id': 'call_o8wmLHNvZf8uWRmqcvS3obhw',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"solve x**2 + 3x = 5\"\n",
    "ai_msg = llm_math_agent.invoke(query)\n",
    "ai_msg.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "687bb64b-50bf-44c7-aa81-74dc518dea91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xâ = (-3 + â29) / 2, xâ = (-3 - â29) / 2'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg.tool_calls[0]['args']['final_answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "854dd00a-bb6d-45ae-83a5-fa3ca59725db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\abrantes\\AppData\\Local\\Temp\\ipykernel_24356\\2662787266.py:17: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = agent.run(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrasÃ­lia.\n"
     ]
    }
   ],
   "source": [
    "# https://python.langchain.com/docs/integrations/tools/ddg/\n",
    "\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "\n",
    "duckduckgo_tool = DuckDuckGoSearchRun()\n",
    "tools = [duckduckgo_tool]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "query = \"Qual a capital do Brasil?\"\n",
    "result = agent.run(query)\n",
    "\n",
    "# Exiba o resultado\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af498bc-d144-4389-8911-7f29cc9fbfd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
