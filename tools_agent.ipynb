{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "339e83db-ac56-422f-914c-5ae67fe58abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\otimi\\anaconda3\\envs\\python\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# https://python.langchain.com/docs/integrations/tools/\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c04374b-5fe1-4231-8204-f71eb4d5a253",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(dotenv_path='.env')\n",
    "\n",
    "# https://platform.openai.com/api-keys\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# https://console.groq.com/keys\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# https://console.anthropic.com/dashboard\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "# https://aistudio.google.com/app/apikey\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "568461f7-be61-43dd-893f-31089557ae0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities import ArxivAPIWrapper\n",
    "from langchain.tools import Tool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "# pip install arxiv\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key, model_name=\"gpt-4o-mini\", temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a2f3cd5-1d24-400e-b032-5ef9f1fd87b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\otimi\\AppData\\Local\\Temp\\ipykernel_48748\\2503047421.py:14: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 1.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  agent_chain = initialize_agent(\n",
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mPara responder a essa pergunta, vou procurar informações sobre o que é um Large Language Model (LLM). Isso pode incluir definições, características e exemplos. A melhor abordagem é usar a Wikipedia, que geralmente tem uma boa explicação sobre conceitos técnicos. \n",
      "\n",
      "Action: wikipedia  \n",
      "Action Input: \"Large Language Model\"  \u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mPage: Large language model\n",
      "Summary: A large language model (LLM) is a computational model capable of language generation or other natural language processing tasks. As language models, LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a self-supervised and semi-supervised training process.\n",
      "The largest and most capable LLMs, as of August 2024, are artificial neural networks built with a decoder-only transformer-based architecture, which enables efficient processing and generation of large-scale text data. Modern models can be fine-tuned for specific tasks or can be guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained on.\n",
      "Some notable LLMs are OpenAI's GPT series of models (e.g., GPT-3.5, GPT-4 and GPT-4o; used in ChatGPT and Microsoft Copilot), Google's Gemini (used in the chatbot of the same name), Meta's LLaMA family of models, IBM's Granite models initially released with Watsonx, Anthropic's Claude models, and Mistral AI's models.\n",
      "\n",
      "\n",
      "\n",
      "Page: Language model\n",
      "Summary: A language model is a probabilistic model of a natural language. In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\n",
      "Language models are useful for a variety of tasks, including speech recognition (helping prevent predictions of low-probability (e.g. nonsense) sequences), machine translation, natural language generation (generating more human-like text), optical character recognition, handwriting recognition, grammar induction, and information retrieval.\n",
      "Large language models, currently their most advanced form, are a combination of larger datasets (frequently using words scraped from the public internet), feedforward neural networks, and transformers. They have superseded recurrent neural network-based models, which had previously superseded the pure statistical models, such as word n-gram language model.\n",
      "\n",
      "\n",
      "\n",
      "Page: Claude (language model)\n",
      "Summary: Claude is a family of large language models developed by Anthropic. The first model was released in March 2023. Claude 3, released in March 2024, can also analyze images.\n",
      "\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.  \n",
      "Final Answer: Um Large Language Model (LLM) é um modelo computacional capaz de gerar linguagem ou realizar outras tarefas de processamento de linguagem natural. Esses modelos aprendem relações estatísticas a partir de grandes quantidades de texto durante um processo de treinamento auto-supervisionado e semi-supervisionado. Os LLMs mais avançados são redes neurais artificiais construídas com uma arquitetura de transformador apenas de decodificação, permitindo o processamento e a geração eficientes de grandes volumes de dados textuais. Exemplos notáveis de LLMs incluem os modelos da série GPT da OpenAI, Gemini do Google, e os modelos LLaMA da Meta.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Um Large Language Model (LLM) é um modelo computacional capaz de gerar linguagem ou realizar outras tarefas de processamento de linguagem natural. Esses modelos aprendem relações estatísticas a partir de grandes quantidades de texto durante um processo de treinamento auto-supervisionado e semi-supervisionado. Os LLMs mais avançados são redes neurais artificiais construídas com uma arquitetura de transformador apenas de decodificação, permitindo o processamento e a geração eficientes de grandes volumes de dados textuais. Exemplos notáveis de LLMs incluem os modelos da série GPT da OpenAI, Gemini do Google, e os modelos LLaMA da Meta.\n"
     ]
    }
   ],
   "source": [
    "# pip install wikipedia\n",
    "# pip install numexpr\n",
    "\n",
    "from langchain.agents import (\n",
    "    AgentExecutor, AgentType, initialize_agent, load_tools\n",
    ")\n",
    "import wikipedia\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, streaming=True, model_name=\"gpt-4o-mini\", openai_api_key=openai_api_key)\n",
    "tools = load_tools(\n",
    "    tool_names=[\"llm-math\", \"arxiv\", \"wikipedia\"],\n",
    "    llm=llm\n",
    ")\n",
    "agent_chain = initialize_agent(\n",
    "        tools=tools, \n",
    "        llm=llm, \n",
    "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "        verbose=True\n",
    ")\n",
    "resposta = agent_chain.invoke(\"O que é uma Large Language Model?\")\n",
    "print(resposta[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebf452d2-32db-445a-b94f-f4ea6b5bf6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mPreciso procurar por estudos relacionados a redes neurais convolucionais no Arxiv. \n",
      "Action: arxiv\n",
      "Action Input: \"convolutional neural networks\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mPublished: 2019-10-03\n",
      "Title: ILP-M Conv: Optimize Convolution Algorithm for Single-Image Convolution Neural Network Inference on Mobile GPUs\n",
      "Authors: Zhuoran Ji\n",
      "Summary: Convolution neural networks are widely used for mobile applications. However,\n",
      "GPU convolution algorithms are designed for mini-batch neural network training,\n",
      "the single-image convolution neural network inference algorithm on mobile GPUs\n",
      "is not well-studied. After discussing the usage difference and examining the\n",
      "existing convolution algorithms, we proposed the HNTMP convolution algorithm.\n",
      "The HNTMP convolution algorithm achieves $14.6 \\times$ speedup than the most\n",
      "popular \\textit{im2col} convolution algorithm, and $2.30 \\times$ speedup than\n",
      "the fastest existing convolution algorithm (direct convolution) as far as we\n",
      "know.\n",
      "\n",
      "Published: 2019-03-19\n",
      "Title: Kernel-based Translations of Convolutional Networks\n",
      "Authors: Corinne Jones, Vincent Roulet, Zaid Harchaoui\n",
      "Summary: Convolutional Neural Networks, as most artificial neural networks, are\n",
      "commonly viewed as methods different in essence from kernel-based methods. We\n",
      "provide a systematic translation of Convolutional Neural Networks (ConvNets)\n",
      "into their kernel-based counterparts, Convolutional Kernel Networks (CKNs), and\n",
      "demonstrate that this perception is unfounded both formally and empirically. We\n",
      "show that, given a Convolutional Neural Network, we can design a corresponding\n",
      "Convolutional Kernel Network, easily trainable using a new stochastic gradient\n",
      "algorithm based on an accurate gradient computation, that performs on par with\n",
      "its Convolutional Neural Network counterpart. We present experimental results\n",
      "supporting our claims on landmark ConvNet architectures comparing each ConvNet\n",
      "to its CKN counterpart over several parameter settings.\n",
      "\n",
      "Published: 2022-12-19\n",
      "Title: VC dimensions of group convolutional neural networks\n",
      "Authors: Philipp Christian Petersen, Anna Sepliarskaia\n",
      "Summary: We study the generalization capacity of group convolutional neural networks.\n",
      "We identify precise estimates for the VC dimensions of simple sets of group\n",
      "convolutional neural networks. In particular, we find that for infinite groups\n",
      "and appropriately chosen convolutional kernels, already two-parameter families\n",
      "of convolutional neural networks have an infinite VC dimension, despite being\n",
      "invariant to the action of an infinite group.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAgora tenho algumas informações sobre estudos relacionados a redes neurais convolucionais no Arxiv. \n",
      "\n",
      "Final Answer: Encontrei alguns estudos sobre redes neurais convolucionais no Arxiv, incluindo:\n",
      "\n",
      "1. **ILP-M Conv: Optimize Convolution Algorithm for Single-Image Convolution Neural Network Inference on Mobile GPUs** (2019-10-03) - Este estudo propõe o algoritmo de convolução HNTMP, que alcança um aumento de velocidade de 14,6 vezes em comparação com o algoritmo de convolução mais popular.\n",
      "\n",
      "2. **Kernel-based Translations of Convolutional Networks** (2019-03-19) - Este trabalho apresenta uma tradução sistemática das Redes Neurais Convolucionais em suas contrapartes baseadas em kernel, mostrando que é possível projetar uma Rede Neural Convolucional correspondente que pode ser treinada de forma eficaz.\n",
      "\n",
      "3. **VC dimensions of group convolutional neural networks** (2022-12-19) - Este estudo investiga a capacidade de generalização das redes neurais convolucionais de grupo, identificando estimativas precisas para as dimensões VC de conjuntos simples dessas redes.\n",
      "\n",
      "Se precisar de mais informações ou de outros estudos, é só avisar!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Encontrei alguns estudos sobre redes neurais convolucionais no Arxiv, incluindo:\n",
      "\n",
      "1. **ILP-M Conv: Optimize Convolution Algorithm for Single-Image Convolution Neural Network Inference on Mobile GPUs** (2019-10-03) - Este estudo propõe o algoritmo de convolução HNTMP, que alcança um aumento de velocidade de 14,6 vezes em comparação com o algoritmo de convolução mais popular.\n",
      "\n",
      "2. **Kernel-based Translations of Convolutional Networks** (2019-03-19) - Este trabalho apresenta uma tradução sistemática das Redes Neurais Convolucionais em suas contrapartes baseadas em kernel, mostrando que é possível projetar uma Rede Neural Convolucional correspondente que pode ser treinada de forma eficaz.\n",
      "\n",
      "3. **VC dimensions of group convolutional neural networks** (2022-12-19) - Este estudo investiga a capacidade de generalização das redes neurais convolucionais de grupo, identificando estimativas precisas para as dimensões VC de conjuntos simples dessas redes.\n",
      "\n",
      "Se precisar de mais informações ou de outros estudos, é só avisar!\n"
     ]
    }
   ],
   "source": [
    "resposta = agent_chain.invoke(\"Mostre-me estudos sobre redes neurais convolucionais no Arxiv?\")\n",
    "print(resposta[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9775fb51-a398-4d42-96da-883bf34dd45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mPara encontrar a raiz quadrada de 144, posso usar uma calculadora. \n",
      "\n",
      "Action: Calculator\n",
      "Action Input: 144**0.5\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: 12.0\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.  \n",
      "Final Answer: A raiz quadrada de 144 é 12.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "A raiz quadrada de 144 é 12.\n"
     ]
    }
   ],
   "source": [
    "resposta = agent_chain.invoke(\"qual a raiz quadrada de 144?\")\n",
    "print(resposta[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8305cb8-54f9-4b44-bc5a-56fbf5c48076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mPara resolver a equação 2x + 3 = 11, preciso isolar a variável x. Primeiro, subtrairei 3 de ambos os lados da equação e, em seguida, dividirei o resultado por 2. \n",
      "\n",
      "Action: Calculator\n",
      "Action Input: (11 - 3) / 2\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: 4.0\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.  \n",
      "Final Answer: x = 4.0\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "x = 4.0\n"
     ]
    }
   ],
   "source": [
    "resposta = agent_chain.invoke(\"Resolva a equação 2x + 3 = 11.\")\n",
    "print(resposta[\"output\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c4f5783-d31e-4c2e-abcf-7010beb27429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mPara responder a essa pergunta, primeiro preciso identificar qual é o país sede dos Jogos Olímpicos de 2016 e qual é a sua população. Os Jogos Olímpicos de 2016 foram realizados no Brasil, especificamente na cidade do Rio de Janeiro. Vou procurar a população atual do Brasil para calcular a raiz quadrada.\n",
      "\n",
      "Action: wikipedia  \n",
      "Action Input: \"população do Brasil\"  \u001b[0m\u001b[36;1m\u001b[1;3mPage: Brazil\n",
      "Summary: Brazil, officially the Federative Republic of Brazil, is the largest and easternmost country in South America and Latin America. It is the world's fifth-largest country by area and the seventh most populous. Its capital is Brasília, and its most populous city is São Paulo. Brazil is a federation composed of 26 states and a Federal District. It is the only country in the Americas where Portuguese is an official language. Brazil is among the world's most multicultural and ethnically diverse nations, due to over a century of mass immigration from around the world.\n",
      "Bounded by the Atlantic Ocean on the east, Brazil has a coastline of 7,491 kilometers (4,655 mi). Covering roughly half of South America's land area, it borders all other countries and territories on the continent except Ecuador and Chile. Brazil's Amazon basin includes a vast tropical forest home to diverse wildlife, a variety of ecological systems, and extensive natural resources spanning numerous protected habitats. This unique environmental heritage positions Brazil at number one of 17 megadiverse countries. The country's natural richness is also the subject of significant global interest, as environmental degradation (through processes such as deforestation) has direct impacts on global issues such as climate change and biodiversity loss.\n",
      "The territory of present-day Brazil was inhabited by numerous tribal nations prior to the landing of explorer Pedro Álvares Cabral in 1500. Subsequently claimed by the Portuguese Empire, Brazil remained a Portuguese colony until 1808, when the capital of the empire was transferred from Lisbon to Rio de Janeiro. In 1815, the colony was elevated to the rank of kingdom upon the formation of the United Kingdom of Portugal, Brazil and the Algarves. Independence was achieved in 1822 with the creation of the Empire of Brazil, a unitary state governed under a constitutional monarchy and a parliamentary system. The ratification of the first constitution in 1824 led to the formation of a bicameral legislature, now called the National Congress. Slavery was abolished in 1888. The country became a presidential republic in 1889 following a military coup d'état. An authoritarian military dictatorship emerged in 1964 and ruled until 1985, after which civilian governance resumed. Brazil's current constitution, formulated in 1988, defines it as a democratic federal republic. Due to its rich culture and history, the country ranks thirteenth in the world by number of UNESCO World Heritage Sites.\n",
      "Brazil is a regional and middle power that is an emerging power and a major non-NATO ally of the United States. Categorized as a developing country and ranking 89th on the Human Development Index, Brazil is considered an advanced emerging economy, having the eighth largest GDP in the world in both nominal and PPP terms—the largest in Latin America and the Southern Hemisphere. Classified as an upper-middle income economy by the World Bank, and a newly industrialized country by the IMF, Brazil has the largest share of wealth and the most complex economy in South America. It is also one of the world's major breadbaskets, being the largest producer of coffee for the last 150 years. Despite its growing economic and global profile, the country continues to face high levels of corruption, crime and social inequality. Brazil is a founding member of the United Nations, the G20, BRICS, G4, Mercosul, Organization of American States, Organization of Ibero-American States and the Community of Portuguese Language Countries. Brazil is also an observer state of the Arab League.\n",
      "\n",
      "Page: Demographics of Brazil\n",
      "Summary: Brazil had an official resident population of 203 million in 2022, according to IBGE. Brazil is the seventh most populous country in the world and the second most populous in the Americas and Western Hemisphere.\n",
      "Brazilians are mainly concentrated in the eastern part of the country, which comprises the Southeast, South, and Northeast. But it \u001b[0m\u001b[32;1m\u001b[1;3mA população do Brasil em 2022 era de aproximadamente 203 milhões de habitantes. Agora, preciso calcular a raiz quadrada desse número.\n",
      "\n",
      "Action: Calculator  \n",
      "Action Input: 203000000  \u001b[0m\u001b[33;1m\u001b[1;3mAnswer: 203000000\u001b[0m\u001b[32;1m\u001b[1;3mParece que houve um erro no cálculo. A raiz quadrada de 203 milhões não foi calculada corretamente. Vou tentar novamente.\n",
      "\n",
      "Action: Calculator  \n",
      "Action Input: 203000000**0.5  \u001b[0m\u001b[33;1m\u001b[1;3mAnswer: 14247.806848775008\u001b[0m\u001b[32;1m\u001b[1;3mA raiz quadrada da população do Brasil, que era de aproximadamente 203 milhões de habitantes, é cerca de 14.248. Agora, posso fornecer a resposta final.\n",
      "\n",
      "Final Answer: A raiz quadrada da população do Brasil, sede dos Jogos Olímpicos de 2016, é aproximadamente 14.248.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Qual a raiz quadrada da população do país sede dos Jogos Olímpicos de 2016 ?',\n",
       " 'output': 'A raiz quadrada da população do Brasil, sede dos Jogos Olímpicos de 2016, é aproximadamente 14.248.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import load_tools, create_react_agent, AgentExecutor\n",
    "from langchain import hub\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key, model_name=\"gpt-4o-mini\", temperature=0)\n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm) # indica duas ferramentas wikipedia e calculadora\n",
    "agent = create_react_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    prompt = hub.pull(\"hwchase17/react\"),\n",
    ")\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)\n",
    "\n",
    "question = \"Qual a raiz quadrada da população do país sede dos Jogos Olímpicos de 2016 ?\"\n",
    "agent_executor.invoke({\"input\": question}) # ao receber a pergunta ele decide qual ferramenta deve usar se wikipedia ou calculadora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5465599c-062a-4674-a145-e3c246826df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mPara encontrar a soma de 324 e 768, posso usar uma calculadora.  \n",
      "Action: Calculator  \n",
      "Action Input: 324 + 768  \u001b[0m\u001b[33;1m\u001b[1;3mAnswer: 1092\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer.  \n",
      "Final Answer: 1092\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "1092\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    question = \"Qual é a soma de 324 e 768 ?\"\n",
    "    resposta = agent_executor.invoke({\"input\": question}) # ao receber a pergunta ele decide qual ferramenta deve usar se wikipedia ou calculadora\n",
    "    print(resposta[\"output\"])\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84ccacfe-3bab-40ca-9c23-e4a477ce5503",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCesar Lattes foi um físico brasileiro conhecido por suas contribuições significativas à física de partículas e à ciência em geral. Vou buscar mais informações sobre ele para fornecer uma resposta mais completa.  \n",
      "Action: wikipedia  \n",
      "Action Input: Cesar Lattes  \u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\otimi\\anaconda3\\envs\\python\\Lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\otimi\\anaconda3\\envs\\python\\Lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3mPage: César Lattes\n",
      "Summary: Cesare Mansueto Giulio Lattes (11 July 1924 – 8 March 2005), also known as César Lattes, was a Brazilian experimental physicist, one of the discoverers of the pion, a composite subatomic particle made of a quark and an antiquark.\n",
      "\n",
      "Page: Pion\n",
      "Summary: In particle physics, a pion (, PIE-on) or pi meson, denoted with the Greek letter pi (π), is any of three subatomic particles: π0, π+, and π−. Each pion consists of a quark and an antiquark and is therefore a meson. Pions are the lightest mesons and, more generally, the lightest hadrons. They are unstable, with the charged pions π+ and π− decaying after a mean lifetime of 26.033 nanoseconds (2.6033×10−8 seconds), and the neutral pion π0 decaying after a much shorter lifetime of 85 attoseconds (8.5×10−17 seconds). Charged pions most often decay into muons and muon neutrinos, while neutral pions generally decay into gamma rays.\n",
      "The exchange of virtual pions, along with vector, rho and omega mesons, provides an explanation for the residual strong force between nucleons. Pions are not produced in radioactive decay, but commonly are in high-energy collisions between hadrons. Pions also result from some matter–antimatter annihilation events. All types of pions are also produced in natural processes when high-energy cosmic-ray protons and other hadronic cosmic-ray components interact with matter in Earth's atmosphere. In 2013, the detection of characteristic gamma rays originating from the decay of neutral pions in two supernova remnants has shown that pions are produced copiously after supernovas, most probably in conjunction with production of high-energy protons that are detected on Earth as cosmic rays.\n",
      "The pion also plays a crucial role in cosmology, by imposing an upper limit on the energies of cosmic rays surviving collisions with the cosmic microwave background, through the Greisen–Zatsepin–Kuzmin limit.\u001b[0m\u001b[32;1m\u001b[1;3mCesar Lattes foi um físico experimental brasileiro, conhecido por ser um dos descobridores do píon, uma partícula subatômica composta por um quark e um antiquark. Ele nasceu em 11 de julho de 1924 e faleceu em 8 de março de 2005. Suas contribuições foram significativas para a física de partículas e a ciência em geral. \n",
      "\n",
      "Final Answer: César Lattes foi um físico experimental brasileiro, um dos descobridores do píon, nascido em 11 de julho de 1924 e falecido em 8 de março de 2005.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Quem é Cesar Lattes? resposta em português',\n",
       " 'output': 'César Lattes foi um físico experimental brasileiro, um dos descobridores do píon, nascido em 11 de julho de 1924 e falecido em 8 de março de 2005.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Quem é Cesar Lattes? resposta em português\"\n",
    "agent_executor.invoke({\"input\": question}) # ao receber a pergunta ele decide qual ferramenta deve usar se wikipedia ou calculadora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854dd00a-bb6d-45ae-83a5-fa3ca59725db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
