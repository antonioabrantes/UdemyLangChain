{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "938f8ec9-e319-473f-86db-bfc49ac305aa",
   "metadata": {},
   "source": [
    "## Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d2c1f76-b4f8-4241-a63d-bd59f19a940d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain\n",
    "#pip install langchain-openai\n",
    "#pip install langchain_community\n",
    "#pip install langchain-groq\n",
    "#pip install datasets\n",
    "#pip install qdrant-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69ba2e2b-87f2-4820-952c-15ee62d1af93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/docs/integrations/llms/openai/\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "load_dotenv('.env',override=True)\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8aa0b34-9182-498d-b63f-678203e99b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(\n",
    "    model='gpt-4o-mini',\n",
    "    openai_api_key = openai_api_key,\n",
    "    max_tokens = 512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f52c263-2b0e-4acd-95e7-e6a04056a0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
    "    HumanMessage(content=\"I'd like to understand machine learning.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "940dfe9d-c6d5-4e75-ab80-1e5e03c7f227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Absolutely! Machine learning is a subset of artificial intelligence that focuses on the development of algorithms and statistical models that enable computers to perform tasks without explicit instructions, relying instead on patterns and inference from data.\\n\\n### Key Concepts in Machine Learning:\\n\\n1. **Types of Machine Learning**:\\n   - **Supervised Learning**: The model is trained on labeled data, which means that the input comes with the corresponding output. The aim is to learn a mapping from inputs to outputs.\\n     - Example: Classification (e.g., email spam detection) and regression (e.g., predicting house prices).\\n     \\n   - **Unsupervised Learning**: The model is trained on data without labeled responses. The goal is to find hidden structures or patterns in the data.\\n     - Example: Clustering (e.g., customer segmentation) and association (e.g., market basket analysis).\\n     \\n   - **Semi-Supervised Learning**: Combines a small amount of labeled data with a large amount of unlabeled data during training.\\n   - **Reinforcement Learning**: The model learns by interacting with an environment and receiving rewards or penalties based on its actions.\\n\\n2. **Key Components**:\\n   - **Data**: The foundation of machine learning; it can be structured (like databases) or unstructured (like text or images).\\n   - **Features**: Individual measurable properties or characteristics used in the model.\\n   - **Model**: The mathematical representation that makes predictions or decisions based on input data.\\n   - **Training**: The process of feeding data into the model to help it learn patterns.\\n   - **Testing/Validation**: Evaluating the model on unseen data to check its performance.\\n\\n3. **Common Algorithms**:\\n   - **Linear Regression**: Used for regression tasks.\\n   - **Logistic Regression**: Used for binary classification.\\n   - **Decision Trees**: A flowchart-like structure for decisions based on features.\\n   - **Support Vector Machines (SVM)**: Used for classification by finding the hyperplane that best separates classes.\\n   - **Neural Networks**: Models inspired by the human brain, commonly used for deep learning tasks like image and speech recognition.\\n\\n4. **Evaluation Metrics**:\\n   - **Accuracy**: The proportion of true results among the total cases.\\n   - **Precision and Recall**: Measures for the performance of classification models, particularly in imbalanced datasets.\\n   - **F1 Score**: A balance between precision and recall.\\n   - **Mean Squared Error (MSE', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 512, 'prompt_tokens': 51, 'total_tokens': 563, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-Bi52e4dUUyj3o7DwmAzhkvSFpJv8o', 'service_tier': 'default', 'finish_reason': 'length', 'logprobs': None}, id='run--25e0978c-87cd-4dd5-bc49-a96dbe61aadf-0', usage_metadata={'input_tokens': 51, 'output_tokens': 512, 'total_tokens': 563, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = chat.invoke(messages)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91387c13-7707-48e4-ad62-a996b3b15df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolutely! Machine learning is a subset of artificial intelligence that focuses on the development of algorithms and statistical models that enable computers to perform tasks without explicit instructions, relying instead on patterns and inference from data.\n",
      "\n",
      "### Key Concepts in Machine Learning:\n",
      "\n",
      "1. **Types of Machine Learning**:\n",
      "   - **Supervised Learning**: The model is trained on labeled data, which means that the input comes with the corresponding output. The aim is to learn a mapping from inputs to outputs.\n",
      "     - Example: Classification (e.g., email spam detection) and regression (e.g., predicting house prices).\n",
      "     \n",
      "   - **Unsupervised Learning**: The model is trained on data without labeled responses. The goal is to find hidden structures or patterns in the data.\n",
      "     - Example: Clustering (e.g., customer segmentation) and association (e.g., market basket analysis).\n",
      "     \n",
      "   - **Semi-Supervised Learning**: Combines a small amount of labeled data with a large amount of unlabeled data during training.\n",
      "   - **Reinforcement Learning**: The model learns by interacting with an environment and receiving rewards or penalties based on its actions.\n",
      "\n",
      "2. **Key Components**:\n",
      "   - **Data**: The foundation of machine learning; it can be structured (like databases) or unstructured (like text or images).\n",
      "   - **Features**: Individual measurable properties or characteristics used in the model.\n",
      "   - **Model**: The mathematical representation that makes predictions or decisions based on input data.\n",
      "   - **Training**: The process of feeding data into the model to help it learn patterns.\n",
      "   - **Testing/Validation**: Evaluating the model on unseen data to check its performance.\n",
      "\n",
      "3. **Common Algorithms**:\n",
      "   - **Linear Regression**: Used for regression tasks.\n",
      "   - **Logistic Regression**: Used for binary classification.\n",
      "   - **Decision Trees**: A flowchart-like structure for decisions based on features.\n",
      "   - **Support Vector Machines (SVM)**: Used for classification by finding the hyperplane that best separates classes.\n",
      "   - **Neural Networks**: Models inspired by the human brain, commonly used for deep learning tasks like image and speech recognition.\n",
      "\n",
      "4. **Evaluation Metrics**:\n",
      "   - **Accuracy**: The proportion of true results among the total cases.\n",
      "   - **Precision and Recall**: Measures for the performance of classification models, particularly in imbalanced datasets.\n",
      "   - **F1 Score**: A balance between precision and recall.\n",
      "   - **Mean Squared Error (MSE\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b647ef7-13b7-4056-ab3d-8477afcbbd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8475c8cd-d450-4e3e-a0ad-4a812f5a6fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hi AI, how are you today?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"I'm great thank you. How can I help you?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"I'd like to understand machine learning.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Sure! Machine learning (ML) is a subset of artificial intelligence (AI) that focuses on developing algorithms that allow computers to learn from and make predictions or decisions based on data. Here’s a brief overview of some key concepts:\\n\\n### 1. **Types of Machine Learning**:\\n   - **Supervised Learning**: The model is trained on a labeled dataset, meaning it learns from input-output pairs. Common algorithms include linear regression, decision trees, and neural networks.\\n   - **Unsupervised Learning**: The model works with unlabeled data and tries to identify patterns or groupings. Examples include clustering and dimensionality reduction techniques (like k-means and PCA).\\n   - **Semi-supervised Learning**: This combines both labeled and unlabeled data to improve learning accuracy.\\n   - **Reinforcement Learning**: The model learns through trial and error, receiving rewards or penalties based on its actions. It is often used in robotics and game playing.\\n\\n### 2. **Key Concepts**:\\n   - **Features**: Attributes or input variables used by the model to make predictions.\\n   - **Labels**: The output variable that the model is trying to predict in supervised learning.\\n   - **Training**: The process of feeding data to the model so it can learn from it.\\n   - **Testing**: Evaluating the model on unseen data to assess its performance.\\n   - **Overfitting**: A situation where the model learns the training data too well, including its noise and outliers, and performs poorly on new data.\\n   - **Underfitting**: When the model is too simple to capture the underlying patterns in the data.\\n\\n### 3. **Common Algorithms**:\\n   - **Linear Regression**: Used for predicting a continuous output based on linear relationships.\\n   - **Logistic Regression**: Used for binary classification problems.\\n   - **Decision Trees**: A flowchart-like model for decision-making.\\n   - **Support Vector Machines**: Used for classification tasks by finding the hyperplane that best separates classes.\\n   - **Neural Networks**: Models inspired by the human brain, effective for complex tasks like image and speech recognition.\\n\\n### 4. **Steps in a Machine Learning Project**:\\n   - **Define the Problem**: Clearly specify what you want to achieve.\\n   - **Collect Data**: Gather and prepare the data needed for training and testing.\\n   - **Preprocess Data**: Clean, normalize, and transform the data as necessary.\\n   - **Choose a', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 512, 'prompt_tokens': 51, 'total_tokens': 563, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-Bi4euwRK8G02BCXRDCAUQN7rQ8X8o', 'service_tier': 'default', 'finish_reason': 'length', 'logprobs': None}, id='run--5f59bcda-744a-4fd8-897e-9fd1f1a6a0dc-0', usage_metadata={'input_tokens': 51, 'output_tokens': 512, 'total_tokens': 563, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02202f41-a839-4871-ab72-128e3808da12",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=\"Whats the difference between supervised and unsupervised?\"\n",
    ")\n",
    "messages.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "967369c8-629f-4def-86bd-7eddee92e36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "685571e1-5b0e-4180-adf6-905df89504dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main difference between supervised and unsupervised learning lies in the type of data used for training the machine learning models and the goals of each learning approach. Here's a breakdown of the two:\n",
      "\n",
      "### Supervised Learning:\n",
      "1. **Definition**: In supervised learning, the algorithm is trained on a labeled dataset. This means that each training example consists of an input and the corresponding correct output (label).\n",
      "\n",
      "2. **Goal**: The objective is to learn a mapping from inputs to outputs, allowing the model to make predictions or classifications on new, unseen data.\n",
      "\n",
      "3. **Examples**:\n",
      "   - **Regression**: Predicting a continuous value (e.g., predicting house prices based on features like size, location, etc.).\n",
      "   - **Classification**: Assigning labels to categories (e.g., classifying emails as spam or non-spam).\n",
      "\n",
      "4. **Common Algorithms**: Linear regression, logistic regression, decision trees, support vector machines, and neural networks.\n",
      "\n",
      "5. **Data Requirement**: Requires a large amount of labeled data, which can be expensive and time-consuming to obtain.\n",
      "\n",
      "### Unsupervised Learning:\n",
      "1. **Definition**: In unsupervised learning, the algorithm is trained on data without any labeled outputs. It must find patterns, structures, or groupings in the input data by itself.\n",
      "\n",
      "2. **Goal**: The objective is to explore the underlying structure of the data, clustering similar data points or reducing dimensions for visualization and interpretation.\n",
      "\n",
      "3. **Examples**:\n",
      "   - **Clustering**: Grouping similar data points together (e.g., customer segmentation).\n",
      "   - **Dimensionality Reduction**: Reducing the number of features (e.g., PCA - Principal Component Analysis).\n",
      "\n",
      "4. **Common Algorithms**: K-means clustering, hierarchical clustering, Gaussian mixture models, and dimensionality reduction techniques (like PCA).\n",
      "\n",
      "5. **Data Requirement**: Does not require labeled data, making it useful for exploratory data analysis when labels are not available or too costly to produce.\n",
      "\n",
      "### Summary:\n",
      "- **Supervised Learning**: Uses labeled data (input-output pairs) to predict outputs for new inputs.\n",
      "- **Unsupervised Learning**: Uses unlabeled data to find patterns or structures without predefined outputs. \n",
      "\n",
      "In practical applications, both methods serve different purposes and can be used complementarily in the data analysis and model development process.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97078638-fb99-4868-9091-f3f7b31a03d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add latest response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"Pode me falar sobre o clima no Rio de Janeiro hoje?\"\n",
    ")\n",
    "# append to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to GPT\n",
    "res = chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e22c0d9-57ac-48b4-bb82-d19ee34acb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desculpe, mas não consigo acessar informações em tempo real, incluindo dados climáticos atuais. Recomendo que você verifique em um aplicativo de previsão do tempo ou em um site confiável para obter as informações mais recentes sobre o clima no Rio de Janeiro. Se precisar, posso te ajudar com informações gerais sobre o clima dessa região!\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5145b4cc-cfbf-4059-8f18-ae33bc2cbd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexto = [\"Hoje o clima no Rio de Janeiro está ensolarado\"]\n",
    "source_knowledge = \"\\n\".join(contexto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5d64ad3-5df6-4daa-bbb0-4c2af38ed82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Pode me falar sobre o clima no Rio de Janeiro hoje?\"\n",
    "\n",
    "augmented_prompt = f\"\"\"Using the contexts below to answer the question.\n",
    "\n",
    "Contexts:\n",
    "{source_knowledge}\n",
    "\n",
    "Question: {query}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb9d8ec7-a2d2-4d96-bc21-26b42e584076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the contexts below to answer the question.\n",
      "\n",
      "Contexts:\n",
      "Hoje o clima no Rio de Janeiro está ensolarado\n",
      "\n",
      "Question: Pode me falar sobre o clima no Rio de Janeiro hoje?\n"
     ]
    }
   ],
   "source": [
    "print(augmented_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16a06b59-f182-4f94-a234-fa8e072023ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augmented_prompt\n",
    ")\n",
    "\n",
    "messages.append(prompt)\n",
    "\n",
    "res = chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7312776f-b8fe-4cb1-a4e8-91c1a95e196b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hoje o clima no Rio de Janeiro está ensolarado.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7979f62e-96c9-4ad9-91cd-9f3d11db2ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embed_model = OpenAIEmbeddings(model=\"text-embedding-3-small\", openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ac41677-3a8e-4802-9ced-c24761889a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1536)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    'this is one chunk',\n",
    "    'this is the second chunk of text'\n",
    "]\n",
    "\n",
    "res = embed_model.embed_documents(texts)\n",
    "len(res), len(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ea18d1-c33b-47d2-ab84-7a5f50dfc48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac680c80-db7f-4925-b3d7-7147000724a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\abrantes\\InstallAnaconda\\envs\\academia3_13\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n",
       "    num_rows: 25\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tqdm as notebook_tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"infoslack/mistral-7b-arxiv-paper-chunked\", split=\"train\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e041dd5-11da-48f7-9325-bf728f32be3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doi': '2310.06825',\n",
       " 'chunk-id': '0',\n",
       " 'chunk': 'Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src',\n",
       " 'id': '2310.06825',\n",
       " 'title': 'Mistral 7B',\n",
       " 'summary': 'We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\\nfor superior performance and efficiency. Mistral 7B outperforms Llama 2 13B\\nacross all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\\ncode generation. Our model leverages grouped-query attention (GQA) for faster\\ninference, coupled with sliding window attention (SWA) to effectively handle\\nsequences of arbitrary length with a reduced inference cost. We also provide a\\nmodel fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses\\nthe Llama 2 13B -- Chat model both on human and automated benchmarks. Our\\nmodels are released under the Apache 2.0 license.',\n",
       " 'source': 'http://arxiv.org/pdf/2310.06825',\n",
       " 'authors': ['Albert Q. Jiang',\n",
       "  'Alexandre Sablayrolles',\n",
       "  'Arthur Mensch',\n",
       "  'Chris Bamford',\n",
       "  'Devendra Singh Chaplot',\n",
       "  'Diego de las Casas',\n",
       "  'Florian Bressand',\n",
       "  'Gianna Lengyel',\n",
       "  'Guillaume Lample',\n",
       "  'Lucile Saulnier',\n",
       "  'Lélio Renard Lavaud',\n",
       "  'Marie-Anne Lachaux',\n",
       "  'Pierre Stock',\n",
       "  'Teven Le Scao',\n",
       "  'Thibaut Lavril',\n",
       "  'Thomas Wang',\n",
       "  'Timothée Lacroix',\n",
       "  'William El Sayed'],\n",
       " 'categories': ['cs.CL', 'cs.AI', 'cs.LG'],\n",
       " 'comment': 'Models and code are available at\\n  https://mistral.ai/news/announcing-mistral-7b/',\n",
       " 'journal_ref': None,\n",
       " 'primary_category': 'cs.CL',\n",
       " 'published': '20231010',\n",
       " 'updated': '20231010',\n",
       " 'references': [{'id': '1808.07036'},\n",
       "  {'id': '1809.02789'},\n",
       "  {'id': '1904.10509'},\n",
       "  {'id': '2302.13971'},\n",
       "  {'id': '2009.03300'},\n",
       "  {'id': '2305.13245'},\n",
       "  {'id': '1904.09728'},\n",
       "  {'id': '1803.05457'},\n",
       "  {'id': '2103.03874'},\n",
       "  {'id': '1905.07830'},\n",
       "  {'id': '2308.12950'},\n",
       "  {'id': '2210.09261'},\n",
       "  {'id': '2310.06825'},\n",
       "  {'id': '2307.09288'},\n",
       "  {'id': '2304.06364'},\n",
       "  {'id': '1905.10044'},\n",
       "  {'id': '2110.14168'},\n",
       "  {'id': '2108.07732'},\n",
       "  {'id': '2107.03374'},\n",
       "  {'id': '1811.00937'},\n",
       "  {'id': '2004.05150'},\n",
       "  {'id': '1705.03551'}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3df3cbff-614f-4cf3-b547-1d92481c060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc2a78c8-5acc-4625-990b-2141cfbab652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doi</th>\n",
       "      <th>chunk-id</th>\n",
       "      <th>chunk</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>source</th>\n",
       "      <th>authors</th>\n",
       "      <th>categories</th>\n",
       "      <th>comment</th>\n",
       "      <th>journal_ref</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>published</th>\n",
       "      <th>updated</th>\n",
       "      <th>references</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2310.06825</td>\n",
       "      <td>0</td>\n",
       "      <td>Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayr...</td>\n",
       "      <td>2310.06825</td>\n",
       "      <td>Mistral 7B</td>\n",
       "      <td>We introduce Mistral 7B v0.1, a 7-billion-para...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "      <td>[Albert Q. Jiang, Alexandre Sablayrolles, Arth...</td>\n",
       "      <td>[cs.CL, cs.AI, cs.LG]</td>\n",
       "      <td>Models and code are available at\\n  https://mi...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20231010</td>\n",
       "      <td>20231010</td>\n",
       "      <td>[{'id': '1808.07036'}, {'id': '1809.02789'}, {...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2310.06825</td>\n",
       "      <td>1</td>\n",
       "      <td>automated benchmarks. Our models are released ...</td>\n",
       "      <td>2310.06825</td>\n",
       "      <td>Mistral 7B</td>\n",
       "      <td>We introduce Mistral 7B v0.1, a 7-billion-para...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "      <td>[Albert Q. Jiang, Alexandre Sablayrolles, Arth...</td>\n",
       "      <td>[cs.CL, cs.AI, cs.LG]</td>\n",
       "      <td>Models and code are available at\\n  https://mi...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20231010</td>\n",
       "      <td>20231010</td>\n",
       "      <td>[{'id': '1808.07036'}, {'id': '1809.02789'}, {...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2310.06825</td>\n",
       "      <td>2</td>\n",
       "      <td>GQA significantly accelerates the inference sp...</td>\n",
       "      <td>2310.06825</td>\n",
       "      <td>Mistral 7B</td>\n",
       "      <td>We introduce Mistral 7B v0.1, a 7-billion-para...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "      <td>[Albert Q. Jiang, Alexandre Sablayrolles, Arth...</td>\n",
       "      <td>[cs.CL, cs.AI, cs.LG]</td>\n",
       "      <td>Models and code are available at\\n  https://mi...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20231010</td>\n",
       "      <td>20231010</td>\n",
       "      <td>[{'id': '1808.07036'}, {'id': '1809.02789'}, {...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2310.06825</td>\n",
       "      <td>3</td>\n",
       "      <td>Mistral 7B takes a significant step in balanci...</td>\n",
       "      <td>2310.06825</td>\n",
       "      <td>Mistral 7B</td>\n",
       "      <td>We introduce Mistral 7B v0.1, a 7-billion-para...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "      <td>[Albert Q. Jiang, Alexandre Sablayrolles, Arth...</td>\n",
       "      <td>[cs.CL, cs.AI, cs.LG]</td>\n",
       "      <td>Models and code are available at\\n  https://mi...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20231010</td>\n",
       "      <td>20231010</td>\n",
       "      <td>[{'id': '1808.07036'}, {'id': '1809.02789'}, {...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2310.06825</td>\n",
       "      <td>4</td>\n",
       "      <td>parameters of the architecture are summarized ...</td>\n",
       "      <td>2310.06825</td>\n",
       "      <td>Mistral 7B</td>\n",
       "      <td>We introduce Mistral 7B v0.1, a 7-billion-para...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "      <td>[Albert Q. Jiang, Alexandre Sablayrolles, Arth...</td>\n",
       "      <td>[cs.CL, cs.AI, cs.LG]</td>\n",
       "      <td>Models and code are available at\\n  https://mi...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20231010</td>\n",
       "      <td>20231010</td>\n",
       "      <td>[{'id': '1808.07036'}, {'id': '1809.02789'}, {...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          doi chunk-id                                              chunk  \\\n",
       "0  2310.06825        0  Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayr...   \n",
       "1  2310.06825        1  automated benchmarks. Our models are released ...   \n",
       "2  2310.06825        2  GQA significantly accelerates the inference sp...   \n",
       "3  2310.06825        3  Mistral 7B takes a significant step in balanci...   \n",
       "4  2310.06825        4  parameters of the architecture are summarized ...   \n",
       "\n",
       "           id       title                                            summary  \\\n",
       "0  2310.06825  Mistral 7B  We introduce Mistral 7B v0.1, a 7-billion-para...   \n",
       "1  2310.06825  Mistral 7B  We introduce Mistral 7B v0.1, a 7-billion-para...   \n",
       "2  2310.06825  Mistral 7B  We introduce Mistral 7B v0.1, a 7-billion-para...   \n",
       "3  2310.06825  Mistral 7B  We introduce Mistral 7B v0.1, a 7-billion-para...   \n",
       "4  2310.06825  Mistral 7B  We introduce Mistral 7B v0.1, a 7-billion-para...   \n",
       "\n",
       "                            source  \\\n",
       "0  http://arxiv.org/pdf/2310.06825   \n",
       "1  http://arxiv.org/pdf/2310.06825   \n",
       "2  http://arxiv.org/pdf/2310.06825   \n",
       "3  http://arxiv.org/pdf/2310.06825   \n",
       "4  http://arxiv.org/pdf/2310.06825   \n",
       "\n",
       "                                             authors             categories  \\\n",
       "0  [Albert Q. Jiang, Alexandre Sablayrolles, Arth...  [cs.CL, cs.AI, cs.LG]   \n",
       "1  [Albert Q. Jiang, Alexandre Sablayrolles, Arth...  [cs.CL, cs.AI, cs.LG]   \n",
       "2  [Albert Q. Jiang, Alexandre Sablayrolles, Arth...  [cs.CL, cs.AI, cs.LG]   \n",
       "3  [Albert Q. Jiang, Alexandre Sablayrolles, Arth...  [cs.CL, cs.AI, cs.LG]   \n",
       "4  [Albert Q. Jiang, Alexandre Sablayrolles, Arth...  [cs.CL, cs.AI, cs.LG]   \n",
       "\n",
       "                                             comment journal_ref  \\\n",
       "0  Models and code are available at\\n  https://mi...        None   \n",
       "1  Models and code are available at\\n  https://mi...        None   \n",
       "2  Models and code are available at\\n  https://mi...        None   \n",
       "3  Models and code are available at\\n  https://mi...        None   \n",
       "4  Models and code are available at\\n  https://mi...        None   \n",
       "\n",
       "  primary_category published   updated  \\\n",
       "0            cs.CL  20231010  20231010   \n",
       "1            cs.CL  20231010  20231010   \n",
       "2            cs.CL  20231010  20231010   \n",
       "3            cs.CL  20231010  20231010   \n",
       "4            cs.CL  20231010  20231010   \n",
       "\n",
       "                                          references  \n",
       "0  [{'id': '1808.07036'}, {'id': '1809.02789'}, {...  \n",
       "1  [{'id': '1808.07036'}, {'id': '1809.02789'}, {...  \n",
       "2  [{'id': '1808.07036'}, {'id': '1809.02789'}, {...  \n",
       "3  [{'id': '1808.07036'}, {'id': '1809.02789'}, {...  \n",
       "4  [{'id': '1808.07036'}, {'id': '1809.02789'}, {...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86d1699d-4010-4c54-b769-cba216878984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayr...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>automated benchmarks. Our models are released ...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GQA significantly accelerates the inference sp...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mistral 7B takes a significant step in balanci...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>parameters of the architecture are summarized ...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               chunk  \\\n",
       "0  Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayr...   \n",
       "1  automated benchmarks. Our models are released ...   \n",
       "2  GQA significantly accelerates the inference sp...   \n",
       "3  Mistral 7B takes a significant step in balanci...   \n",
       "4  parameters of the architecture are summarized ...   \n",
       "\n",
       "                            source  \n",
       "0  http://arxiv.org/pdf/2310.06825  \n",
       "1  http://arxiv.org/pdf/2310.06825  \n",
       "2  http://arxiv.org/pdf/2310.06825  \n",
       "3  http://arxiv.org/pdf/2310.06825  \n",
       "4  http://arxiv.org/pdf/2310.06825  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = data[['chunk', 'source']]\n",
    "docs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfe399e6-e0a7-4f98-88dc-ecdc28d445bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce99f4d0-391c-41f1-95fa-840d34668a71",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02c04511-834c-45ca-b6a6-223d2bfb66ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "\n",
    "loader = DataFrameLoader(docs, page_content_column=\"chunk\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2240c2f7-0339-471b-9e40-09882fc503aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'http://arxiv.org/pdf/2310.06825'}, page_content='Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2325928d-285f-4781-8c18-c6dcaaed8907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'http://arxiv.org/pdf/2310.06825'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec4bc1df-26b9-489c-b121-147a7fc609af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collections=[]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", openai_api_key=openai_api_key)\n",
    "\n",
    "url = os.getenv(\"QDRANT_URL\")\n",
    "api_key = os.getenv(\"QDRANT_API_KEY\")\n",
    "\n",
    "# https://cloud.qdrant.io/\n",
    "# crie um cluster free: langchain\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "qdrant_host=\"https://8bebc27e-a00d-45ad-bddf-dc91eeee5cd1.us-east4-0.gcp.cloud.qdrant.io:6333\"\n",
    "qdrant_api_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.3TIbXfah514A3sPTgAAYDoAxQRTPooJD5H0xb30yhRM\"\n",
    "collection_name=\"langchain\"\n",
    "\n",
    "qdrant_client = QdrantClient(\n",
    "    url=\"https://8bebc27e-a00d-45ad-bddf-dc91eeee5cd1.us-east4-0.gcp.cloud.qdrant.io:6333\", \n",
    "    api_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.3TIbXfah514A3sPTgAAYDoAxQRTPooJD5H0xb30yhRM\",\n",
    ")\n",
    "\n",
    "print(qdrant_client.get_collections())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e31d978-69bd-4e3e-ac3d-27a7bd393855",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Qdrant\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "# Parâmetros de conexão remota (use seu endpoint e API key se necessário)\n",
    "# qdrant_host = \"https://SEU_ENDPOINT.cloud.qdrant.io\"\n",
    "# qdrant_api_key = \"SUA_API_KEY\"\n",
    "# collection_name = \"minha_colecao\"\n",
    "\n",
    "# Inicializa o cliente Qdrant remoto\n",
    "client = QdrantClient(\n",
    "    url=qdrant_host,\n",
    "    api_key=qdrant_api_key,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0141118a-72cc-4a21-9519-d04030ee3974",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", dimensions=1536, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "040a0b17-f06d-4c1c-886e-f0fdbd3a5c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\abrantes\\AppData\\Local\\Temp\\ipykernel_8724\\2771757229.py:3: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    }
   ],
   "source": [
    "# Verifica se a coleção existe, se não, cria\n",
    "if collection_name not in [col.name for col in client.get_collections().collections]:\n",
    "    client.recreate_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a81c634-0fa2-4270-a07b-c95e6c8171bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\abrantes\\AppData\\Local\\Temp\\ipykernel_8724\\1861352040.py:2: LangChainDeprecationWarning: The class `Qdrant` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-qdrant package and should be used instead. To use it run `pip install -U :class:`~langchain-qdrant` and import as `from :class:`~langchain_qdrant import Qdrant``.\n",
      "  vectorstore = Qdrant(\n"
     ]
    }
   ],
   "source": [
    "# Cria um vectorstore do Langchain com Qdrant\n",
    "vectorstore = Qdrant(\n",
    "    client=client,\n",
    "    collection_name=collection_name,\n",
    "    embeddings=embeddings,\n",
    ")\n",
    "\n",
    "# Adiciona documentos ao Qdrant\n",
    "texts = [\n",
    "    \"O céu é azul e bonito.\",\n",
    "    \"Python é uma ótima linguagem de programação.\",\n",
    "    \"A IA está mudando o mundo rapidamente.\",\n",
    "]\n",
    "metadatas = [{\"tema\": \"natureza\"}, {\"tema\": \"tecnologia\"}, {\"tema\": \"inovação\"}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2820df0c-d82d-4359-a3a5-2eee43b76a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fa6ecc603fbf48debc92ba339a00697c',\n",
       " 'e46abdb724274976946cb6d304f3dfb0',\n",
       " '4001797b0a7444469f99f91be898698a']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adiciona ao vectorstore\n",
    "vectorstore.add_texts(texts, metadatas=metadatas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2fbb3d64-5780-4396-ab1c-58cc4c936dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto: Python é uma ótima linguagem de programação. | Metadata: {'tema': 'tecnologia', '_id': 'e46abdb7-2427-4976-946c-b6d304f3dfb0', '_collection_name': 'langchain'}\n",
      "Texto: O céu é azul e bonito. | Metadata: {'tema': 'natureza', '_id': 'fa6ecc60-3fbf-48de-bc92-ba339a00697c', '_collection_name': 'langchain'}\n"
     ]
    }
   ],
   "source": [
    "# Faz uma busca semântica\n",
    "query = \"Qual linguagem é boa para programar?\"\n",
    "results = vectorstore.similarity_search(query, k=2)\n",
    "\n",
    "# Mostra os resultados\n",
    "for r in results:\n",
    "    print(f\"Texto: {r.page_content} | Metadata: {r.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dff0621a-e0b2-40df-9a4a-2b45ff321764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collections=[CollectionDescription(name='langchain')]\n"
     ]
    }
   ],
   "source": [
    "print(qdrant_client.get_collections())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "80e1377b-8197-4383-823d-8d77d1976780",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant = Qdrant.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    "    url=qdrant_host,\n",
    "    api_key=qdrant_api_key,\n",
    "    collection_name=\"langchain\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "600e3615-f2c7-4175-ad21-cdc7b1f69b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'http://arxiv.org/pdf/2310.06825', '_id': '8e9428d0-75c9-4fc5-b1ab-ddf494801194', '_collection_name': 'langchain'}, page_content='Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2310.06825', '_id': '10d0f939-df2d-49ff-a3ce-3041bf7bae71', '_collection_name': 'langchain'}, page_content='GQA significantly accelerates the inference speed, and also reduces the memory requirement during\\ndecoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time\\napplications. In addition, SWA is designed to handle longer sequences more effectively at a reduced\\ncomputational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms\\ncollectively contribute to the enhanced performance and efficiency of Mistral 7B.arXiv:2310.06825v1  [cs.CL]  10 Oct 2023\\nMistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference\\nimplementation1facilitating easy deployment either locally or on cloud platforms such as AWS, GCP,\\nor Azure using the vLLM [ 17] inference server and SkyPilot2. Integration with Hugging Face3is\\nalso streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across\\na myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat\\nmodel fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B – Chat model.\\nMistral 7B takes a significant step in balancing the goals of getting high performance while keeping\\nlarge language models efficient. Through our work, our aim is to help the community create more'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2310.06825', '_id': '21bc5fb6-ad7b-4c6f-8fd4-c50eb7ac62c3', '_collection_name': 'langchain'}, page_content='automated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src\\nWebpage: https://mistral.ai/news/announcing-mistral-7b/\\n1 Introduction\\nIn the rapidly evolving domain of Natural Language Processing (NLP), the race towards higher model\\nperformance often necessitates an escalation in model size. However, this scaling tends to increase\\ncomputational costs and inference latency, thereby raising barriers to deployment in practical,\\nreal-world scenarios. In this context, the search for balanced models delivering both high-level\\nperformance and efficiency becomes critically essential. Our model, Mistral 7B, demonstrates that\\na carefully designed language model can deliver high performance while maintaining an efficient\\ninference. Mistral 7B outperforms the previous best 13B model (Llama 2, [ 26]) across all tested\\nbenchmarks, and surpasses the best 34B model (LLaMa 34B, [ 25]) in mathematics and code\\ngeneration. Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B [ 20],\\nwithout sacrificing performance on non-code related benchmarks.\\nMistral 7B leverages grouped-query attention (GQA) [ 1], and sliding window attention (SWA) [ 6,3].\\nGQA significantly accelerates the inference speed, and also reduces the memory requirement during')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"O que há de tão especial no Mistral 7B? Responda em portugues em um único parágrafo\"\n",
    "qdrant.similarity_search(query, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eabeb994-35c4-421a-8f76-559826ec87d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_prompt(query: str):\n",
    "    results = qdrant.similarity_search(query, k=3)\n",
    "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "    augment_prompt = f\"\"\"Using the contexts below, answer the query:\n",
    "\n",
    "    Contexts:\n",
    "    {source_knowledge}\n",
    "\n",
    "    Query: {query}\"\"\"\n",
    "    return augment_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9529893-611d-4db8-8f30-8117ecd897ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the contexts below, answer the query:\n",
      "\n",
      "    Contexts:\n",
      "    Mistral 7B\n",
      "Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\n",
      "Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\n",
      "Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\n",
      "Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\n",
      "William El Sayed\n",
      "Abstract\n",
      "We introduce Mistral 7B, a 7–billion-parameter language model engineered for\n",
      "superior performance and efficiency. Mistral 7B outperforms the best open 13B\n",
      "model (Llama 2) across all evaluated benchmarks, and the best released 34B\n",
      "model (Llama 1) in reasoning, mathematics, and code generation. Our model\n",
      "leverages grouped-query attention (GQA) for faster inference, coupled with sliding\n",
      "window attention (SWA) to effectively handle sequences of arbitrary length with a\n",
      "reduced inference cost. We also provide a model fine-tuned to follow instructions,\n",
      "Mistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\n",
      "automated benchmarks. Our models are released under the Apache 2.0 license.\n",
      "Code: https://github.com/mistralai/mistral-src\n",
      "GQA significantly accelerates the inference speed, and also reduces the memory requirement during\n",
      "decoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time\n",
      "applications. In addition, SWA is designed to handle longer sequences more effectively at a reduced\n",
      "computational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms\n",
      "collectively contribute to the enhanced performance and efficiency of Mistral 7B.arXiv:2310.06825v1  [cs.CL]  10 Oct 2023\n",
      "Mistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference\n",
      "implementation1facilitating easy deployment either locally or on cloud platforms such as AWS, GCP,\n",
      "or Azure using the vLLM [ 17] inference server and SkyPilot2. Integration with Hugging Face3is\n",
      "also streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across\n",
      "a myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat\n",
      "model fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B – Chat model.\n",
      "Mistral 7B takes a significant step in balancing the goals of getting high performance while keeping\n",
      "large language models efficient. Through our work, our aim is to help the community create more\n",
      "automated benchmarks. Our models are released under the Apache 2.0 license.\n",
      "Code: https://github.com/mistralai/mistral-src\n",
      "Webpage: https://mistral.ai/news/announcing-mistral-7b/\n",
      "1 Introduction\n",
      "In the rapidly evolving domain of Natural Language Processing (NLP), the race towards higher model\n",
      "performance often necessitates an escalation in model size. However, this scaling tends to increase\n",
      "computational costs and inference latency, thereby raising barriers to deployment in practical,\n",
      "real-world scenarios. In this context, the search for balanced models delivering both high-level\n",
      "performance and efficiency becomes critically essential. Our model, Mistral 7B, demonstrates that\n",
      "a carefully designed language model can deliver high performance while maintaining an efficient\n",
      "inference. Mistral 7B outperforms the previous best 13B model (Llama 2, [ 26]) across all tested\n",
      "benchmarks, and surpasses the best 34B model (LLaMa 34B, [ 25]) in mathematics and code\n",
      "generation. Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B [ 20],\n",
      "without sacrificing performance on non-code related benchmarks.\n",
      "Mistral 7B leverages grouped-query attention (GQA) [ 1], and sliding window attention (SWA) [ 6,3].\n",
      "GQA significantly accelerates the inference speed, and also reduces the memory requirement during\n",
      "\n",
      "    Query: O que há de tão especial no Mistral 7B? Responda em portugues em um único parágrafo\n"
     ]
    }
   ],
   "source": [
    "print(custom_prompt(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0b58d584-eed7-40b4-a762-aad0f4ce2cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O Mistral 7B é um modelo de linguagem com 7 bilhões de parâmetros projetado para oferecer desempenho superior e eficiência, superando o melhor modelo aberto de 13 bilhões de parâmetros (Llama 2) em todos os benchmarks avaliados, além de se destacar em raciocínio, matemática e geração de código em relação ao modelo de 34 bilhões (Llama 1). O modelo utiliza mecanismos de atenção como a atenção de consultas agrupadas (GQA) para acelerar a inferência e reduzir requisitos de memória, enquanto a atenção de janela deslizante (SWA) permite lidar com sequências mais longas de forma mais eficaz. Isso possibilita um maior desempenho em aplicações em tempo real, mantendo baixa latência e custos computacionais, e é lançado sob a licença Apache 2.0, facilitando integração e fine-tuning em diversas tarefas.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=custom_prompt(query)\n",
    ")\n",
    "\n",
    "messages.append(prompt)\n",
    "\n",
    "res = chat.invoke(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ab60f0-d513-4de5-84e4-b65559bd9c12",
   "metadata": {},
   "source": [
    "## Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cc72e141-52dd-4377-9ee2-b239af041c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "chat = ChatGroq(temperature=0, model_name=\"llama-3.3-70b-versatile\", max_tokens=512, api_key=groq_api_key)\n",
    "\n",
    "# modelos deprecados devem ser substituídos: https://console.groq.com/docs/deprecations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b452f061-ecf8-4c40-b5e4-a606892adee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O Mistral 7B é um modelo de linguagem de 7 bilhões de parâmetros projetado para desempenho superior e eficiência. Ele supera o melhor modelo de 13 bilhões de parâmetros (Llama 2) em todos os benchmarks avaliados e o melhor modelo de 34 bilhões de parâmetros (Llama 1) em raciocínio, matemática e geração de código. Além disso, o Mistral 7B utiliza mecanismos de atenção em grupo (GQA) e janela deslizante (SWA) para acelerar a inferência e reduzir os requisitos de memória, tornando-o mais eficiente e escalável. Essas características tornam o Mistral 7B um modelo de linguagem avançado e eficiente, capaz de lidar com sequências de comprimento arbitrário e realizar tarefas complexas com alta precisão.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=custom_prompt(query)\n",
    ")\n",
    "\n",
    "messages.append(prompt)\n",
    "res = chat.invoke(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3ba80bd2-8d85-404c-af21-a1d0349753f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hi AI, how are you today?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"I'm great thank you. How can I help you?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"I'd like to understand machine learning.\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Using the contexts below, answer the query:\\n\\n    Contexts:\\n    Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src\\nGQA significantly accelerates the inference speed, and also reduces the memory requirement during\\ndecoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time\\napplications. In addition, SWA is designed to handle longer sequences more effectively at a reduced\\ncomputational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms\\ncollectively contribute to the enhanced performance and efficiency of Mistral 7B.arXiv:2310.06825v1  [cs.CL]  10 Oct 2023\\nMistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference\\nimplementation1facilitating easy deployment either locally or on cloud platforms such as AWS, GCP,\\nor Azure using the vLLM [ 17] inference server and SkyPilot2. Integration with Hugging Face3is\\nalso streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across\\na myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat\\nmodel fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B – Chat model.\\nMistral 7B takes a significant step in balancing the goals of getting high performance while keeping\\nlarge language models efficient. Through our work, our aim is to help the community create more\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src\\nWebpage: https://mistral.ai/news/announcing-mistral-7b/\\n1 Introduction\\nIn the rapidly evolving domain of Natural Language Processing (NLP), the race towards higher model\\nperformance often necessitates an escalation in model size. However, this scaling tends to increase\\ncomputational costs and inference latency, thereby raising barriers to deployment in practical,\\nreal-world scenarios. In this context, the search for balanced models delivering both high-level\\nperformance and efficiency becomes critically essential. Our model, Mistral 7B, demonstrates that\\na carefully designed language model can deliver high performance while maintaining an efficient\\ninference. Mistral 7B outperforms the previous best 13B model (Llama 2, [ 26]) across all tested\\nbenchmarks, and surpasses the best 34B model (LLaMa 34B, [ 25]) in mathematics and code\\ngeneration. Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B [ 20],\\nwithout sacrificing performance on non-code related benchmarks.\\nMistral 7B leverages grouped-query attention (GQA) [ 1], and sliding window attention (SWA) [ 6,3].\\nGQA significantly accelerates the inference speed, and also reduces the memory requirement during\\n\\n    Query: O que há de tão especial no Mistral 7B? Responda em portugues em um único parágrafo', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Using the contexts below, answer the query:\\n\\n    Contexts:\\n    Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src\\nGQA significantly accelerates the inference speed, and also reduces the memory requirement during\\ndecoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time\\napplications. In addition, SWA is designed to handle longer sequences more effectively at a reduced\\ncomputational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms\\ncollectively contribute to the enhanced performance and efficiency of Mistral 7B.arXiv:2310.06825v1  [cs.CL]  10 Oct 2023\\nMistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference\\nimplementation1facilitating easy deployment either locally or on cloud platforms such as AWS, GCP,\\nor Azure using the vLLM [ 17] inference server and SkyPilot2. Integration with Hugging Face3is\\nalso streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across\\na myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat\\nmodel fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B – Chat model.\\nMistral 7B takes a significant step in balancing the goals of getting high performance while keeping\\nlarge language models efficient. Through our work, our aim is to help the community create more\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src\\nWebpage: https://mistral.ai/news/announcing-mistral-7b/\\n1 Introduction\\nIn the rapidly evolving domain of Natural Language Processing (NLP), the race towards higher model\\nperformance often necessitates an escalation in model size. However, this scaling tends to increase\\ncomputational costs and inference latency, thereby raising barriers to deployment in practical,\\nreal-world scenarios. In this context, the search for balanced models delivering both high-level\\nperformance and efficiency becomes critically essential. Our model, Mistral 7B, demonstrates that\\na carefully designed language model can deliver high performance while maintaining an efficient\\ninference. Mistral 7B outperforms the previous best 13B model (Llama 2, [ 26]) across all tested\\nbenchmarks, and surpasses the best 34B model (LLaMa 34B, [ 25]) in mathematics and code\\ngeneration. Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B [ 20],\\nwithout sacrificing performance on non-code related benchmarks.\\nMistral 7B leverages grouped-query attention (GQA) [ 1], and sliding window attention (SWA) [ 6,3].\\nGQA significantly accelerates the inference speed, and also reduces the memory requirement during\\n\\n    Query: O que há de tão especial no Mistral 7B? Responda em portugues em um único parágrafo', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Using the contexts below, answer the query:\\n\\n    Contexts:\\n    Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src\\nGQA significantly accelerates the inference speed, and also reduces the memory requirement during\\ndecoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time\\napplications. In addition, SWA is designed to handle longer sequences more effectively at a reduced\\ncomputational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms\\ncollectively contribute to the enhanced performance and efficiency of Mistral 7B.arXiv:2310.06825v1  [cs.CL]  10 Oct 2023\\nMistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference\\nimplementation1facilitating easy deployment either locally or on cloud platforms such as AWS, GCP,\\nor Azure using the vLLM [ 17] inference server and SkyPilot2. Integration with Hugging Face3is\\nalso streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across\\na myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat\\nmodel fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B – Chat model.\\nMistral 7B takes a significant step in balancing the goals of getting high performance while keeping\\nlarge language models efficient. Through our work, our aim is to help the community create more\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src\\nWebpage: https://mistral.ai/news/announcing-mistral-7b/\\n1 Introduction\\nIn the rapidly evolving domain of Natural Language Processing (NLP), the race towards higher model\\nperformance often necessitates an escalation in model size. However, this scaling tends to increase\\ncomputational costs and inference latency, thereby raising barriers to deployment in practical,\\nreal-world scenarios. In this context, the search for balanced models delivering both high-level\\nperformance and efficiency becomes critically essential. Our model, Mistral 7B, demonstrates that\\na carefully designed language model can deliver high performance while maintaining an efficient\\ninference. Mistral 7B outperforms the previous best 13B model (Llama 2, [ 26]) across all tested\\nbenchmarks, and surpasses the best 34B model (LLaMa 34B, [ 25]) in mathematics and code\\ngeneration. Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B [ 20],\\nwithout sacrificing performance on non-code related benchmarks.\\nMistral 7B leverages grouped-query attention (GQA) [ 1], and sliding window attention (SWA) [ 6,3].\\nGQA significantly accelerates the inference speed, and also reduces the memory requirement during\\n\\n    Query: O que há de tão especial no Mistral 7B? Responda em portugues em um único parágrafo', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e542fcf-2f02-4b13-bdb2-ae898f11ed43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
