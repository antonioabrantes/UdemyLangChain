{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "938f8ec9-e319-473f-86db-bfc49ac305aa",
   "metadata": {},
   "source": [
    "## Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2c1f76-b4f8-4241-a63d-bd59f19a940d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain\n",
    "#pip install langchain-openai\n",
    "#pip install langchain_community\n",
    "#pip install langchain-groq\n",
    "#pip install datasets\n",
    "#pip install qdrant-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69ba2e2b-87f2-4820-952c-15ee62d1af93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/docs/integrations/llms/openai/\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "load_dotenv('./.env')\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8aa0b34-9182-498d-b63f-678203e99b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(\n",
    "    model='gpt-4o-mini',\n",
    "    openai_api_key = openai_api_key,\n",
    "    max_tokens = 512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f52c263-2b0e-4acd-95e7-e6a04056a0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
    "    HumanMessage(content=\"I'd like to understand machine learning.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "940dfe9d-c6d5-4e75-ab80-1e5e03c7f227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Absolutely! Machine learning is a subset of artificial intelligence (AI) that focuses on the development of algorithms and statistical models that enable computers to perform specific tasks without using explicit instructions. Instead, they rely on patterns and inference from data.\\n\\nHere are some key concepts to help you understand machine learning better:\\n\\n### 1. **Types of Machine Learning:**\\n   - **Supervised Learning:** The model is trained on a labeled dataset, meaning the input comes with the correct output. The model learns to map inputs to outputs. Examples include classification and regression tasks.\\n   - **Unsupervised Learning:** The model works with unlabeled data and tries to find patterns or groupings in the data. Common techniques include clustering and dimensionality reduction.\\n   - **Semi-Supervised Learning:** A combination of supervised and unsupervised learning where the model is trained on a small amount of labeled data and a large amount of unlabeled data.\\n   - **Reinforcement Learning:** The model learns by interacting with an environment and receiving feedback in the form of rewards or penalties.\\n\\n### 2. **Key Terminology:**\\n   - **Dataset:** The collection of data used for training and testing the model.\\n   - **Features:** The individual measurable properties or characteristics of the data used as input for the model.\\n   - **Labels:** The output or target variable that the model is trying to predict in supervised learning.\\n   - **Model:** A mathematical representation of a process that makes predictions based on input data.\\n   - **Training:** The process of teaching the model using a dataset.\\n   - **Testing:** Evaluating the model's performance on a separate dataset that it hasn't seen before.\\n\\n### 3. **Common Algorithms:**\\n   - **Linear Regression:** Used for regression tasks to model the relationship between a dependent variable and one or more independent variables.\\n   - **Logistic Regression:** Used for binary classification tasks.\\n   - **Decision Trees:** A model that makes decisions based on answering a series of questions about the features.\\n   - **Support Vector Machines (SVM):** Used for classification tasks by finding the hyperplane that best separates different classes.\\n   - **Neural Networks:** Computational models inspired by the human brain that are particularly good at recognizing patterns in large datasets.\\n\\n### 4. **Steps in a Machine Learning Project:**\\n   1. **Define the Problem:** Clearly articulate what you want to solve.\\n   2. **Collect Data:** Gather the data needed for training and testing.\\n   3. **Preprocess Data\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 512, 'prompt_tokens': 51, 'total_tokens': 563, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'length', 'logprobs': None}, id='run-37bd960e-b627-4685-a81d-6a3363196522-0', usage_metadata={'input_tokens': 51, 'output_tokens': 512, 'total_tokens': 563, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = chat.invoke(messages)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91387c13-7707-48e4-ad62-a996b3b15df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolutely! Machine learning is a subset of artificial intelligence (AI) that focuses on the development of algorithms and statistical models that enable computers to perform specific tasks without using explicit instructions. Instead, they rely on patterns and inference from data.\n",
      "\n",
      "Here are some key concepts to help you understand machine learning better:\n",
      "\n",
      "### 1. **Types of Machine Learning:**\n",
      "   - **Supervised Learning:** The model is trained on a labeled dataset, meaning the input comes with the correct output. The model learns to map inputs to outputs. Examples include classification and regression tasks.\n",
      "   - **Unsupervised Learning:** The model works with unlabeled data and tries to find patterns or groupings in the data. Common techniques include clustering and dimensionality reduction.\n",
      "   - **Semi-Supervised Learning:** A combination of supervised and unsupervised learning where the model is trained on a small amount of labeled data and a large amount of unlabeled data.\n",
      "   - **Reinforcement Learning:** The model learns by interacting with an environment and receiving feedback in the form of rewards or penalties.\n",
      "\n",
      "### 2. **Key Terminology:**\n",
      "   - **Dataset:** The collection of data used for training and testing the model.\n",
      "   - **Features:** The individual measurable properties or characteristics of the data used as input for the model.\n",
      "   - **Labels:** The output or target variable that the model is trying to predict in supervised learning.\n",
      "   - **Model:** A mathematical representation of a process that makes predictions based on input data.\n",
      "   - **Training:** The process of teaching the model using a dataset.\n",
      "   - **Testing:** Evaluating the model's performance on a separate dataset that it hasn't seen before.\n",
      "\n",
      "### 3. **Common Algorithms:**\n",
      "   - **Linear Regression:** Used for regression tasks to model the relationship between a dependent variable and one or more independent variables.\n",
      "   - **Logistic Regression:** Used for binary classification tasks.\n",
      "   - **Decision Trees:** A model that makes decisions based on answering a series of questions about the features.\n",
      "   - **Support Vector Machines (SVM):** Used for classification tasks by finding the hyperplane that best separates different classes.\n",
      "   - **Neural Networks:** Computational models inspired by the human brain that are particularly good at recognizing patterns in large datasets.\n",
      "\n",
      "### 4. **Steps in a Machine Learning Project:**\n",
      "   1. **Define the Problem:** Clearly articulate what you want to solve.\n",
      "   2. **Collect Data:** Gather the data needed for training and testing.\n",
      "   3. **Preprocess Data\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b647ef7-13b7-4056-ab3d-8477afcbbd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8475c8cd-d450-4e3e-a0ad-4a812f5a6fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hi AI, how are you today?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"I'm great thank you. How can I help you?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"I'd like to understand machine learning.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Absolutely! Machine learning is a subset of artificial intelligence (AI) that focuses on the development of algorithms and statistical models that enable computers to perform specific tasks without using explicit instructions. Instead, they rely on patterns and inference from data.\\n\\nHere are some key concepts to help you understand machine learning better:\\n\\n### 1. **Types of Machine Learning:**\\n   - **Supervised Learning:** The model is trained on a labeled dataset, meaning the input comes with the correct output. The model learns to map inputs to outputs. Examples include classification and regression tasks.\\n   - **Unsupervised Learning:** The model works with unlabeled data and tries to find patterns or groupings in the data. Common techniques include clustering and dimensionality reduction.\\n   - **Semi-Supervised Learning:** A combination of supervised and unsupervised learning where the model is trained on a small amount of labeled data and a large amount of unlabeled data.\\n   - **Reinforcement Learning:** The model learns by interacting with an environment and receiving feedback in the form of rewards or penalties.\\n\\n### 2. **Key Terminology:**\\n   - **Dataset:** The collection of data used for training and testing the model.\\n   - **Features:** The individual measurable properties or characteristics of the data used as input for the model.\\n   - **Labels:** The output or target variable that the model is trying to predict in supervised learning.\\n   - **Model:** A mathematical representation of a process that makes predictions based on input data.\\n   - **Training:** The process of teaching the model using a dataset.\\n   - **Testing:** Evaluating the model's performance on a separate dataset that it hasn't seen before.\\n\\n### 3. **Common Algorithms:**\\n   - **Linear Regression:** Used for regression tasks to model the relationship between a dependent variable and one or more independent variables.\\n   - **Logistic Regression:** Used for binary classification tasks.\\n   - **Decision Trees:** A model that makes decisions based on answering a series of questions about the features.\\n   - **Support Vector Machines (SVM):** Used for classification tasks by finding the hyperplane that best separates different classes.\\n   - **Neural Networks:** Computational models inspired by the human brain that are particularly good at recognizing patterns in large datasets.\\n\\n### 4. **Steps in a Machine Learning Project:**\\n   1. **Define the Problem:** Clearly articulate what you want to solve.\\n   2. **Collect Data:** Gather the data needed for training and testing.\\n   3. **Preprocess Data\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 512, 'prompt_tokens': 51, 'total_tokens': 563, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'length', 'logprobs': None}, id='run-37bd960e-b627-4685-a81d-6a3363196522-0', usage_metadata={'input_tokens': 51, 'output_tokens': 512, 'total_tokens': 563, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02202f41-a839-4871-ab72-128e3808da12",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=\"Whats the difference between supervised and unsupervised?\"\n",
    ")\n",
    "messages.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "967369c8-629f-4def-86bd-7eddee92e36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "685571e1-5b0e-4180-adf6-905df89504dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main difference between supervised and unsupervised learning lies in the type of data used for training and the goals of the learning process. Here’s a breakdown of the key distinctions:\n",
      "\n",
      "### Supervised Learning\n",
      "\n",
      "1. **Labeled Data:**\n",
      "   - In supervised learning, the algorithm is trained on a labeled dataset. This means that each training example is paired with an output label or target value.\n",
      "   - For example, if you're building a model to predict house prices, your dataset might include features like the size of the house, the number of bedrooms, and the price (label) of each house.\n",
      "\n",
      "2. **Objective:**\n",
      "   - The goal is to learn a mapping from inputs (features) to outputs (labels) so that the model can make accurate predictions on new, unseen data.\n",
      "   - Common tasks include classification (predicting a category) and regression (predicting a continuous value).\n",
      "\n",
      "3. **Examples:**\n",
      "   - Classifying emails as \"spam\" or \"not spam.\"\n",
      "   - Predicting the price of a stock based on historical data.\n",
      "   - Image recognition tasks where images are labeled with specific objects (e.g., cat, dog).\n",
      "\n",
      "### Unsupervised Learning\n",
      "\n",
      "1. **Unlabeled Data:**\n",
      "   - In unsupervised learning, the algorithm is trained on data that does not have labeled outputs. The model tries to learn the underlying structure or distribution of the data without any explicit guidance on what the outputs should be.\n",
      "   - For example, you might have a dataset of customer purchase behavior, but you don't have labels indicating which customers belong to which segments.\n",
      "\n",
      "2. **Objective:**\n",
      "   - The goal is to discover patterns, groupings, or relationships within the data. This might include identifying clusters of similar data points or reducing the dimensionality of the data for visualization.\n",
      "   - Common tasks include clustering (grouping similar items) and association (finding rules that describe large portions of the data).\n",
      "\n",
      "3. **Examples:**\n",
      "   - Grouping customers into segments based on purchasing behavior (customer segmentation).\n",
      "   - Identifying topics in a collection of documents without predefined categories (topic modeling).\n",
      "   - Anomaly detection to find unusual data points in datasets.\n",
      "\n",
      "### Summary\n",
      "\n",
      "In summary, supervised learning requires labeled data and focuses on predicting outcomes, while unsupervised learning works with unlabeled data and aims to identify patterns or groupings in the data. Each approach has its own applications and use cases, depending on the nature of the problem and the available data.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97078638-fb99-4868-9091-f3f7b31a03d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add latest response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"Pode me falar sobre o clima no Rio de Janeiro hoje?\"\n",
    ")\n",
    "# append to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to GPT\n",
    "res = chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e22c0d9-57ac-48b4-bb82-d19ee34acb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desculpe, mas não tenho acesso a informações em tempo real, incluindo dados sobre o clima. No entanto, o clima no Rio de Janeiro geralmente é tropical, com temperaturas quentes e umidade alta. Durante a primavera e o verão, as temperaturas podem variar entre 25°C e 35°C, e há chances de chuvas, especialmente no verão. No outono e inverno, as temperaturas são mais amenas, variando entre 15°C e 25°C.\n",
      "\n",
      "Para informações precisas sobre o clima no Rio de Janeiro hoje, recomendo verificar um site de meteorologia ou um aplicativo de clima. Se precisar de mais informações sobre o clima ou dicas sobre o Rio de Janeiro, estou à disposição!\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5145b4cc-cfbf-4059-8f18-ae33bc2cbd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexto = [\"Hoje o clima no Rio de Janeiro está ensolarado\"]\n",
    "source_knowledge = \"\\n\".join(contexto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5d64ad3-5df6-4daa-bbb0-4c2af38ed82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Pode me falar sobre o clima no Rio de Janeiro hoje?\"\n",
    "\n",
    "augmented_prompt = f\"\"\"Using the contexts below to answer the question.\n",
    "\n",
    "Contexts:\n",
    "{source_knowledge}\n",
    "\n",
    "Question: {query}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb9d8ec7-a2d2-4d96-bc21-26b42e584076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the contexts below to answer the question.\n",
      "\n",
      "Contexts:\n",
      "Hoje o clima no Rio de Janeiro está ensolarado\n",
      "\n",
      "Question: Pode me falar sobre o clima no Rio de Janeiro hoje?\n"
     ]
    }
   ],
   "source": [
    "print(augmented_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16a06b59-f182-4f94-a234-fa8e072023ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augmented_prompt\n",
    ")\n",
    "\n",
    "messages.append(prompt)\n",
    "\n",
    "res = chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7312776f-b8fe-4cb1-a4e8-91c1a95e196b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hoje o clima no Rio de Janeiro está ensolarado.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7979f62e-96c9-4ad9-91cd-9f3d11db2ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embed_model = OpenAIEmbeddings(model=\"text-embedding-3-small\", openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ac41677-3a8e-4802-9ced-c24761889a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1536)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    'this is one chunk',\n",
    "    'this is the second chunk of text'\n",
    "]\n",
    "\n",
    "res = embed_model.embed_documents(texts)\n",
    "len(res), len(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01ea18d1-c33b-47d2-ab84-7a5f50dfc48c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.007134586106985807,\n",
       "  -0.011869040317833424,\n",
       "  0.006611976772546768,\n",
       "  0.03307536616921425,\n",
       "  -0.018891362473368645,\n",
       "  -0.020780498161911964,\n",
       "  0.025874972343444824,\n",
       "  -0.012372293509542942,\n",
       "  -0.03070620447397232,\n",
       "  0.012999424710869789,\n",
       "  0.09594333916902542,\n",
       "  -0.030133269727230072,\n",
       "  -0.04202553629875183,\n",
       "  -0.04741422086954117,\n",
       "  0.014625320211052895,\n",
       "  0.05308162793517113,\n",
       "  -0.009623754769563675,\n",
       "  -0.0019201056566089392,\n",
       "  -0.03973379731178284,\n",
       "  0.004893172532320023,\n",
       "  0.011327074840664864,\n",
       "  -0.020873406901955605,\n",
       "  0.035119350999593735,\n",
       "  0.026107242330908775,\n",
       "  -0.02519364282488823,\n",
       "  -0.006112594157457352,\n",
       "  -0.008439173921942711,\n",
       "  0.015825387090444565,\n",
       "  0.022994812577962875,\n",
       "  -0.011296105571091175,\n",
       "  -0.030272632837295532,\n",
       "  -0.04075578972697258,\n",
       "  0.014331110753118992,\n",
       "  -0.04660901427268982,\n",
       "  0.03123268485069275,\n",
       "  -0.02736150473356247,\n",
       "  -0.02678856998682022,\n",
       "  0.035490985959768295,\n",
       "  0.015345360152423382,\n",
       "  -0.012170991860330105,\n",
       "  0.023753564804792404,\n",
       "  -0.03967186063528061,\n",
       "  0.009205667302012444,\n",
       "  -0.0019588174764066935,\n",
       "  0.015562145970761776,\n",
       "  0.009221152402460575,\n",
       "  -0.058377403765916824,\n",
       "  0.020161109045147896,\n",
       "  -0.022298000752925873,\n",
       "  0.05357713997364044,\n",
       "  0.029018370434641838,\n",
       "  0.04735228046774864,\n",
       "  -0.07061033695936203,\n",
       "  0.008206903003156185,\n",
       "  -0.014269172213971615,\n",
       "  0.005349971819669008,\n",
       "  -0.006232600659132004,\n",
       "  0.0661507323384285,\n",
       "  0.022948358207941055,\n",
       "  0.0009586011292412877,\n",
       "  0.036853641271591187,\n",
       "  -0.03267276659607887,\n",
       "  -0.008834034204483032,\n",
       "  -0.021492794156074524,\n",
       "  0.003578906413167715,\n",
       "  0.00834626518189907,\n",
       "  -0.06119562312960625,\n",
       "  0.017435798421502113,\n",
       "  0.0238619577139616,\n",
       "  0.02476007118821144,\n",
       "  0.022050244733691216,\n",
       "  -0.019417842850089073,\n",
       "  -0.05295775085687637,\n",
       "  -0.0004330883384682238,\n",
       "  0.02683502435684204,\n",
       "  -0.009809572249650955,\n",
       "  -0.0056364391930401325,\n",
       "  0.016243474557995796,\n",
       "  -0.003911828156560659,\n",
       "  0.009801829233765602,\n",
       "  8.298843749798834e-05,\n",
       "  -0.006321637891232967,\n",
       "  0.0010819949675351381,\n",
       "  -0.05562112480401993,\n",
       "  0.02660275436937809,\n",
       "  0.0011168356286361814,\n",
       "  -0.09996937215328217,\n",
       "  0.015662796795368195,\n",
       "  -0.004788650665432215,\n",
       "  0.028987400233745575,\n",
       "  -0.024280045181512833,\n",
       "  -0.02222057618200779,\n",
       "  -0.046113502234220505,\n",
       "  0.018318427726626396,\n",
       "  0.04444115236401558,\n",
       "  -0.014122067019343376,\n",
       "  -0.04128227010369301,\n",
       "  -0.0027330536395311356,\n",
       "  0.004660901613533497,\n",
       "  -0.034933533519506454,\n",
       "  0.018643606454133987,\n",
       "  -0.007633968256413937,\n",
       "  -0.02141537144780159,\n",
       "  -0.004567993339151144,\n",
       "  0.028879007324576378,\n",
       "  -0.03731818124651909,\n",
       "  -0.015043407678604126,\n",
       "  0.03561486303806305,\n",
       "  -0.04645416885614395,\n",
       "  -0.019077178090810776,\n",
       "  -0.017249980941414833,\n",
       "  0.026633722707629204,\n",
       "  -0.020548226311802864,\n",
       "  0.018411334604024887,\n",
       "  0.02429552935063839,\n",
       "  0.011528375558555126,\n",
       "  0.01853521354496479,\n",
       "  -0.01622798852622509,\n",
       "  0.005729347467422485,\n",
       "  0.0130226518958807,\n",
       "  -0.004993822891265154,\n",
       "  -0.0025298164691776037,\n",
       "  -0.036853641271591187,\n",
       "  -0.04490569606423378,\n",
       "  -0.007572029251605272,\n",
       "  -0.03415929898619652,\n",
       "  0.05905873328447342,\n",
       "  -0.04815748706459999,\n",
       "  -0.021105676889419556,\n",
       "  0.010041842237114906,\n",
       "  0.004397661425173283,\n",
       "  -0.01632089726626873,\n",
       "  -0.014795652590692043,\n",
       "  -0.011373529210686684,\n",
       "  -0.042675893753767014,\n",
       "  -0.006182275712490082,\n",
       "  -0.03381863236427307,\n",
       "  0.00767655111849308,\n",
       "  -0.0313565619289875,\n",
       "  0.046485137194395065,\n",
       "  4.941804218105972e-05,\n",
       "  -0.032239191234111786,\n",
       "  -0.011125773191452026,\n",
       "  -0.02939000353217125,\n",
       "  -0.015438268892467022,\n",
       "  0.02462070807814598,\n",
       "  -0.036110371351242065,\n",
       "  0.053236477077007294,\n",
       "  -0.014609836041927338,\n",
       "  -0.03239404037594795,\n",
       "  0.03518129140138626,\n",
       "  -0.01802421733736992,\n",
       "  -0.0442553386092186,\n",
       "  -0.00014831460430286825,\n",
       "  -0.034933533519506454,\n",
       "  -0.02297932840883732,\n",
       "  0.01348719373345375,\n",
       "  -0.013239437714219093,\n",
       "  -0.040786758065223694,\n",
       "  0.0394241027534008,\n",
       "  -0.02976163662970066,\n",
       "  -0.06447838246822357,\n",
       "  0.03199143707752228,\n",
       "  0.02683502435684204,\n",
       "  -0.0014710486866533756,\n",
       "  0.042521048337221146,\n",
       "  -0.041530024260282516,\n",
       "  0.0037124622613191605,\n",
       "  -0.07135359942913055,\n",
       "  -0.025085249915719032,\n",
       "  -0.03202240541577339,\n",
       "  0.019634628668427467,\n",
       "  -0.042335230857133865,\n",
       "  -0.042613957077264786,\n",
       "  -0.013502677902579308,\n",
       "  -0.013750433921813965,\n",
       "  -0.020718559622764587,\n",
       "  -0.04902463033795357,\n",
       "  0.009538589045405388,\n",
       "  -0.00603904202580452,\n",
       "  0.02104373835027218,\n",
       "  0.04382176324725151,\n",
       "  0.0038924722466617823,\n",
       "  0.021337946876883507,\n",
       "  -0.058098677545785904,\n",
       "  0.007564287167042494,\n",
       "  0.017946792766451836,\n",
       "  0.025410430505871773,\n",
       "  0.02395486645400524,\n",
       "  0.007529446389526129,\n",
       "  -0.03995058313012123,\n",
       "  0.03660588338971138,\n",
       "  -0.026711147278547287,\n",
       "  0.029281610623002052,\n",
       "  0.011172227561473846,\n",
       "  -0.03490256518125534,\n",
       "  0.1009603887796402,\n",
       "  0.01853521354496479,\n",
       "  0.04456503316760063,\n",
       "  -0.0022762541193515062,\n",
       "  0.00144201482180506,\n",
       "  0.06838053464889526,\n",
       "  0.005516432225704193,\n",
       "  0.016398321837186813,\n",
       "  -0.009763117879629135,\n",
       "  -0.011845813132822514,\n",
       "  -0.0493343248963356,\n",
       "  0.018844908103346825,\n",
       "  0.010413476265966892,\n",
       "  -0.02226703055202961,\n",
       "  -0.0026672433596104383,\n",
       "  -0.024233590811491013,\n",
       "  -8.068991883192211e-05,\n",
       "  -0.02297932840883732,\n",
       "  0.007095874287188053,\n",
       "  -0.0011284491047263145,\n",
       "  -0.013750433921813965,\n",
       "  0.018364880234003067,\n",
       "  -0.0077268765307962894,\n",
       "  -0.009143728762865067,\n",
       "  0.0229328740388155,\n",
       "  0.01581764407455921,\n",
       "  0.06302282214164734,\n",
       "  0.06289894133806229,\n",
       "  -0.020625650882720947,\n",
       "  -0.018364880234003067,\n",
       "  0.01839585043489933,\n",
       "  -0.031170746311545372,\n",
       "  0.009043077938258648,\n",
       "  -0.0005976135144010186,\n",
       "  0.013734948821365833,\n",
       "  -0.00947665050625801,\n",
       "  -0.01684737764298916,\n",
       "  -0.039269257336854935,\n",
       "  -0.015593115240335464,\n",
       "  -0.011094803921878338,\n",
       "  -0.03437608480453491,\n",
       "  0.0038692450616508722,\n",
       "  -0.003553743939846754,\n",
       "  -0.006317766848951578,\n",
       "  -0.05060407519340515,\n",
       "  -0.03821629658341408,\n",
       "  0.048374272882938385,\n",
       "  -0.005702249240130186,\n",
       "  -0.00678230868652463,\n",
       "  -0.02325805276632309,\n",
       "  -0.029498396441340446,\n",
       "  0.010258628986775875,\n",
       "  -0.01783839985728264,\n",
       "  0.055590152740478516,\n",
       "  0.03654394671320915,\n",
       "  -0.003834404516965151,\n",
       "  -0.026618238538503647,\n",
       "  -0.007626225706189871,\n",
       "  -0.061907920986413956,\n",
       "  -0.019309449940919876,\n",
       "  0.012248415499925613,\n",
       "  0.03169722855091095,\n",
       "  -0.007165555376559496,\n",
       "  -0.008532081730663776,\n",
       "  -0.008121737278997898,\n",
       "  0.008973396383225918,\n",
       "  -0.056395359337329865,\n",
       "  0.0525551475584507,\n",
       "  -0.06299185007810593,\n",
       "  -0.013223953545093536,\n",
       "  0.06664624810218811,\n",
       "  0.01783839985728264,\n",
       "  0.019743021577596664,\n",
       "  -0.006611976772546768,\n",
       "  0.01092447154223919,\n",
       "  0.009817314334213734,\n",
       "  0.04825039580464363,\n",
       "  -0.045122481882572174,\n",
       "  -0.04967499151825905,\n",
       "  0.011241908185184002,\n",
       "  0.027795076370239258,\n",
       "  0.01887587644159794,\n",
       "  -0.0359555259346962,\n",
       "  0.023691626265645027,\n",
       "  -0.016243474557995796,\n",
       "  -0.02722214162349701,\n",
       "  0.0033369576558470726,\n",
       "  -0.0005211576935835183,\n",
       "  -0.008764352649450302,\n",
       "  -0.027376988902688026,\n",
       "  0.040539003908634186,\n",
       "  0.0007055226597003639,\n",
       "  0.026664692908525467,\n",
       "  -0.003278889926150441,\n",
       "  0.042861711233854294,\n",
       "  0.0028743515722453594,\n",
       "  0.042861711233854294,\n",
       "  -0.009739890694618225,\n",
       "  0.0016578331124037504,\n",
       "  0.009298576042056084,\n",
       "  -0.0456489622592926,\n",
       "  -0.004370562732219696,\n",
       "  0.020408865064382553,\n",
       "  -0.0029479041695594788,\n",
       "  0.001461370731703937,\n",
       "  0.025921424850821495,\n",
       "  0.05193575844168663,\n",
       "  -0.013100075535476208,\n",
       "  0.01087027508765459,\n",
       "  0.07296401262283325,\n",
       "  0.020393379032611847,\n",
       "  0.013789145275950432,\n",
       "  -0.026804054155945778,\n",
       "  -0.000262030545854941,\n",
       "  0.0178693700581789,\n",
       "  -0.008594021201133728,\n",
       "  -0.0008168191416189075,\n",
       "  -0.00767655111849308,\n",
       "  -0.017373858019709587,\n",
       "  -0.028971916064620018,\n",
       "  0.04546314477920532,\n",
       "  0.0019365581683814526,\n",
       "  0.02424907498061657,\n",
       "  -0.014214975759387016,\n",
       "  0.029777120798826218,\n",
       "  -0.022158637642860413,\n",
       "  -0.020919859409332275,\n",
       "  0.014307883568108082,\n",
       "  -0.019262995570898056,\n",
       "  0.00020263211627025157,\n",
       "  -0.007742361165583134,\n",
       "  0.028414465487003326,\n",
       "  -0.006395190488547087,\n",
       "  -0.028987400233745575,\n",
       "  -0.00809076800942421,\n",
       "  -0.016816409304738045,\n",
       "  -0.01585635542869568,\n",
       "  -0.005926777608692646,\n",
       "  -0.01731191948056221,\n",
       "  -0.04137517884373665,\n",
       "  -0.010010872967541218,\n",
       "  -0.01436208002269268,\n",
       "  0.002723375568166375,\n",
       "  0.01646026037633419,\n",
       "  -0.02688147872686386,\n",
       "  -0.06042138859629631,\n",
       "  0.036853641271591187,\n",
       "  -0.024651678279042244,\n",
       "  -0.030907506123185158,\n",
       "  0.02528655156493187,\n",
       "  0.02081146650016308,\n",
       "  -0.013169756159186363,\n",
       "  -0.0022181866224855185,\n",
       "  -0.019650112837553024,\n",
       "  -0.0010945763206109405,\n",
       "  0.018457788974046707,\n",
       "  -0.004049255046993494,\n",
       "  0.013402027077972889,\n",
       "  0.015167285688221455,\n",
       "  -0.04050803557038307,\n",
       "  -0.01768355257809162,\n",
       "  -0.04140614718198776,\n",
       "  0.014772425405681133,\n",
       "  0.04859105870127678,\n",
       "  0.0039718314073979855,\n",
       "  0.028368011116981506,\n",
       "  0.029328064993023872,\n",
       "  0.02655629999935627,\n",
       "  -0.007564287167042494,\n",
       "  0.016258958727121353,\n",
       "  -0.03784466162323952,\n",
       "  -0.021756036207079887,\n",
       "  0.0026382096111774445,\n",
       "  0.011807100847363472,\n",
       "  0.0035208389163017273,\n",
       "  0.01721901074051857,\n",
       "  -0.02849189005792141,\n",
       "  -0.002452393062412739,\n",
       "  -0.04589671641588211,\n",
       "  0.04301656037569046,\n",
       "  0.02533300593495369,\n",
       "  0.07500799745321274,\n",
       "  0.03679170086979866,\n",
       "  -0.040539003908634186,\n",
       "  -0.005485462956130505,\n",
       "  0.0025414300616830587,\n",
       "  -0.011830328032374382,\n",
       "  -0.007560415659099817,\n",
       "  0.014346595853567123,\n",
       "  -0.010583807714283466,\n",
       "  -0.019464297220110893,\n",
       "  -0.0005071246414445341,\n",
       "  -0.07160136103630066,\n",
       "  -0.08398913592100143,\n",
       "  0.05713862553238869,\n",
       "  -0.008237872272729874,\n",
       "  0.07184911519289017,\n",
       "  0.01499695423990488,\n",
       "  0.04100354388356209,\n",
       "  -0.015190512873232365,\n",
       "  0.004087966866791248,\n",
       "  0.0006435837713070214,\n",
       "  -0.04509151354432106,\n",
       "  0.0199443232268095,\n",
       "  0.04800264164805412,\n",
       "  0.017280951142311096,\n",
       "  0.00836949236690998,\n",
       "  -0.0003423575253691524,\n",
       "  0.01287554670125246,\n",
       "  0.02637048251926899,\n",
       "  -0.03759690746665001,\n",
       "  0.00844691600650549,\n",
       "  0.00598871661350131,\n",
       "  -0.06689400225877762,\n",
       "  -0.01966559700667858,\n",
       "  -0.06169113516807556,\n",
       "  0.027717653661966324,\n",
       "  0.022561240941286087,\n",
       "  0.028027348220348358,\n",
       "  -0.00345502863638103,\n",
       "  -0.01344073936343193,\n",
       "  0.02434198372066021,\n",
       "  0.013835599645972252,\n",
       "  0.05363908037543297,\n",
       "  -0.0056983777321875095,\n",
       "  -0.017342889681458473,\n",
       "  -0.030117785558104515,\n",
       "  0.0027001486159861088,\n",
       "  0.02556527778506279,\n",
       "  0.07420279085636139,\n",
       "  0.0298545453697443,\n",
       "  0.00099102221429348,\n",
       "  -0.011629026383161545,\n",
       "  -0.044936664402484894,\n",
       "  0.0315888337790966,\n",
       "  0.02825961820781231,\n",
       "  0.008624990470707417,\n",
       "  0.04187069088220596,\n",
       "  0.020021745935082436,\n",
       "  -0.02565818466246128,\n",
       "  0.012093568220734596,\n",
       "  0.0159028097987175,\n",
       "  0.0016239603282883763,\n",
       "  -0.06330154836177826,\n",
       "  -0.007293304428458214,\n",
       "  0.014416276477277279,\n",
       "  -0.04128227010369301,\n",
       "  -0.049086570739746094,\n",
       "  -0.05147121846675873,\n",
       "  0.02773313783109188,\n",
       "  -0.013990446925163269,\n",
       "  0.0009029528591781855,\n",
       "  0.018612636253237724,\n",
       "  -0.024744587019085884,\n",
       "  -0.030040360987186432,\n",
       "  -0.030721690505743027,\n",
       "  -0.018008733168244362,\n",
       "  -0.00033316347980871797,\n",
       "  -0.018643606454133987,\n",
       "  -0.037999510765075684,\n",
       "  0.053515199571847916,\n",
       "  -0.002415616763755679,\n",
       "  0.025921424850821495,\n",
       "  0.0322701632976532,\n",
       "  0.00936051458120346,\n",
       "  0.003755045123398304,\n",
       "  -0.01721901074051857,\n",
       "  0.019557204097509384,\n",
       "  -0.016057657077908516,\n",
       "  0.016661562025547028,\n",
       "  -0.011234166100621223,\n",
       "  -0.027764108031988144,\n",
       "  -0.0029362905770540237,\n",
       "  0.01542278379201889,\n",
       "  -0.03230113163590431,\n",
       "  -0.00361374719068408,\n",
       "  0.008384977467358112,\n",
       "  0.061815012246370316,\n",
       "  -0.02816671133041382,\n",
       "  -0.06528358906507492,\n",
       "  -0.04506054148077965,\n",
       "  0.005113829858601093,\n",
       "  0.022623179480433464,\n",
       "  -0.030210694298148155,\n",
       "  -0.03372572734951973,\n",
       "  -0.004916399251669645,\n",
       "  -0.010738654993474483,\n",
       "  0.0037318181712180376,\n",
       "  -0.017203526571393013,\n",
       "  0.053422294557094574,\n",
       "  0.02339741587638855,\n",
       "  0.007734619081020355,\n",
       "  0.004614447243511677,\n",
       "  -0.03075265884399414,\n",
       "  -0.019835930317640305,\n",
       "  0.04348110035061836,\n",
       "  -0.0026053045876324177,\n",
       "  -0.019835930317640305,\n",
       "  -0.004447986371815205,\n",
       "  0.011381271295249462,\n",
       "  -0.04084869846701622,\n",
       "  -0.04481278732419014,\n",
       "  0.04571090266108513,\n",
       "  -0.03042748011648655,\n",
       "  0.042706865817308426,\n",
       "  0.019030723720788956,\n",
       "  -0.005891936831176281,\n",
       "  -0.05345326289534569,\n",
       "  0.007293304428458214,\n",
       "  -0.03951701149344444,\n",
       "  -0.06048332527279854,\n",
       "  -0.0032130798790603876,\n",
       "  0.01301490981131792,\n",
       "  0.014826621860265732,\n",
       "  0.028337042778730392,\n",
       "  -0.0061203367076814175,\n",
       "  0.006615847814828157,\n",
       "  -0.018148094415664673,\n",
       "  -0.036822669208049774,\n",
       "  0.012140022590756416,\n",
       "  0.014741456136107445,\n",
       "  0.013092332519590855,\n",
       "  -0.01825648732483387,\n",
       "  0.005504819098860025,\n",
       "  -0.01980496011674404,\n",
       "  0.007037806324660778,\n",
       "  -0.007316531613469124,\n",
       "  -0.07042451947927475,\n",
       "  0.007161684334278107,\n",
       "  0.021399887278676033,\n",
       "  -0.00889597274363041,\n",
       "  -0.05497076362371445,\n",
       "  -0.01457886677235365,\n",
       "  0.024976857006549835,\n",
       "  0.002839511027559638,\n",
       "  0.0032653408125042915,\n",
       "  0.05032534897327423,\n",
       "  0.0410345159471035,\n",
       "  -0.031774651259183884,\n",
       "  0.014284656383097172,\n",
       "  -0.007103616371750832,\n",
       "  0.008462401106953621,\n",
       "  0.02208121493458748,\n",
       "  -0.0047499388456344604,\n",
       "  0.032951489090919495,\n",
       "  0.015407298691570759,\n",
       "  -0.014416276477277279,\n",
       "  0.0011274813441559672,\n",
       "  -0.04499860480427742,\n",
       "  0.052245453000068665,\n",
       "  0.013657525181770325,\n",
       "  -0.008276584558188915,\n",
       "  0.03824726492166519,\n",
       "  0.02646339125931263,\n",
       "  -0.02095082961022854,\n",
       "  -0.00018605861987452954,\n",
       "  0.002069146139547229,\n",
       "  -0.01627444289624691,\n",
       "  0.03778272494673729,\n",
       "  0.009817314334213734,\n",
       "  0.0368846096098423,\n",
       "  -0.007885594852268696,\n",
       "  -0.02014562487602234,\n",
       "  0.006232600659132004,\n",
       "  -0.0031608189456164837,\n",
       "  0.01398270484060049,\n",
       "  0.01289877388626337,\n",
       "  0.007401697337627411,\n",
       "  -0.050449226051568985,\n",
       "  0.04877687618136406,\n",
       "  0.01881393790245056,\n",
       "  -0.008315295912325382,\n",
       "  -0.008361750282347202,\n",
       "  -0.022483816370368004,\n",
       "  0.004769294522702694,\n",
       "  0.031867559999227524,\n",
       "  -0.05246223881840706,\n",
       "  -0.03970282897353172,\n",
       "  -0.01242648996412754,\n",
       "  0.01906169392168522,\n",
       "  -0.01816358044743538,\n",
       "  0.014911787584424019,\n",
       "  0.0394241027534008,\n",
       "  -0.032610826194286346,\n",
       "  -0.03911440819501877,\n",
       "  -0.00844691600650549,\n",
       "  -0.012743926607072353,\n",
       "  -0.010684458538889885,\n",
       "  -0.015585373155772686,\n",
       "  0.010978667996823788,\n",
       "  0.005075118038803339,\n",
       "  -0.015453753061592579,\n",
       "  -0.00891919992864132,\n",
       "  0.02494588866829872,\n",
       "  -0.008973396383225918,\n",
       "  0.019402356818318367,\n",
       "  0.002057532547041774,\n",
       "  0.016475744545459747,\n",
       "  0.022561240941286087,\n",
       "  0.0018165515502914786,\n",
       "  0.02438843809068203,\n",
       "  0.0028007992077618837,\n",
       "  -0.012294869869947433,\n",
       "  0.025255583226680756,\n",
       "  0.018705544993281364,\n",
       "  0.008671444840729237,\n",
       "  0.0095076197758317,\n",
       "  -0.0401054322719574,\n",
       "  -0.011079318821430206,\n",
       "  0.0011594186071306467,\n",
       "  -0.03409735858440399,\n",
       "  -0.0008390783914364874,\n",
       "  0.0440385527908802,\n",
       "  0.0012552302796393633,\n",
       "  -0.0009929578518494964,\n",
       "  0.02609175816178322,\n",
       "  0.03199143707752228,\n",
       "  -0.02023853175342083,\n",
       "  0.03772078454494476,\n",
       "  0.00804431363940239,\n",
       "  -0.0033446999732404947,\n",
       "  -0.005899679381400347,\n",
       "  0.04546314477920532,\n",
       "  0.00017940503312274814,\n",
       "  0.002256898209452629,\n",
       "  -0.024976857006549835,\n",
       "  0.01895330101251602,\n",
       "  0.025131704285740852,\n",
       "  -0.011110288091003895,\n",
       "  0.0161350816488266,\n",
       "  -0.0053538428619503975,\n",
       "  -0.023289022967219353,\n",
       "  0.021260524168610573,\n",
       "  0.05329841375350952,\n",
       "  0.006344865076243877,\n",
       "  0.009205667302012444,\n",
       "  0.039362166076898575,\n",
       "  -0.004866074305027723,\n",
       "  -0.06441644579172134,\n",
       "  -0.019448811188340187,\n",
       "  0.010854790918529034,\n",
       "  0.01650671474635601,\n",
       "  0.013092332519590855,\n",
       "  -0.00497446721419692,\n",
       "  0.0248994342982769,\n",
       "  0.03400444984436035,\n",
       "  0.029064824804663658,\n",
       "  -0.030628781765699387,\n",
       "  0.040724821388721466,\n",
       "  -0.003952475264668465,\n",
       "  0.016537683084607124,\n",
       "  0.0108935022726655,\n",
       "  0.0099102221429348,\n",
       "  -0.047569066286087036,\n",
       "  0.03123268485069275,\n",
       "  -0.015298905782401562,\n",
       "  0.011528375558555126,\n",
       "  -0.01995980739593506,\n",
       "  -0.017915824428200722,\n",
       "  0.015686023980379105,\n",
       "  -0.016073141247034073,\n",
       "  -0.035119350999593735,\n",
       "  -0.03679170086979866,\n",
       "  -0.011303847655653954,\n",
       "  -0.0014845978002995253,\n",
       "  0.05908970162272453,\n",
       "  0.023242568597197533,\n",
       "  -0.011350302025675774,\n",
       "  -0.007506219204515219,\n",
       "  0.021152131259441376,\n",
       "  -0.017342889681458473,\n",
       "  -0.022065728902816772,\n",
       "  0.01044444553554058,\n",
       "  0.0007084260578267276,\n",
       "  0.01816358044743538,\n",
       "  0.03886665403842926,\n",
       "  0.0500156544148922,\n",
       "  0.010018615983426571,\n",
       "  0.03778272494673729,\n",
       "  0.024218106642365456,\n",
       "  -0.014013674110174179,\n",
       "  -0.04245910793542862,\n",
       "  -0.049365296959877014,\n",
       "  0.04552508518099785,\n",
       "  -0.02141537144780159,\n",
       "  0.015600858256220818,\n",
       "  -0.03381863236427307,\n",
       "  -0.05320550873875618,\n",
       "  0.037380121648311615,\n",
       "  -0.011706450022757053,\n",
       "  0.014036901295185089,\n",
       "  -0.005280290264636278,\n",
       "  0.0053538428619503975,\n",
       "  -0.016878347843885422,\n",
       "  -0.01816358044743538,\n",
       "  -0.013533647172152996,\n",
       "  0.017946792766451836,\n",
       "  0.013789145275950432,\n",
       "  0.012349066324532032,\n",
       "  -0.016382835805416107,\n",
       "  0.008555308915674686,\n",
       "  0.010057327337563038,\n",
       "  -0.022282516583800316,\n",
       "  -0.008083024993538857,\n",
       "  -0.018380366265773773,\n",
       "  0.0010655424557626247,\n",
       "  -0.00030340379453264177,\n",
       "  -0.02796540968120098,\n",
       "  0.017157072201371193,\n",
       "  -0.004289268050342798,\n",
       "  0.026571784168481827,\n",
       "  0.026664692908525467,\n",
       "  0.01952623575925827,\n",
       "  -0.004626060836017132,\n",
       "  0.023273538798093796,\n",
       "  0.004591220058500767,\n",
       "  -0.0007132650353014469,\n",
       "  0.007858497090637684,\n",
       "  0.01858166791498661,\n",
       "  -0.00957730133086443,\n",
       "  -0.007959146983921528,\n",
       "  -0.06652236729860306,\n",
       "  -0.0010297340340912342,\n",
       "  -0.043326254934072495,\n",
       "  0.02321159839630127,\n",
       "  -0.004862202797085047,\n",
       "  0.016382835805416107,\n",
       "  -0.009105016477406025,\n",
       "  0.0030214565340429544,\n",
       "  0.001848488813266158,\n",
       "  -0.0024020676501095295,\n",
       "  -0.019077178090810776,\n",
       "  0.009298576042056084,\n",
       "  0.02584400214254856,\n",
       "  -0.028786098584532738,\n",
       "  0.0009619884076528251,\n",
       "  0.02288641966879368,\n",
       "  0.0014584673335775733,\n",
       "  -0.010692200623452663,\n",
       "  0.0315888337790966,\n",
       "  -0.007297175470739603,\n",
       "  0.016661562025547028,\n",
       "  -0.0352741964161396,\n",
       "  -0.005566757638007402,\n",
       "  0.008857261389493942,\n",
       "  -0.03970282897353172,\n",
       "  0.030164239928126335,\n",
       "  0.03533613681793213,\n",
       "  0.019077178090810776,\n",
       "  0.0032769544050097466,\n",
       "  0.0077849444933235645,\n",
       "  -0.02208121493458748,\n",
       "  -0.000406715931603685,\n",
       "  -0.019324934110045433,\n",
       "  -0.0696193128824234,\n",
       "  0.014509185217320919,\n",
       "  0.016661562025547028,\n",
       "  0.001280392985790968,\n",
       "  -0.03353990986943245,\n",
       "  -0.029312580823898315,\n",
       "  0.012798123061656952,\n",
       "  -0.015624085441231728,\n",
       "  0.0009668273851275444,\n",
       "  -0.022514786571264267,\n",
       "  0.009399226866662502,\n",
       "  0.0018136481521651149,\n",
       "  0.030009392648935318,\n",
       "  0.015267936512827873,\n",
       "  0.0019259123364463449,\n",
       "  0.019727537408471107,\n",
       "  -0.006170662119984627,\n",
       "  -0.012349066324532032,\n",
       "  0.022747058421373367,\n",
       "  0.01287554670125246,\n",
       "  0.02400132082402706,\n",
       "  0.029792606830596924,\n",
       "  0.028693191707134247,\n",
       "  0.01957269012928009,\n",
       "  0.04803360998630524,\n",
       "  -0.0493343248963356,\n",
       "  -0.0027949924115091562,\n",
       "  -0.005508690141141415,\n",
       "  -0.018148094415664673,\n",
       "  -0.011706450022757053,\n",
       "  0.013131044805049896,\n",
       "  0.02602981962263584,\n",
       "  0.02307223714888096,\n",
       "  0.008717899210751057,\n",
       "  -0.0020865662954747677,\n",
       "  0.018984269350767136,\n",
       "  0.047569066286087036,\n",
       "  0.028321558609604836,\n",
       "  0.00894242711365223,\n",
       "  -0.019216541200876236,\n",
       "  -0.024264561012387276,\n",
       "  -0.023846473544836044,\n",
       "  0.0012929743388667703,\n",
       "  0.012263900600373745,\n",
       "  0.02198830619454384,\n",
       "  -0.0067242407239973545,\n",
       "  0.008338523097336292,\n",
       "  -0.11619735509157181,\n",
       "  0.016723500564694405,\n",
       "  0.031217200681567192,\n",
       "  -0.01892233081161976,\n",
       "  -0.003834404516965151,\n",
       "  0.031960468739271164,\n",
       "  0.025905940681695938,\n",
       "  0.02061016671359539,\n",
       "  -0.008686929009854794,\n",
       "  -0.028321558609604836,\n",
       "  0.024729102849960327,\n",
       "  0.014842106960713863,\n",
       "  0.0008095606463029981,\n",
       "  0.008206903003156185,\n",
       "  -0.01443176157772541,\n",
       "  0.02014562487602234,\n",
       "  -0.015461495146155357,\n",
       "  0.02957582101225853,\n",
       "  -0.014447245746850967,\n",
       "  4.9478527216706425e-05,\n",
       "  -0.0014362080255523324,\n",
       "  -0.006654559634625912,\n",
       "  0.017915824428200722,\n",
       "  0.01084704790264368,\n",
       "  -0.022437363862991333,\n",
       "  0.011543860659003258,\n",
       "  -0.031155262142419815,\n",
       "  -0.010142493061721325,\n",
       "  0.013835599645972252,\n",
       "  -0.03542904555797577,\n",
       "  0.029281610623002052,\n",
       "  -0.003611811436712742,\n",
       "  0.0024911046493798494,\n",
       "  -0.008632732555270195,\n",
       "  -0.015887325629591942,\n",
       "  0.008160448633134365,\n",
       "  -0.017528705298900604,\n",
       "  -0.026401452720165253,\n",
       "  0.03310633823275566,\n",
       "  0.009221152402460575,\n",
       "  -0.010382506065070629,\n",
       "  -0.05655020847916603,\n",
       "  0.007649452891200781,\n",
       "  0.0070184506475925446,\n",
       "  -0.01995980739593506,\n",
       "  -0.02349032461643219,\n",
       "  -0.0011739354813471437,\n",
       "  -0.027144718915224075,\n",
       "  -0.011938720941543579,\n",
       "  -0.02980809099972248,\n",
       "  0.018179064616560936,\n",
       "  -0.015577631071209908,\n",
       "  0.00888823065906763,\n",
       "  0.020455319434404373,\n",
       "  0.029931968078017235,\n",
       "  0.031046869233250618,\n",
       "  -0.02349032461643219,\n",
       "  -0.009066305123269558,\n",
       "  0.01768355257809162,\n",
       "  0.029126763343811035,\n",
       "  0.017327405512332916,\n",
       "  0.028739646077156067,\n",
       "  -0.0006232601008377969,\n",
       "  -0.03564583137631416,\n",
       "  -0.003091137856245041,\n",
       "  -0.013657525181770325,\n",
       "  -0.019681083038449287,\n",
       "  0.029591305181384087,\n",
       "  -0.011659996584057808,\n",
       "  -0.013192983344197273,\n",
       "  0.03697751834988594,\n",
       "  -0.030953960493206978,\n",
       "  0.007010708097368479,\n",
       "  0.02650984562933445,\n",
       "  -0.0007195557118393481,\n",
       "  -0.02457425557076931,\n",
       "  -0.012170991860330105,\n",
       "  0.02443489246070385,\n",
       "  -0.024791041389107704,\n",
       "  0.002200766233727336,\n",
       "  0.026169180870056152,\n",
       "  0.03160431981086731,\n",
       "  0.01834939606487751,\n",
       "  0.015236967243254185,\n",
       "  -0.041530024260282516,\n",
       "  -0.022205092012882233,\n",
       "  -0.01134255900979042,\n",
       "  -0.007533317431807518,\n",
       "  -0.03669879212975502,\n",
       "  0.001963656395673752,\n",
       "  0.023707110434770584,\n",
       "  0.02061016671359539,\n",
       "  -0.02890997752547264,\n",
       "  -0.021167615428566933,\n",
       "  -0.05881097540259361,\n",
       "  -0.015260194428265095,\n",
       "  0.0051177009008824825,\n",
       "  -0.027903469279408455,\n",
       "  0.007506219204515219,\n",
       "  0.016661562025547028,\n",
       "  0.022143153473734856,\n",
       "  0.011892266571521759,\n",
       "  0.02301029860973358,\n",
       "  0.006422288715839386,\n",
       "  -0.04230426251888275,\n",
       "  -0.03524322807788849,\n",
       "  0.011683222837746143,\n",
       "  -0.01143546774983406,\n",
       "  0.011791616678237915,\n",
       "  0.013231695629656315,\n",
       "  0.024450376629829407,\n",
       "  -0.0049280128441751,\n",
       "  0.00477703707292676,\n",
       "  0.03270373493432999,\n",
       "  0.01136578619480133,\n",
       "  0.03750399872660637,\n",
       "  0.007424924522638321,\n",
       "  0.004699613433331251,\n",
       "  -0.022747058421373367,\n",
       "  -0.01244971714913845,\n",
       "  -0.017002224922180176,\n",
       "  0.0161350816488266,\n",
       "  0.00933728739619255,\n",
       "  -0.01388979610055685,\n",
       "  -0.03169722855091095,\n",
       "  0.03753496706485748,\n",
       "  0.0010694136144593358,\n",
       "  0.05165703594684601,\n",
       "  0.003015649737790227,\n",
       "  -0.006395190488547087,\n",
       "  0.025828517973423004,\n",
       "  0.0036214895080775023,\n",
       "  0.011094803921878338,\n",
       "  -0.00028574152383953333,\n",
       "  0.05020147189497948,\n",
       "  -0.01712610386312008,\n",
       "  -0.005133185535669327,\n",
       "  -0.02579754777252674,\n",
       "  -0.019495265558362007,\n",
       "  -0.022793510928750038,\n",
       "  0.025921424850821495,\n",
       "  -0.045308299362659454,\n",
       "  0.005764187779277563,\n",
       "  0.007843011990189552,\n",
       "  -0.030861051753163338,\n",
       "  -0.017280951142311096,\n",
       "  -0.03772078454494476,\n",
       "  0.06014266237616539,\n",
       "  -0.008717899210751057,\n",
       "  0.00542352395132184,\n",
       "  -0.020130138844251633,\n",
       "  -0.015740221366286278,\n",
       "  0.008957912214100361,\n",
       "  -0.03075265884399414,\n",
       "  0.013796888291835785,\n",
       "  0.017342889681458473,\n",
       "  -0.004045384004712105,\n",
       "  0.013998189009726048,\n",
       "  0.02330450713634491,\n",
       "  0.005775801371783018,\n",
       "  0.022251546382904053,\n",
       "  0.027376988902688026,\n",
       "  -0.019371388480067253,\n",
       "  0.012201961129903793,\n",
       "  0.020021745935082436,\n",
       "  0.04481278732419014,\n",
       "  -0.01768355257809162,\n",
       "  -0.021353432908654213,\n",
       "  -0.04338819161057472,\n",
       "  0.014060128480196,\n",
       "  -0.0144085343927145,\n",
       "  0.036729760468006134,\n",
       "  0.019448811188340187,\n",
       "  0.0037608519196510315,\n",
       "  -0.004447986371815205,\n",
       "  -0.004157647956162691,\n",
       "  0.011396755464375019,\n",
       "  0.02019207738339901,\n",
       "  -0.014238202013075352,\n",
       "  0.002787250094115734,\n",
       "  0.01082382071763277,\n",
       "  0.019696567207574844,\n",
       "  -0.022390909492969513,\n",
       "  0.02203476056456566,\n",
       "  0.04354304075241089,\n",
       "  0.012798123061656952,\n",
       "  -0.008361750282347202,\n",
       "  0.02335096150636673,\n",
       "  0.03141850233078003,\n",
       "  0.034499961882829666,\n",
       "  0.04465794190764427,\n",
       "  0.03217725455760956,\n",
       "  -0.030876537784934044,\n",
       "  -0.02174055017530918,\n",
       "  0.01580990105867386,\n",
       "  -0.019835930317640305,\n",
       "  -0.0005027696024626493,\n",
       "  0.0014729842077940702,\n",
       "  -0.0013055556919425726,\n",
       "  0.012372293509542942,\n",
       "  0.008810807019472122,\n",
       "  0.06051429733633995,\n",
       "  0.05184284970164299,\n",
       "  ...],\n",
       " [0.0030308449640870094,\n",
       "  0.029904838651418686,\n",
       "  0.004100222140550613,\n",
       "  -0.0016559314681217074,\n",
       "  -0.010063838213682175,\n",
       "  0.02021820656955242,\n",
       "  0.013602024875581264,\n",
       "  0.0045076035894453526,\n",
       "  -0.016129299998283386,\n",
       "  -0.011550027877092361,\n",
       "  0.04604922607541084,\n",
       "  -0.017834268510341644,\n",
       "  -0.058844033628702164,\n",
       "  -0.014612934552133083,\n",
       "  0.023069879040122032,\n",
       "  0.0452042892575264,\n",
       "  -0.01468083169311285,\n",
       "  -0.010637190192937851,\n",
       "  -0.034823596477508545,\n",
       "  0.024835199117660522,\n",
       "  0.0012947573559358716,\n",
       "  0.00025154880131594837,\n",
       "  0.014386611990630627,\n",
       "  0.05549445003271103,\n",
       "  -0.04381617158651352,\n",
       "  -0.01203285064548254,\n",
       "  0.01590297743678093,\n",
       "  0.010373147204518318,\n",
       "  0.03108171932399273,\n",
       "  -0.036573830991983414,\n",
       "  -0.02296426147222519,\n",
       "  -0.05166204273700714,\n",
       "  0.02895428240299225,\n",
       "  -0.018920619040727615,\n",
       "  0.011587748304009438,\n",
       "  -0.008924677968025208,\n",
       "  -0.009166089817881584,\n",
       "  0.016717741265892982,\n",
       "  0.023069879040122032,\n",
       "  -0.018181297928094864,\n",
       "  0.026344021782279015,\n",
       "  -0.06777625530958176,\n",
       "  -0.004575500730425119,\n",
       "  0.012523217126727104,\n",
       "  -0.009664000943303108,\n",
       "  0.025876285508275032,\n",
       "  -0.018483061343431473,\n",
       "  -0.001441867440007627,\n",
       "  0.023809200152754784,\n",
       "  0.04288070276379585,\n",
       "  0.021078234538435936,\n",
       "  0.011135101318359375,\n",
       "  -0.01626509428024292,\n",
       "  0.028924105688929558,\n",
       "  -0.011919688433408737,\n",
       "  0.008283428847789764,\n",
       "  0.0012457206612452865,\n",
       "  0.07682918012142181,\n",
       "  -0.0014626136980950832,\n",
       "  -0.037961944937705994,\n",
       "  0.0623444989323616,\n",
       "  0.00937732495367527,\n",
       "  -0.001653102459385991,\n",
       "  -0.009188721887767315,\n",
       "  -0.01417537685483694,\n",
       "  -0.007740253582596779,\n",
       "  -0.034099362790584564,\n",
       "  0.03560818359255791,\n",
       "  -0.0039946045726537704,\n",
       "  0.03144383803009987,\n",
       "  0.06306873261928558,\n",
       "  -0.014756272546947002,\n",
       "  -0.0748978927731514,\n",
       "  -0.004794280044734478,\n",
       "  -0.006514336448162794,\n",
       "  -0.034099362790584564,\n",
       "  -0.022873731330037117,\n",
       "  0.02225511521100998,\n",
       "  0.011368968524038792,\n",
       "  0.039410412311553955,\n",
       "  -0.01324745174497366,\n",
       "  0.014643111266195774,\n",
       "  -0.022918997332453728,\n",
       "  -0.01911676675081253,\n",
       "  -0.01360956858843565,\n",
       "  0.024684317409992218,\n",
       "  -0.0900464579463005,\n",
       "  0.028531812131404877,\n",
       "  -0.014575214125216007,\n",
       "  -0.00040879627340473235,\n",
       "  -0.02542364038527012,\n",
       "  -0.004545324482023716,\n",
       "  -0.02303970232605934,\n",
       "  0.012251629494130611,\n",
       "  0.015540859661996365,\n",
       "  0.007536562625318766,\n",
       "  -0.05902509018778801,\n",
       "  -0.024141142144799232,\n",
       "  -0.009679089300334454,\n",
       "  -0.034883949905633926,\n",
       "  0.012251629494130611,\n",
       "  -0.01222145278006792,\n",
       "  -0.05009286850690842,\n",
       "  0.020082412287592888,\n",
       "  0.03144383803009987,\n",
       "  -0.0015003342414274812,\n",
       "  -0.0014013178879395127,\n",
       "  0.008343782275915146,\n",
       "  -0.05787838622927666,\n",
       "  -0.03536677360534668,\n",
       "  -0.07266483455896378,\n",
       "  0.033465657383203506,\n",
       "  0.044027406722307205,\n",
       "  0.05597727373242378,\n",
       "  0.024623963981866837,\n",
       "  0.010003485716879368,\n",
       "  0.034099362790584564,\n",
       "  0.013707642443478107,\n",
       "  0.011384056881070137,\n",
       "  -0.007762886118143797,\n",
       "  0.007298923097550869,\n",
       "  -0.033767420798540115,\n",
       "  -0.02438255399465561,\n",
       "  -0.022541791200637817,\n",
       "  -0.010931410826742649,\n",
       "  -0.05519268661737442,\n",
       "  0.04556640610098839,\n",
       "  -0.05449862778186798,\n",
       "  -0.000944427854847163,\n",
       "  0.03575906530022621,\n",
       "  -0.0005455331993289292,\n",
       "  -0.022526703774929047,\n",
       "  -0.018271826207637787,\n",
       "  -0.03753947466611862,\n",
       "  -0.06074514612555504,\n",
       "  0.010916322469711304,\n",
       "  -0.03642294928431511,\n",
       "  -0.008736075833439827,\n",
       "  -0.027083342894911766,\n",
       "  0.046200111508369446,\n",
       "  -0.019388355314731598,\n",
       "  -0.047980520874261856,\n",
       "  -0.0066652181558310986,\n",
       "  -0.05516250804066658,\n",
       "  -0.020701028406620026,\n",
       "  -0.02758125402033329,\n",
       "  0.0008194785914383829,\n",
       "  0.03669453412294388,\n",
       "  -0.003176068887114525,\n",
       "  -0.016400888562202454,\n",
       "  -0.018603768199682236,\n",
       "  0.059175971895456314,\n",
       "  -0.024986080825328827,\n",
       "  -0.008223076350986958,\n",
       "  -0.045747462660074234,\n",
       "  0.011715997941792011,\n",
       "  0.040919236838817596,\n",
       "  -0.009656456299126148,\n",
       "  -0.016415975987911224,\n",
       "  0.03575906530022621,\n",
       "  -0.05655062571167946,\n",
       "  -0.07839835435152054,\n",
       "  0.022405996918678284,\n",
       "  -0.009271707385778427,\n",
       "  0.010825793258845806,\n",
       "  0.03002554550766945,\n",
       "  0.011331248097121716,\n",
       "  0.01075789611786604,\n",
       "  -0.07332871854305267,\n",
       "  -0.029965192079544067,\n",
       "  -0.04846334084868431,\n",
       "  0.031685248017311096,\n",
       "  -0.028048988431692123,\n",
       "  -0.055343568325042725,\n",
       "  -0.0026083749253302813,\n",
       "  0.008781339973211288,\n",
       "  -0.024986080825328827,\n",
       "  -0.017472151666879654,\n",
       "  -0.022421086207032204,\n",
       "  0.029467280954122543,\n",
       "  0.025951728224754333,\n",
       "  0.034461479634046555,\n",
       "  0.04638116806745529,\n",
       "  0.01914694346487522,\n",
       "  -0.020082412287592888,\n",
       "  -0.006978299003094435,\n",
       "  0.026947548612952232,\n",
       "  0.028048988431692123,\n",
       "  0.036543652415275574,\n",
       "  0.007283835206180811,\n",
       "  0.009105737321078777,\n",
       "  -0.009641368873417377,\n",
       "  -0.00793640036135912,\n",
       "  -0.005243154242634773,\n",
       "  -0.007306467276066542,\n",
       "  -0.024955905973911285,\n",
       "  0.08654599636793137,\n",
       "  -0.01621983014047146,\n",
       "  0.004288824740797281,\n",
       "  0.0073517318814992905,\n",
       "  0.027686871588230133,\n",
       "  0.04592852294445038,\n",
       "  0.013534127734601498,\n",
       "  0.024578699842095375,\n",
       "  -0.019916441291570663,\n",
       "  -0.010335426777601242,\n",
       "  -0.05552462488412857,\n",
       "  0.007740253582596779,\n",
       "  -0.01025244127959013,\n",
       "  -0.02655525505542755,\n",
       "  0.030644161626696587,\n",
       "  -0.02830548956990242,\n",
       "  -0.04327299818396568,\n",
       "  -0.009482942521572113,\n",
       "  -0.02970869280397892,\n",
       "  0.012010217644274235,\n",
       "  -0.03186630830168724,\n",
       "  -0.002440518466755748,\n",
       "  0.023205673322081566,\n",
       "  -0.011248263530433178,\n",
       "  0.04309193789958954,\n",
       "  0.04553622752428055,\n",
       "  0.0417339988052845,\n",
       "  0.03497447818517685,\n",
       "  -0.006140903104096651,\n",
       "  -0.014952419325709343,\n",
       "  0.027490725740790367,\n",
       "  -0.009679089300334454,\n",
       "  -0.0003878142451867461,\n",
       "  -0.004194523207843304,\n",
       "  0.0729062482714653,\n",
       "  -0.0038493804167956114,\n",
       "  -0.014665743336081505,\n",
       "  -0.02233055606484413,\n",
       "  0.025996992364525795,\n",
       "  -0.023839376866817474,\n",
       "  -0.02577066794037819,\n",
       "  0.0359703004360199,\n",
       "  -0.03672471269965172,\n",
       "  0.06125814840197563,\n",
       "  -0.03470288962125778,\n",
       "  0.0027347386348992586,\n",
       "  0.06318943947553635,\n",
       "  0.026721226051449776,\n",
       "  0.015103301964700222,\n",
       "  -0.04221682250499725,\n",
       "  -0.023054789751768112,\n",
       "  -0.04429899528622627,\n",
       "  0.028833575546741486,\n",
       "  0.03406918793916702,\n",
       "  0.022692672908306122,\n",
       "  0.006842504721134901,\n",
       "  -0.0016813927795737982,\n",
       "  -0.019976794719696045,\n",
       "  -0.06735378503799438,\n",
       "  -0.03880688548088074,\n",
       "  0.018226562067866325,\n",
       "  0.03600047901272774,\n",
       "  0.029814310371875763,\n",
       "  -0.01062210276722908,\n",
       "  0.00903029553592205,\n",
       "  -0.04463093727827072,\n",
       "  -0.07006966322660446,\n",
       "  0.024503258988261223,\n",
       "  -0.032379306852817535,\n",
       "  -0.022526703774929047,\n",
       "  0.05368386581540108,\n",
       "  0.022466350346803665,\n",
       "  0.02335655502974987,\n",
       "  -0.053201042115688324,\n",
       "  -0.029965192079544067,\n",
       "  0.004496287554502487,\n",
       "  0.05386492237448692,\n",
       "  0.019720295444130898,\n",
       "  -0.083467997610569,\n",
       "  0.01588788814842701,\n",
       "  -0.022783203050494194,\n",
       "  -0.0030044405721127987,\n",
       "  0.0012164872605353594,\n",
       "  0.05057569220662117,\n",
       "  -0.0014277221634984016,\n",
       "  0.017487239092588425,\n",
       "  -0.04100976511836052,\n",
       "  -0.0069896150380373,\n",
       "  -0.0339786559343338,\n",
       "  -0.021153675392270088,\n",
       "  0.0045981332659721375,\n",
       "  -0.0046358536928892136,\n",
       "  0.07743271440267563,\n",
       "  0.014394155703485012,\n",
       "  0.05021357536315918,\n",
       "  -0.026736315339803696,\n",
       "  0.033073365688323975,\n",
       "  0.0011759375920519233,\n",
       "  -0.03280177712440491,\n",
       "  -0.007008475251495838,\n",
       "  -0.017924798652529716,\n",
       "  0.0020369088742882013,\n",
       "  -0.004371809773147106,\n",
       "  0.026811756193637848,\n",
       "  -0.0014965621521696448,\n",
       "  0.0017153412336483598,\n",
       "  0.03177577629685402,\n",
       "  0.010931410826742649,\n",
       "  0.010433499701321125,\n",
       "  0.05863279849290848,\n",
       "  0.03189648315310478,\n",
       "  0.009045383892953396,\n",
       "  -0.03367689251899719,\n",
       "  0.006868909113109112,\n",
       "  0.012191276997327805,\n",
       "  -0.025876285508275032,\n",
       "  -0.022134410217404366,\n",
       "  -0.02898445911705494,\n",
       "  -0.04891598969697952,\n",
       "  -0.03986306115984917,\n",
       "  0.04423864185810089,\n",
       "  0.003826748114079237,\n",
       "  -0.018452884629368782,\n",
       "  -0.03283195197582245,\n",
       "  0.046109579503536224,\n",
       "  -0.006091866176575422,\n",
       "  -0.009724353440105915,\n",
       "  0.004552868660539389,\n",
       "  0.021832644939422607,\n",
       "  0.015141022391617298,\n",
       "  0.0014843030367046595,\n",
       "  0.049006517976522446,\n",
       "  0.018483061343431473,\n",
       "  -0.03790159150958061,\n",
       "  -0.016023682430386543,\n",
       "  -0.03219824656844139,\n",
       "  -0.019750472158193588,\n",
       "  0.0004865948867518455,\n",
       "  0.013941509649157524,\n",
       "  -0.061680618673563004,\n",
       "  -0.014899610541760921,\n",
       "  0.01692897640168667,\n",
       "  -0.008049561642110348,\n",
       "  0.009452765807509422,\n",
       "  -0.033344950526952744,\n",
       "  -0.044751640409231186,\n",
       "  0.007860959507524967,\n",
       "  -0.0077251652255654335,\n",
       "  -0.009920500218868256,\n",
       "  0.00564676383510232,\n",
       "  -0.0150504931807518,\n",
       "  -0.0027668012771755457,\n",
       "  -0.0077251652255654335,\n",
       "  0.013058848679065704,\n",
       "  -0.014673287980258465,\n",
       "  0.007310239598155022,\n",
       "  0.03280177712440491,\n",
       "  -0.050062693655490875,\n",
       "  0.008932222612202168,\n",
       "  -0.051390454173088074,\n",
       "  -0.002470694947987795,\n",
       "  -0.02933148667216301,\n",
       "  0.03575906530022621,\n",
       "  0.030206603929400444,\n",
       "  0.006974526681005955,\n",
       "  0.026706138625741005,\n",
       "  0.055011626332998276,\n",
       "  0.030991191044449806,\n",
       "  0.004311456810683012,\n",
       "  -0.010893690399825573,\n",
       "  -0.05169222131371498,\n",
       "  0.02196843922138214,\n",
       "  0.005654308013617992,\n",
       "  0.011934776790440083,\n",
       "  -0.005880631040781736,\n",
       "  0.0019331773510202765,\n",
       "  -0.01809076778590679,\n",
       "  0.02937675267457962,\n",
       "  -0.04360493645071983,\n",
       "  0.046532049775123596,\n",
       "  0.004341633524745703,\n",
       "  0.05211469158530235,\n",
       "  -0.0062955571338534355,\n",
       "  -0.011240718886256218,\n",
       "  0.013534127734601498,\n",
       "  0.015752095729112625,\n",
       "  -0.03883706033229828,\n",
       "  -0.0030214148573577404,\n",
       "  0.021440351381897926,\n",
       "  0.00025838566944003105,\n",
       "  -0.020685940980911255,\n",
       "  0.024503258988261223,\n",
       "  -0.03817318007349968,\n",
       "  0.004549096338450909,\n",
       "  0.05102833732962608,\n",
       "  -0.017441974952816963,\n",
       "  0.021002793684601784,\n",
       "  0.01905641332268715,\n",
       "  0.02224002592265606,\n",
       "  0.005560006480664015,\n",
       "  0.0169893279671669,\n",
       "  0.02580084465444088,\n",
       "  0.0166573878377676,\n",
       "  0.010644734837114811,\n",
       "  0.06747449189424515,\n",
       "  -0.00238770991563797,\n",
       "  0.004877265077084303,\n",
       "  0.021998615935444832,\n",
       "  -0.06222379207611084,\n",
       "  0.027385108172893524,\n",
       "  -0.024141142144799232,\n",
       "  -0.0071065486408770084,\n",
       "  0.03403900936245918,\n",
       "  -0.024895552545785904,\n",
       "  0.002110463799908757,\n",
       "  -0.05407615751028061,\n",
       "  0.03437095135450363,\n",
       "  0.00016549884458072484,\n",
       "  0.010999307967722416,\n",
       "  0.0030402750708162785,\n",
       "  0.04161329194903374,\n",
       "  0.03633241727948189,\n",
       "  0.01842270791530609,\n",
       "  0.03965182602405548,\n",
       "  0.004341633524745703,\n",
       "  -0.020278559997677803,\n",
       "  -0.019343089312314987,\n",
       "  -0.01978064887225628,\n",
       "  0.059598442167043686,\n",
       "  0.047920167446136475,\n",
       "  0.02444290556013584,\n",
       "  -0.02373375929892063,\n",
       "  0.004220927599817514,\n",
       "  -0.048976343125104904,\n",
       "  -0.03192666172981262,\n",
       "  0.00023987902386579663,\n",
       "  0.016415975987911224,\n",
       "  0.011014396324753761,\n",
       "  0.020323824137449265,\n",
       "  -0.02764160744845867,\n",
       "  0.009316971525549889,\n",
       "  0.024186406284570694,\n",
       "  -0.013813259080052376,\n",
       "  -0.06554319709539413,\n",
       "  0.027777401730418205,\n",
       "  0.01502786111086607,\n",
       "  -0.034793421626091,\n",
       "  -0.0275510773062706,\n",
       "  -0.06880225241184235,\n",
       "  0.0729062482714653,\n",
       "  0.004515147767961025,\n",
       "  0.02798863686621189,\n",
       "  0.02515205182135105,\n",
       "  -0.03071960248053074,\n",
       "  -0.04064764827489853,\n",
       "  -0.022481437772512436,\n",
       "  -0.016702651977539062,\n",
       "  0.004926301538944244,\n",
       "  -0.03434077277779579,\n",
       "  -0.010735264047980309,\n",
       "  0.038746532052755356,\n",
       "  -0.002708334242925048,\n",
       "  0.017185475677251816,\n",
       "  0.00937732495367527,\n",
       "  0.010146823711693287,\n",
       "  -0.016084035858511925,\n",
       "  -0.033073365688323975,\n",
       "  -0.0005045121652074158,\n",
       "  -0.031021367758512497,\n",
       "  0.0008529555634595454,\n",
       "  -0.015239095315337181,\n",
       "  0.060956381261348724,\n",
       "  0.020685940980911255,\n",
       "  0.0004611335170920938,\n",
       "  -0.0006860421854071319,\n",
       "  0.001002423232421279,\n",
       "  -0.013006039895117283,\n",
       "  0.01949397288262844,\n",
       "  -0.03358636423945427,\n",
       "  -0.036604005843400955,\n",
       "  -0.023824289441108704,\n",
       "  0.007189533673226833,\n",
       "  -0.006910401862114668,\n",
       "  0.003185498993843794,\n",
       "  -0.023597966879606247,\n",
       "  0.0048621767200529575,\n",
       "  -0.01910167932510376,\n",
       "  0.030629074200987816,\n",
       "  -0.001460727653466165,\n",
       "  0.017970062792301178,\n",
       "  0.014605390839278698,\n",
       "  0.028878841549158096,\n",
       "  -0.006250292528420687,\n",
       "  -0.02122911624610424,\n",
       "  -0.02085191197693348,\n",
       "  0.03720753639936447,\n",
       "  0.019011149182915688,\n",
       "  -0.018905531615018845,\n",
       "  -0.022783203050494194,\n",
       "  0.009105737321078777,\n",
       "  0.0037475349381566048,\n",
       "  -0.07634636014699936,\n",
       "  0.011482130736112595,\n",
       "  -0.031021367758512497,\n",
       "  0.03781106323003769,\n",
       "  0.026087520644068718,\n",
       "  0.016536682844161987,\n",
       "  -0.004054957535117865,\n",
       "  0.0524466298520565,\n",
       "  -0.012545849196612835,\n",
       "  -0.04390669986605644,\n",
       "  0.0046358536928892136,\n",
       "  0.008509752340614796,\n",
       "  0.018286915495991707,\n",
       "  0.04073817655444145,\n",
       "  -0.003349583363160491,\n",
       "  0.009581015445291996,\n",
       "  -0.03156454116106033,\n",
       "  -0.03216807171702385,\n",
       "  0.028214959427714348,\n",
       "  0.03046310320496559,\n",
       "  -0.014839258044958115,\n",
       "  0.02162140980362892,\n",
       "  -0.03180595487356186,\n",
       "  -0.0018086995696648955,\n",
       "  0.01727600395679474,\n",
       "  0.01876973733305931,\n",
       "  -0.05483056977391243,\n",
       "  -0.005175257101655006,\n",
       "  0.008947310969233513,\n",
       "  -0.011429321952164173,\n",
       "  0.0010184544371441007,\n",
       "  -0.01293059904128313,\n",
       "  0.004933845717459917,\n",
       "  0.016415975987911224,\n",
       "  -0.005612815264612436,\n",
       "  0.04764857888221741,\n",
       "  -0.009068015962839127,\n",
       "  -0.01411502342671156,\n",
       "  0.0008793599554337561,\n",
       "  -0.00477541983127594,\n",
       "  -0.011746174655854702,\n",
       "  0.001963353715837002,\n",
       "  -0.009460309520363808,\n",
       "  0.02759634330868721,\n",
       "  0.024337287992239,\n",
       "  -0.002455606823787093,\n",
       "  -0.007808150723576546,\n",
       "  -0.05486074462532997,\n",
       "  0.04773910716176033,\n",
       "  -0.006499248091131449,\n",
       "  0.0008571991347707808,\n",
       "  0.016068946570158005,\n",
       "  0.03962164744734764,\n",
       "  -0.0014767588581889868,\n",
       "  -0.008072194643318653,\n",
       "  0.02723422646522522,\n",
       "  -0.0051941173151135445,\n",
       "  -0.007578055374324322,\n",
       "  -0.005175257101655006,\n",
       "  0.022692672908306122,\n",
       "  0.010071382857859135,\n",
       "  -0.01414520014077425,\n",
       "  -0.005616587586700916,\n",
       "  0.01005629450082779,\n",
       "  0.020987704396247864,\n",
       "  0.00849466398358345,\n",
       "  0.0044321627356112,\n",
       "  -0.031021367758512497,\n",
       "  0.026404373347759247,\n",
       "  0.014944875612854958,\n",
       "  0.015337169170379639,\n",
       "  0.0113010723143816,\n",
       "  0.01762303337454796,\n",
       "  0.02756616659462452,\n",
       "  0.02233055606484413,\n",
       "  -0.04070800170302391,\n",
       "  0.0013767995405942202,\n",
       "  -0.002261345973238349,\n",
       "  -0.016385799273848534,\n",
       "  -0.008434311486780643,\n",
       "  0.006178623531013727,\n",
       "  0.025363286957144737,\n",
       "  0.0005210148519836366,\n",
       "  -0.04221682250499725,\n",
       "  -0.0023349011316895485,\n",
       "  0.009837515652179718,\n",
       "  0.03989323601126671,\n",
       "  -0.007344188168644905,\n",
       "  0.033797599375247955,\n",
       "  0.03225859999656677,\n",
       "  -0.018996061757206917,\n",
       "  0.0011042685946449637,\n",
       "  0.019629765301942825,\n",
       "  0.006838732864707708,\n",
       "  -0.012115835212171078,\n",
       "  0.0008100484847091138,\n",
       "  0.045143935829401016,\n",
       "  0.008969943039119244,\n",
       "  0.0011617924319580197,\n",
       "  0.053170863538980484,\n",
       "  -0.03578924387693405,\n",
       "  -0.029859574511647224,\n",
       "  0.03256036341190338,\n",
       "  0.009588560089468956,\n",
       "  0.0001943786337506026,\n",
       "  0.006340821739286184,\n",
       "  -0.037992123514413834,\n",
       "  -0.0028384702745825052,\n",
       "  0.006446439307183027,\n",
       "  -0.019750472158193588,\n",
       "  -0.0038701267912983894,\n",
       "  0.02118385210633278,\n",
       "  0.016702651977539062,\n",
       "  -0.0326508954167366,\n",
       "  0.016446152701973915,\n",
       "  0.043967053294181824,\n",
       "  -0.008736075833439827,\n",
       "  0.019750472158193588,\n",
       "  0.016370711848139763,\n",
       "  -0.0051111322827637196,\n",
       "  -0.036935947835445404,\n",
       "  0.05241645500063896,\n",
       "  -0.035547830164432526,\n",
       "  0.007578055374324322,\n",
       "  -0.006570917088538408,\n",
       "  0.04167364537715912,\n",
       "  -0.0047376989386975765,\n",
       "  -0.04444987699389458,\n",
       "  0.014409244060516357,\n",
       "  0.0028309260960668325,\n",
       "  -0.03253018856048584,\n",
       "  -0.008011841215193272,\n",
       "  0.012621290981769562,\n",
       "  0.014567670412361622,\n",
       "  0.02196843922138214,\n",
       "  0.047165755182504654,\n",
       "  -0.007860959507524967,\n",
       "  -0.05652044713497162,\n",
       "  -0.010795616544783115,\n",
       "  0.052899278700351715,\n",
       "  0.04607940465211868,\n",
       "  -0.015299448743462563,\n",
       "  -0.008705899119377136,\n",
       "  0.0137755386531353,\n",
       "  0.003236421849578619,\n",
       "  0.027701960876584053,\n",
       "  -0.0367850661277771,\n",
       "  0.019976794719696045,\n",
       "  -0.013692554086446762,\n",
       "  0.011678277514874935,\n",
       "  0.01075789611786604,\n",
       "  0.009731898084282875,\n",
       "  -0.03292248025536537,\n",
       "  0.011006851680576801,\n",
       "  -0.009460309520363808,\n",
       "  0.016129299998283386,\n",
       "  0.0032137895468622446,\n",
       "  -0.006137130782008171,\n",
       "  -0.004051185213029385,\n",
       "  -0.023145319893956184,\n",
       "  -0.03536677360534668,\n",
       "  -0.013933965004980564,\n",
       "  -0.03071960248053074,\n",
       "  -0.020685940980911255,\n",
       "  0.030312221497297287,\n",
       "  0.007287607062608004,\n",
       "  -0.020429441705346107,\n",
       "  -0.02442781813442707,\n",
       "  0.0054619330912828445,\n",
       "  -0.0011042685946449637,\n",
       "  -0.004341633524745703,\n",
       "  0.011089837178587914,\n",
       "  0.019297825172543526,\n",
       "  0.0040134647861123085,\n",
       "  0.03578924387693405,\n",
       "  0.057154152542352676,\n",
       "  -0.004609449300915003,\n",
       "  0.011218086816370487,\n",
       "  0.03470288962125778,\n",
       "  -0.020414352416992188,\n",
       "  -0.06469825655221939,\n",
       "  -0.0297690462321043,\n",
       "  0.02685702033340931,\n",
       "  0.011942321434617043,\n",
       "  0.0077100773341953754,\n",
       "  0.0007893021684139967,\n",
       "  -0.011346336454153061,\n",
       "  0.03443130478262901,\n",
       "  -0.012010217644274235,\n",
       "  0.04043641313910484,\n",
       "  -0.0065445126965641975,\n",
       "  -0.010063838213682175,\n",
       "  -0.03947076573967934,\n",
       "  -0.02231546863913536,\n",
       "  -0.008343782275915146,\n",
       "  0.014288538135588169,\n",
       "  -0.00038616397068835795,\n",
       "  0.031262777745723724,\n",
       "  -0.019524147734045982,\n",
       "  0.021817557513713837,\n",
       "  -0.0011042685946449637,\n",
       "  -0.007366820238530636,\n",
       "  0.006073005963116884,\n",
       "  -0.04312211275100708,\n",
       "  -0.011112469248473644,\n",
       "  -0.005907035432755947,\n",
       "  -0.013994318433105946,\n",
       "  -0.006853821221739054,\n",
       "  -0.004186979494988918,\n",
       "  -0.006533196661621332,\n",
       "  0.03150419145822525,\n",
       "  0.03473306819796562,\n",
       "  -0.008909590542316437,\n",
       "  0.02472958154976368,\n",
       "  -0.016023682430386543,\n",
       "  -0.01434134691953659,\n",
       "  0.001671019708737731,\n",
       "  -0.0027818894013762474,\n",
       "  0.017834268510341644,\n",
       "  -0.015012772753834724,\n",
       "  -0.058874208480119705,\n",
       "  -0.006103182211518288,\n",
       "  -0.00830606184899807,\n",
       "  -0.01620474085211754,\n",
       "  -0.004179435316473246,\n",
       "  0.023567790165543556,\n",
       "  -0.028833575546741486,\n",
       "  0.019946618005633354,\n",
       "  0.004952705930918455,\n",
       "  0.01468083169311285,\n",
       "  -0.03853529691696167,\n",
       "  0.027701960876584053,\n",
       "  0.011761262081563473,\n",
       "  0.00864554662257433,\n",
       "  -0.004918757826089859,\n",
       "  0.008600281551480293,\n",
       "  -0.0006412490620277822,\n",
       "  0.023854466155171394,\n",
       "  0.023914817720651627,\n",
       "  -0.016536682844161987,\n",
       "  0.02408078871667385,\n",
       "  -0.02682684361934662,\n",
       "  0.00016620609676465392,\n",
       "  0.00974698644131422,\n",
       "  -0.02973886951804161,\n",
       "  0.024261847138404846,\n",
       "  0.012885333970189095,\n",
       "  0.018860267475247383,\n",
       "  -0.023431995883584023,\n",
       "  -0.01058438140898943,\n",
       "  -0.011708453297615051,\n",
       "  0.023100055754184723,\n",
       "  0.0031364622991532087,\n",
       "  -0.049972161650657654,\n",
       "  0.013405878096818924,\n",
       "  0.005488337483257055,\n",
       "  -0.005488337483257055,\n",
       "  -0.014582758769392967,\n",
       "  -0.011912144720554352,\n",
       "  0.011972497217357159,\n",
       "  0.015767183154821396,\n",
       "  0.01434889156371355,\n",
       "  -0.021319646388292313,\n",
       "  0.018181297928094864,\n",
       "  0.005661852192133665,\n",
       "  -0.007193305995315313,\n",
       "  0.027475636452436447,\n",
       "  -0.024156229570508003,\n",
       "  -0.0001422771456418559,\n",
       "  -0.008426766842603683,\n",
       "  -0.006785924080759287,\n",
       "  -0.0036720940843224525,\n",
       "  0.008766252547502518,\n",
       "  0.012244085781276226,\n",
       "  -0.004349177703261375,\n",
       "  0.037630002945661545,\n",
       "  0.0033288372214883566,\n",
       "  0.06512072682380676,\n",
       "  -0.04351440817117691,\n",
       "  -0.01732126995921135,\n",
       "  0.025876285508275032,\n",
       "  0.011889512650668621,\n",
       "  0.008004297502338886,\n",
       "  0.029497457668185234,\n",
       "  0.020625587552785873,\n",
       "  -0.007464893627911806,\n",
       "  0.025001170113682747,\n",
       "  0.007132953032851219,\n",
       "  -0.0331638939678669,\n",
       "  0.023824289441108704,\n",
       "  0.05009286850690842,\n",
       "  0.010071382857859135,\n",
       "  -0.007574283052235842,\n",
       "  -0.025710316374897957,\n",
       "  -0.013850980438292027,\n",
       "  0.006054145749658346,\n",
       "  -0.02017294242978096,\n",
       "  -0.0018671664875000715,\n",
       "  0.006518108304589987,\n",
       "  0.013375701382756233,\n",
       "  -0.11298054456710815,\n",
       "  -0.008404134772717953,\n",
       "  0.01626509428024292,\n",
       "  0.007193305995315313,\n",
       "  -0.012130923569202423,\n",
       "  0.04137188196182251,\n",
       "  0.01588788814842701,\n",
       "  0.023567790165543556,\n",
       "  0.004715066868811846,\n",
       "  -0.033767420798540115,\n",
       "  0.02720404975116253,\n",
       "  0.004085133783519268,\n",
       "  -0.010139279998838902,\n",
       "  -0.008728531189262867,\n",
       "  -0.009052928537130356,\n",
       "  0.03298283368349075,\n",
       "  -0.015525772236287594,\n",
       "  0.040557119995355606,\n",
       "  -0.02151579223573208,\n",
       "  -0.012870246544480324,\n",
       "  -0.027490725740790367,\n",
       "  0.0438765250146389,\n",
       "  0.000983562902547419,\n",
       "  0.0060466015711426735,\n",
       "  -0.025740493088960648,\n",
       "  0.010659823194146156,\n",
       "  -0.046139758080244064,\n",
       "  -0.015420154668390751,\n",
       "  0.031624894589185715,\n",
       "  -0.027867930009961128,\n",
       "  0.050394631922245026,\n",
       "  -0.008826605044305325,\n",
       "  0.0019246902083978057,\n",
       "  -0.026328932493925095,\n",
       "  -0.002704562386497855,\n",
       "  -0.013217275030910969,\n",
       "  -0.020670853555202484,\n",
       "  -0.01536734588444233,\n",
       "  0.04390669986605644,\n",
       "  0.00433408934623003,\n",
       "  0.008547472767531872,\n",
       "  -0.03793177008628845,\n",
       "  -0.030659250915050507,\n",
       "  0.026419462636113167,\n",
       "  -0.0031458926387131214,\n",
       "  0.01593315415084362,\n",
       "  0.0013098455965518951,\n",
       "  -0.03503483161330223,\n",
       "  -0.014092391356825829,\n",
       "  -0.005401580594480038,\n",
       "  0.020278559997677803,\n",
       "  0.00989032443612814,\n",
       "  0.02860725298523903,\n",
       "  0.002498985268175602,\n",
       "  0.0077063050121068954,\n",
       "  0.03844476863741875,\n",
       "  -0.009392413310706615,\n",
       "  -0.03639277070760727,\n",
       "  0.022058967500925064,\n",
       "  0.038746532052755356,\n",
       "  0.00796657707542181,\n",
       "  -0.0032722563482820988,\n",
       "  -0.02408078871667385,\n",
       "  -0.015058036893606186,\n",
       "  0.0008373958407901227,\n",
       "  -0.006299328990280628,\n",
       "  -0.03925953060388565,\n",
       "  0.01984100043773651,\n",
       "  -0.009611192159354687,\n",
       "  -0.012078114785254002,\n",
       "  0.01914694346487522,\n",
       "  -0.017049681395292282,\n",
       "  -0.001135388039983809,\n",
       "  -0.01276462897658348,\n",
       "  0.007012247107923031,\n",
       "  -0.03286213055253029,\n",
       "  0.004220927599817514,\n",
       "  0.009105737321078777,\n",
       "  -0.012387423776090145,\n",
       "  -0.016114212572574615,\n",
       "  0.02119893953204155,\n",
       "  0.04266946762800217,\n",
       "  0.0019426074577495456,\n",
       "  -0.012855158187448978,\n",
       "  -0.005993792787194252,\n",
       "  -0.010554205626249313,\n",
       "  0.001737973652780056,\n",
       "  0.0059447563253343105,\n",
       "  -0.03509518504142761,\n",
       "  0.018256738781929016,\n",
       "  0.05609797686338425,\n",
       "  0.018603768199682236,\n",
       "  -0.019886266440153122,\n",
       "  -0.02163649909198284,\n",
       "  -0.048614222556352615,\n",
       "  -0.012093203142285347,\n",
       "  -0.0017860672669485211,\n",
       "  -0.03008589707314968,\n",
       "  0.0006836846587248147,\n",
       "  0.009769618511199951,\n",
       "  0.030221691355109215,\n",
       "  -0.00655205687507987,\n",
       "  0.011580203659832478,\n",
       "  -0.017411798238754272,\n",
       "  -0.03708682954311371,\n",
       "  -0.02545381709933281,\n",
       "  0.005363859701901674,\n",
       "  0.018588678911328316,\n",
       "  0.01623491756618023,\n",
       "  -0.023975171148777008,\n",
       "  0.009694177657365799,\n",
       "  0.010184544138610363,\n",
       "  0.010169455781579018,\n",
       "  0.05045498535037041,\n",
       "  0.025378374382853508,\n",
       "  0.029271135106682777,\n",
       "  0.018980972468852997,\n",
       "  -0.010041206143796444,\n",
       "  -0.020323824137449265,\n",
       "  -0.037961944937705994,\n",
       "  -0.005778785794973373,\n",
       "  0.032379306852817535,\n",
       "  0.006944350432604551,\n",
       "  0.007321555633097887,\n",
       "  -0.026027169078588486,\n",
       "  0.02657034434378147,\n",
       "  -0.031262777745723724,\n",
       "  0.03283195197582245,\n",
       "  -0.010795616544783115,\n",
       "  0.029874663800001144,\n",
       "  0.009694177657365799,\n",
       "  0.004979110322892666,\n",
       "  0.024608876556158066,\n",
       "  0.030478190630674362,\n",
       "  0.026645785197615623,\n",
       "  -0.021847732365131378,\n",
       "  -0.002331129042431712,\n",
       "  0.00547702144831419,\n",
       "  0.019267648458480835,\n",
       "  -0.018799914047122,\n",
       "  0.029240958392620087,\n",
       "  -0.030523456633090973,\n",
       "  0.022013703361153603,\n",
       "  0.02684193290770054,\n",
       "  -0.013451142236590385,\n",
       "  -0.03108171932399273,\n",
       "  -0.015571036376059055,\n",
       "  0.048221930861473083,\n",
       "  -0.022753026336431503,\n",
       "  0.017547592520713806,\n",
       "  -0.003753193188458681,\n",
       "  -0.004070045426487923,\n",
       "  0.009852603077888489,\n",
       "  -0.010848425328731537,\n",
       "  -0.008751164190471172,\n",
       "  0.010018574073910713,\n",
       "  -0.021364910528063774,\n",
       "  0.028471458703279495,\n",
       "  0.024684317409992218,\n",
       "  0.039440590888261795,\n",
       "  0.005759925581514835,\n",
       "  0.041190821677446365,\n",
       "  -0.020459618419408798,\n",
       "  -0.012749540619552135,\n",
       "  0.012915510684251785,\n",
       "  0.017804091796278954,\n",
       "  -0.024865375831723213,\n",
       "  -0.013277627527713776,\n",
       "  -0.0010769212385639548,\n",
       "  0.008547472767531872,\n",
       "  -0.022179674357175827,\n",
       "  0.04110029339790344,\n",
       "  0.004662258084863424,\n",
       "  0.022360732778906822,\n",
       "  -0.016400888562202454,\n",
       "  -0.0026781579945236444,\n",
       "  0.016159476712346077,\n",
       "  -0.030116073787212372,\n",
       "  -0.000811462989076972,\n",
       "  0.0381128266453743,\n",
       "  -0.007208393886685371,\n",
       "  -0.0022104233503341675,\n",
       "  -0.028743047267198563,\n",
       "  0.047920167446136475,\n",
       "  0.03895776718854904,\n",
       "  -0.0028969370760023594,\n",
       "  0.02370358444750309,\n",
       "  0.0030346170533448458,\n",
       "  0.008396591059863567,\n",
       "  0.05965879559516907,\n",
       "  0.0247446708381176,\n",
       "  0.012923055328428745,\n",
       "  -0.034491654485464096,\n",
       "  -0.0047188387252390385,\n",
       "  -0.0024574927520006895,\n",
       "  -0.007687444798648357,\n",
       "  -0.0015521999448537827,\n",
       "  -0.0002363427192904055,\n",
       "  -0.002223625546321273,\n",
       "  0.016355624422430992,\n",
       "  -0.022798290476202965,\n",
       "  0.030629074200987816,\n",
       "  0.04058729484677315,\n",
       "  ...]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac680c80-db7f-4925-b3d7-7147000724a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\otimi\\anaconda3\\envs\\quadrant\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n",
       "    num_rows: 25\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tqdm as notebook_tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"infoslack/mistral-7b-arxiv-paper-chunked\", split=\"train\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e041dd5-11da-48f7-9325-bf728f32be3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doi': '2310.06825',\n",
       " 'chunk-id': '0',\n",
       " 'chunk': 'Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src',\n",
       " 'id': '2310.06825',\n",
       " 'title': 'Mistral 7B',\n",
       " 'summary': 'We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\\nfor superior performance and efficiency. Mistral 7B outperforms Llama 2 13B\\nacross all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\\ncode generation. Our model leverages grouped-query attention (GQA) for faster\\ninference, coupled with sliding window attention (SWA) to effectively handle\\nsequences of arbitrary length with a reduced inference cost. We also provide a\\nmodel fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses\\nthe Llama 2 13B -- Chat model both on human and automated benchmarks. Our\\nmodels are released under the Apache 2.0 license.',\n",
       " 'source': 'http://arxiv.org/pdf/2310.06825',\n",
       " 'authors': ['Albert Q. Jiang',\n",
       "  'Alexandre Sablayrolles',\n",
       "  'Arthur Mensch',\n",
       "  'Chris Bamford',\n",
       "  'Devendra Singh Chaplot',\n",
       "  'Diego de las Casas',\n",
       "  'Florian Bressand',\n",
       "  'Gianna Lengyel',\n",
       "  'Guillaume Lample',\n",
       "  'Lucile Saulnier',\n",
       "  'Lélio Renard Lavaud',\n",
       "  'Marie-Anne Lachaux',\n",
       "  'Pierre Stock',\n",
       "  'Teven Le Scao',\n",
       "  'Thibaut Lavril',\n",
       "  'Thomas Wang',\n",
       "  'Timothée Lacroix',\n",
       "  'William El Sayed'],\n",
       " 'categories': ['cs.CL', 'cs.AI', 'cs.LG'],\n",
       " 'comment': 'Models and code are available at\\n  https://mistral.ai/news/announcing-mistral-7b/',\n",
       " 'journal_ref': None,\n",
       " 'primary_category': 'cs.CL',\n",
       " 'published': '20231010',\n",
       " 'updated': '20231010',\n",
       " 'references': [{'id': '1808.07036'},\n",
       "  {'id': '1809.02789'},\n",
       "  {'id': '1904.10509'},\n",
       "  {'id': '2302.13971'},\n",
       "  {'id': '2009.03300'},\n",
       "  {'id': '2305.13245'},\n",
       "  {'id': '1904.09728'},\n",
       "  {'id': '1803.05457'},\n",
       "  {'id': '2103.03874'},\n",
       "  {'id': '1905.07830'},\n",
       "  {'id': '2308.12950'},\n",
       "  {'id': '2210.09261'},\n",
       "  {'id': '2310.06825'},\n",
       "  {'id': '2307.09288'},\n",
       "  {'id': '2304.06364'},\n",
       "  {'id': '1905.10044'},\n",
       "  {'id': '2110.14168'},\n",
       "  {'id': '2108.07732'},\n",
       "  {'id': '2107.03374'},\n",
       "  {'id': '1811.00937'},\n",
       "  {'id': '2004.05150'},\n",
       "  {'id': '1705.03551'}]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3df3cbff-614f-4cf3-b547-1d92481c060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc2a78c8-5acc-4625-990b-2141cfbab652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doi</th>\n",
       "      <th>chunk-id</th>\n",
       "      <th>chunk</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>source</th>\n",
       "      <th>authors</th>\n",
       "      <th>categories</th>\n",
       "      <th>comment</th>\n",
       "      <th>journal_ref</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>published</th>\n",
       "      <th>updated</th>\n",
       "      <th>references</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2310.06825</td>\n",
       "      <td>0</td>\n",
       "      <td>Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayr...</td>\n",
       "      <td>2310.06825</td>\n",
       "      <td>Mistral 7B</td>\n",
       "      <td>We introduce Mistral 7B v0.1, a 7-billion-para...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "      <td>[Albert Q. Jiang, Alexandre Sablayrolles, Arth...</td>\n",
       "      <td>[cs.CL, cs.AI, cs.LG]</td>\n",
       "      <td>Models and code are available at\\n  https://mi...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20231010</td>\n",
       "      <td>20231010</td>\n",
       "      <td>[{'id': '1808.07036'}, {'id': '1809.02789'}, {...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2310.06825</td>\n",
       "      <td>1</td>\n",
       "      <td>automated benchmarks. Our models are released ...</td>\n",
       "      <td>2310.06825</td>\n",
       "      <td>Mistral 7B</td>\n",
       "      <td>We introduce Mistral 7B v0.1, a 7-billion-para...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "      <td>[Albert Q. Jiang, Alexandre Sablayrolles, Arth...</td>\n",
       "      <td>[cs.CL, cs.AI, cs.LG]</td>\n",
       "      <td>Models and code are available at\\n  https://mi...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20231010</td>\n",
       "      <td>20231010</td>\n",
       "      <td>[{'id': '1808.07036'}, {'id': '1809.02789'}, {...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2310.06825</td>\n",
       "      <td>2</td>\n",
       "      <td>GQA significantly accelerates the inference sp...</td>\n",
       "      <td>2310.06825</td>\n",
       "      <td>Mistral 7B</td>\n",
       "      <td>We introduce Mistral 7B v0.1, a 7-billion-para...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "      <td>[Albert Q. Jiang, Alexandre Sablayrolles, Arth...</td>\n",
       "      <td>[cs.CL, cs.AI, cs.LG]</td>\n",
       "      <td>Models and code are available at\\n  https://mi...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20231010</td>\n",
       "      <td>20231010</td>\n",
       "      <td>[{'id': '1808.07036'}, {'id': '1809.02789'}, {...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2310.06825</td>\n",
       "      <td>3</td>\n",
       "      <td>Mistral 7B takes a significant step in balanci...</td>\n",
       "      <td>2310.06825</td>\n",
       "      <td>Mistral 7B</td>\n",
       "      <td>We introduce Mistral 7B v0.1, a 7-billion-para...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "      <td>[Albert Q. Jiang, Alexandre Sablayrolles, Arth...</td>\n",
       "      <td>[cs.CL, cs.AI, cs.LG]</td>\n",
       "      <td>Models and code are available at\\n  https://mi...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20231010</td>\n",
       "      <td>20231010</td>\n",
       "      <td>[{'id': '1808.07036'}, {'id': '1809.02789'}, {...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2310.06825</td>\n",
       "      <td>4</td>\n",
       "      <td>parameters of the architecture are summarized ...</td>\n",
       "      <td>2310.06825</td>\n",
       "      <td>Mistral 7B</td>\n",
       "      <td>We introduce Mistral 7B v0.1, a 7-billion-para...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "      <td>[Albert Q. Jiang, Alexandre Sablayrolles, Arth...</td>\n",
       "      <td>[cs.CL, cs.AI, cs.LG]</td>\n",
       "      <td>Models and code are available at\\n  https://mi...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20231010</td>\n",
       "      <td>20231010</td>\n",
       "      <td>[{'id': '1808.07036'}, {'id': '1809.02789'}, {...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          doi chunk-id                                              chunk  \\\n",
       "0  2310.06825        0  Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayr...   \n",
       "1  2310.06825        1  automated benchmarks. Our models are released ...   \n",
       "2  2310.06825        2  GQA significantly accelerates the inference sp...   \n",
       "3  2310.06825        3  Mistral 7B takes a significant step in balanci...   \n",
       "4  2310.06825        4  parameters of the architecture are summarized ...   \n",
       "\n",
       "           id       title                                            summary  \\\n",
       "0  2310.06825  Mistral 7B  We introduce Mistral 7B v0.1, a 7-billion-para...   \n",
       "1  2310.06825  Mistral 7B  We introduce Mistral 7B v0.1, a 7-billion-para...   \n",
       "2  2310.06825  Mistral 7B  We introduce Mistral 7B v0.1, a 7-billion-para...   \n",
       "3  2310.06825  Mistral 7B  We introduce Mistral 7B v0.1, a 7-billion-para...   \n",
       "4  2310.06825  Mistral 7B  We introduce Mistral 7B v0.1, a 7-billion-para...   \n",
       "\n",
       "                            source  \\\n",
       "0  http://arxiv.org/pdf/2310.06825   \n",
       "1  http://arxiv.org/pdf/2310.06825   \n",
       "2  http://arxiv.org/pdf/2310.06825   \n",
       "3  http://arxiv.org/pdf/2310.06825   \n",
       "4  http://arxiv.org/pdf/2310.06825   \n",
       "\n",
       "                                             authors             categories  \\\n",
       "0  [Albert Q. Jiang, Alexandre Sablayrolles, Arth...  [cs.CL, cs.AI, cs.LG]   \n",
       "1  [Albert Q. Jiang, Alexandre Sablayrolles, Arth...  [cs.CL, cs.AI, cs.LG]   \n",
       "2  [Albert Q. Jiang, Alexandre Sablayrolles, Arth...  [cs.CL, cs.AI, cs.LG]   \n",
       "3  [Albert Q. Jiang, Alexandre Sablayrolles, Arth...  [cs.CL, cs.AI, cs.LG]   \n",
       "4  [Albert Q. Jiang, Alexandre Sablayrolles, Arth...  [cs.CL, cs.AI, cs.LG]   \n",
       "\n",
       "                                             comment journal_ref  \\\n",
       "0  Models and code are available at\\n  https://mi...        None   \n",
       "1  Models and code are available at\\n  https://mi...        None   \n",
       "2  Models and code are available at\\n  https://mi...        None   \n",
       "3  Models and code are available at\\n  https://mi...        None   \n",
       "4  Models and code are available at\\n  https://mi...        None   \n",
       "\n",
       "  primary_category published   updated  \\\n",
       "0            cs.CL  20231010  20231010   \n",
       "1            cs.CL  20231010  20231010   \n",
       "2            cs.CL  20231010  20231010   \n",
       "3            cs.CL  20231010  20231010   \n",
       "4            cs.CL  20231010  20231010   \n",
       "\n",
       "                                          references  \n",
       "0  [{'id': '1808.07036'}, {'id': '1809.02789'}, {...  \n",
       "1  [{'id': '1808.07036'}, {'id': '1809.02789'}, {...  \n",
       "2  [{'id': '1808.07036'}, {'id': '1809.02789'}, {...  \n",
       "3  [{'id': '1808.07036'}, {'id': '1809.02789'}, {...  \n",
       "4  [{'id': '1808.07036'}, {'id': '1809.02789'}, {...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86d1699d-4010-4c54-b769-cba216878984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayr...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>automated benchmarks. Our models are released ...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GQA significantly accelerates the inference sp...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mistral 7B takes a significant step in balanci...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>parameters of the architecture are summarized ...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               chunk  \\\n",
       "0  Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayr...   \n",
       "1  automated benchmarks. Our models are released ...   \n",
       "2  GQA significantly accelerates the inference sp...   \n",
       "3  Mistral 7B takes a significant step in balanci...   \n",
       "4  parameters of the architecture are summarized ...   \n",
       "\n",
       "                            source  \n",
       "0  http://arxiv.org/pdf/2310.06825  \n",
       "1  http://arxiv.org/pdf/2310.06825  \n",
       "2  http://arxiv.org/pdf/2310.06825  \n",
       "3  http://arxiv.org/pdf/2310.06825  \n",
       "4  http://arxiv.org/pdf/2310.06825  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = data[['chunk', 'source']]\n",
    "docs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dfe399e6-e0a7-4f98-88dc-ecdc28d445bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce99f4d0-391c-41f1-95fa-840d34668a71",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02c04511-834c-45ca-b6a6-223d2bfb66ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "\n",
    "loader = DataFrameLoader(docs, page_content_column=\"chunk\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2240c2f7-0339-471b-9e40-09882fc503aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'http://arxiv.org/pdf/2310.06825'}, page_content='Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2325928d-285f-4781-8c18-c6dcaaed8907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'http://arxiv.org/pdf/2310.06825'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec4bc1df-26b9-489c-b121-147a7fc609af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", openai_api_key=openai_api_key)\n",
    "\n",
    "url = os.getenv(\"QDRANT_URL\")\n",
    "api_key = os.getenv(\"QDRANT_KEY\")\n",
    "\n",
    "# https://cloud.qdrant.io/accounts/abd7a63a-ef6d-4b15-bcbf-c01a7f8cba55/overview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd3287df-f040-4020-82b8-8e6b5ded21f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "qdrant_client = QdrantClient(\n",
    "    url=url,\n",
    "    api_key=api_key,\n",
    ")\n",
    "\n",
    "#print(qdrant_client.get_collections())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80e1377b-8197-4383-823d-8d77d1976780",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant = Qdrant.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    "    url=url,\n",
    "    collection_name=\"sympathetic-sawfish-maroon\",\n",
    "    api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "600e3615-f2c7-4175-ad21-cdc7b1f69b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'http://arxiv.org/pdf/2310.06825', '_id': '6572eebf-611a-4133-92a2-b458cd67f741', '_collection_name': 'sympathetic-sawfish-maroon'}, page_content='Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2310.06825', '_id': '2d2f5c70-8910-4195-9430-b334d7f8bf4d', '_collection_name': 'sympathetic-sawfish-maroon'}, page_content='Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2310.06825', '_id': '4785f1fa-c257-4564-92f7-2509acb9a89b', '_collection_name': 'sympathetic-sawfish-maroon'}, page_content='Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"O que há de tão especial no Mistral 7B? Responda em portugues em um único parágrafo\"\n",
    "qdrant.similarity_search(query, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eabeb994-35c4-421a-8f76-559826ec87d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_prompt(query: str):\n",
    "    results = qdrant.similarity_search(query, k=3)\n",
    "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "    augment_prompt = f\"\"\"Using the contexts below, answer the query:\n",
    "\n",
    "    Contexts:\n",
    "    {source_knowledge}\n",
    "\n",
    "    Query: {query}\"\"\"\n",
    "    return augment_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f9529893-611d-4db8-8f30-8117ecd897ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the contexts below, answer the query:\n",
      "\n",
      "    Contexts:\n",
      "    Mistral 7B\n",
      "Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\n",
      "Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\n",
      "Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\n",
      "Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\n",
      "William El Sayed\n",
      "Abstract\n",
      "We introduce Mistral 7B, a 7–billion-parameter language model engineered for\n",
      "superior performance and efficiency. Mistral 7B outperforms the best open 13B\n",
      "model (Llama 2) across all evaluated benchmarks, and the best released 34B\n",
      "model (Llama 1) in reasoning, mathematics, and code generation. Our model\n",
      "leverages grouped-query attention (GQA) for faster inference, coupled with sliding\n",
      "window attention (SWA) to effectively handle sequences of arbitrary length with a\n",
      "reduced inference cost. We also provide a model fine-tuned to follow instructions,\n",
      "Mistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\n",
      "automated benchmarks. Our models are released under the Apache 2.0 license.\n",
      "Code: https://github.com/mistralai/mistral-src\n",
      "Mistral 7B\n",
      "Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\n",
      "Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\n",
      "Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\n",
      "Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\n",
      "William El Sayed\n",
      "Abstract\n",
      "We introduce Mistral 7B, a 7–billion-parameter language model engineered for\n",
      "superior performance and efficiency. Mistral 7B outperforms the best open 13B\n",
      "model (Llama 2) across all evaluated benchmarks, and the best released 34B\n",
      "model (Llama 1) in reasoning, mathematics, and code generation. Our model\n",
      "leverages grouped-query attention (GQA) for faster inference, coupled with sliding\n",
      "window attention (SWA) to effectively handle sequences of arbitrary length with a\n",
      "reduced inference cost. We also provide a model fine-tuned to follow instructions,\n",
      "Mistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\n",
      "automated benchmarks. Our models are released under the Apache 2.0 license.\n",
      "Code: https://github.com/mistralai/mistral-src\n",
      "Mistral 7B\n",
      "Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\n",
      "Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\n",
      "Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\n",
      "Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\n",
      "William El Sayed\n",
      "Abstract\n",
      "We introduce Mistral 7B, a 7–billion-parameter language model engineered for\n",
      "superior performance and efficiency. Mistral 7B outperforms the best open 13B\n",
      "model (Llama 2) across all evaluated benchmarks, and the best released 34B\n",
      "model (Llama 1) in reasoning, mathematics, and code generation. Our model\n",
      "leverages grouped-query attention (GQA) for faster inference, coupled with sliding\n",
      "window attention (SWA) to effectively handle sequences of arbitrary length with a\n",
      "reduced inference cost. We also provide a model fine-tuned to follow instructions,\n",
      "Mistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\n",
      "automated benchmarks. Our models are released under the Apache 2.0 license.\n",
      "Code: https://github.com/mistralai/mistral-src\n",
      "\n",
      "    Query: O que há de tão especial no Mistral 7B? Responda em portugues em um único parágrafo\n"
     ]
    }
   ],
   "source": [
    "print(custom_prompt(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b58d584-eed7-40b4-a762-aad0f4ce2cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O Mistral 7B é um modelo de linguagem com 7 bilhões de parâmetros projetado para oferecer desempenho e eficiência superiores, superando o melhor modelo aberto de 13 bilhões de parâmetros (Llama 2) em todas as avaliações e também superando o modelo liberado de 34 bilhões (Llama 1) em tarefas de raciocínio, matemática e geração de código. Ele utiliza uma técnica chamada atenção de consulta agrupada (GQA) para proporcionar inferência mais rápida e uma atenção de janela deslizante (SWA) que permite lidar com sequências de comprimento arbitrário com um custo de inferência reduzido. Além disso, o modelo Mistral 7B – Instruct é ajustado para seguir instruções, demonstrando desempenho superior ao modelo de chat Llama 2 de 13 bilhões tanto em benchmarks humanos quanto automatizados. Todos os modelos estão disponíveis sob a licença Apache 2.0.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=custom_prompt(query)\n",
    ")\n",
    "\n",
    "messages.append(prompt)\n",
    "\n",
    "res = chat.invoke(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ab60f0-d513-4de5-84e4-b65559bd9c12",
   "metadata": {},
   "source": [
    "## Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc72e141-52dd-4377-9ee2-b239af041c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "chat = ChatGroq(temperature=0, model_name=\"mixtral-8x7b-32768\", max_tokens=512, api_key=groq_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b452f061-ecf8-4c40-b5e4-a606892adee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O Mistral 7B é um modelo de linguagem com 7 bilhões de parâmetros, projetado para oferecer alta performance e eficiência. Ele supera o melhor modelo aberto de 13 bilhões de parâmetros (Llama 2) em todos os benchmarks avaliados e o melhor modelo lançado de 34 bilhões de parâmetros (Llama 1) em razãoamento, matemática e geração de código. O Mistral 7B utiliza a atenção de consulta agrupada (GQA) para uma inferência mais rápida, combinada com a atenção de janela deslizante (SWA) para lidar com sequências de qualquer tamanho com um custo de inferência reduzido. Além disso, fornecemos um modelo aperfeiçoado para seguir instruções, o Mistral 7B – Instruct, que ultrapassa o modelo de chat Llama 2 de 13 bilhões de parâmetros tanto em benchmarks humanos quanto automatizados. Nossos modelos são lançados sob a licença Apache 2.0.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=custom_prompt(query)\n",
    ")\n",
    "\n",
    "messages.append(prompt)\n",
    "res = chat.invoke(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3ba80bd2-8d85-404c-af21-a1d0349753f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hi AI, how are you today?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"I'm great thank you. How can I help you?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"I'd like to understand machine learning.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Absolutely! Machine learning is a subset of artificial intelligence (AI) that focuses on the development of algorithms and statistical models that enable computers to perform specific tasks without using explicit instructions. Instead, they rely on patterns and inference from data.\\n\\nHere are some key concepts to help you understand machine learning better:\\n\\n### 1. **Types of Machine Learning:**\\n   - **Supervised Learning:** The model is trained on a labeled dataset, meaning the input comes with the correct output. The model learns to map inputs to outputs. Examples include classification and regression tasks.\\n   - **Unsupervised Learning:** The model works with unlabeled data and tries to find patterns or groupings in the data. Common techniques include clustering and dimensionality reduction.\\n   - **Semi-Supervised Learning:** A combination of supervised and unsupervised learning where the model is trained on a small amount of labeled data and a large amount of unlabeled data.\\n   - **Reinforcement Learning:** The model learns by interacting with an environment and receiving feedback in the form of rewards or penalties.\\n\\n### 2. **Key Terminology:**\\n   - **Dataset:** The collection of data used for training and testing the model.\\n   - **Features:** The individual measurable properties or characteristics of the data used as input for the model.\\n   - **Labels:** The output or target variable that the model is trying to predict in supervised learning.\\n   - **Model:** A mathematical representation of a process that makes predictions based on input data.\\n   - **Training:** The process of teaching the model using a dataset.\\n   - **Testing:** Evaluating the model's performance on a separate dataset that it hasn't seen before.\\n\\n### 3. **Common Algorithms:**\\n   - **Linear Regression:** Used for regression tasks to model the relationship between a dependent variable and one or more independent variables.\\n   - **Logistic Regression:** Used for binary classification tasks.\\n   - **Decision Trees:** A model that makes decisions based on answering a series of questions about the features.\\n   - **Support Vector Machines (SVM):** Used for classification tasks by finding the hyperplane that best separates different classes.\\n   - **Neural Networks:** Computational models inspired by the human brain that are particularly good at recognizing patterns in large datasets.\\n\\n### 4. **Steps in a Machine Learning Project:**\\n   1. **Define the Problem:** Clearly articulate what you want to solve.\\n   2. **Collect Data:** Gather the data needed for training and testing.\\n   3. **Preprocess Data\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 512, 'prompt_tokens': 51, 'total_tokens': 563, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'length', 'logprobs': None}, id='run-37bd960e-b627-4685-a81d-6a3363196522-0', usage_metadata={'input_tokens': 51, 'output_tokens': 512, 'total_tokens': 563, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}),\n",
       " HumanMessage(content='Whats the difference between supervised and unsupervised?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='The main difference between supervised and unsupervised learning lies in the type of data used for training and the goals of the learning process. Here’s a breakdown of the key distinctions:\\n\\n### Supervised Learning\\n\\n1. **Labeled Data:**\\n   - In supervised learning, the algorithm is trained on a labeled dataset. This means that each training example is paired with an output label or target value.\\n   - For example, if you\\'re building a model to predict house prices, your dataset might include features like the size of the house, the number of bedrooms, and the price (label) of each house.\\n\\n2. **Objective:**\\n   - The goal is to learn a mapping from inputs (features) to outputs (labels) so that the model can make accurate predictions on new, unseen data.\\n   - Common tasks include classification (predicting a category) and regression (predicting a continuous value).\\n\\n3. **Examples:**\\n   - Classifying emails as \"spam\" or \"not spam.\"\\n   - Predicting the price of a stock based on historical data.\\n   - Image recognition tasks where images are labeled with specific objects (e.g., cat, dog).\\n\\n### Unsupervised Learning\\n\\n1. **Unlabeled Data:**\\n   - In unsupervised learning, the algorithm is trained on data that does not have labeled outputs. The model tries to learn the underlying structure or distribution of the data without any explicit guidance on what the outputs should be.\\n   - For example, you might have a dataset of customer purchase behavior, but you don\\'t have labels indicating which customers belong to which segments.\\n\\n2. **Objective:**\\n   - The goal is to discover patterns, groupings, or relationships within the data. This might include identifying clusters of similar data points or reducing the dimensionality of the data for visualization.\\n   - Common tasks include clustering (grouping similar items) and association (finding rules that describe large portions of the data).\\n\\n3. **Examples:**\\n   - Grouping customers into segments based on purchasing behavior (customer segmentation).\\n   - Identifying topics in a collection of documents without predefined categories (topic modeling).\\n   - Anomaly detection to find unusual data points in datasets.\\n\\n### Summary\\n\\nIn summary, supervised learning requires labeled data and focuses on predicting outcomes, while unsupervised learning works with unlabeled data and aims to identify patterns or groupings in the data. Each approach has its own applications and use cases, depending on the nature of the problem and the available data.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 505, 'prompt_tokens': 581, 'total_tokens': 1086, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'stop', 'logprobs': None}, id='run-bcf8caa5-a444-4191-8cc3-f2e0d19a0e84-0', usage_metadata={'input_tokens': 581, 'output_tokens': 505, 'total_tokens': 1086, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}),\n",
       " HumanMessage(content='Pode me falar sobre o clima no Rio de Janeiro hoje?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Using the contexts below to answer the question.\\n\\nContexts:\\nHoje o clima no Rio de Janeiro está ensolarado\\n\\nQuestion: Pode me falar sobre o clima no Rio de Janeiro hoje?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Using the contexts below, answer the query:\\n\\n    Contexts:\\n    Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src\\nMistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src\\nMistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src\\n\\n    Query: O que há de tão especial no Mistral 7B? Responda em portugues em um único parágrafo', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e542fcf-2f02-4b13-bdb2-ae898f11ed43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
