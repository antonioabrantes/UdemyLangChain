{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cfd13b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\otimi\\anaconda3\\envs\\python\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# billing https://platform.openai.com/settings/organization/billing/overview\n",
    "# instalação do ffmpeg no windows: https://www.youtube.com/watch?v=JR36oH35Fgg\n",
    "# https://ffmpeg.org/ffmpeg.html\n",
    "# https://ffmpeg.org/download.html\n",
    "# https://www.gyan.dev/ffmpeg/builds/ffmpeg-git-essentials.7z\n",
    "# no Jupiter Notebook ffmpeg -i apresentacao.mp3 teste.avi\n",
    "\n",
    "import gradio as gr\n",
    "import whisper\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path='.env')\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=openai_api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc397a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install whisper\n",
    "# pip install gradio\n",
    "# pip install openai-whisper\n",
    "# pip install playsound\n",
    "# pip install pygame\n",
    "# pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a0b50e9-e91a-4948-83ab-187021044c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Curso completo de Langtian, com várias ferramentas úteis para seus projetos.\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"base\")\n",
    "result = model.transcribe(\"apresentacao.mp3\", fp16=False)\n",
    "print(result[\"text\"])\n",
    "\n",
    "# https://www.youtube.com/watch?v=UAdX0cGuC28\n",
    "# Transcribe Audio Files with OpenAI Whisper\n",
    "\n",
    "with open(\"transcription.txt\",\"w\") as f:\n",
    "\tf.write(result[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59af5cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from playsound import playsound\n",
    "\n",
    "filename = \"apresentacao.mp3\"\n",
    "filename = os.path.abspath(filename)\n",
    "playsound(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d121ebf-7115-4469-b4c7-e00264e1aab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.4)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import time\n",
    "\n",
    "# Initialize pygame mixer\n",
    "pygame.mixer.init()\n",
    "\n",
    "# Load and play the sound\n",
    "filename = \"C:\\\\Users\\\\otimi\\\\Udemy\\\\apresentacao.mp3\"\n",
    "filename = \"apresentacao.mp3\"\n",
    "pygame.mixer.music.load(filename)\n",
    "pygame.mixer.music.play()\n",
    "\n",
    "# Wait for the audio to finish\n",
    "while pygame.mixer.music.get_busy():\n",
    "    time.sleep(1)\n",
    "\n",
    "# Stop the audio playback\n",
    "pygame.mixer.music.stop()\n",
    "\n",
    "# Clean up: Quit the pygame mixer and remove the temporary audio file\n",
    "pygame.mixer.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf46764f-05a0-4885-9fa9-219c76fdfd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from playsound import playsound\n",
    "\n",
    "# Convert MP3 to WAV\n",
    "filename = \"apresentacao.mp3\"\n",
    "filename = os.path.abspath(filename)\n",
    "sound = AudioSegment.from_mp3(filename)\n",
    "wav_filename = filename.replace('.mp3', '.wav')\n",
    "sound.export(wav_filename, format=\"wav\")\n",
    "\n",
    "# Play the WAV file. Erro\n",
    "playsound(filename) # ao executar em seguida dá erro....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c58b4eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos no diretório: ['apresentacao.mp3']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Especifica o caminho do diretório\n",
    "directory_path = './'\n",
    "\n",
    "# Lista todos os arquivos e diretórios\n",
    "files_and_directories = os.listdir(directory_path)\n",
    "\n",
    "# Filtra apenas os arquivos\n",
    "files = [f for f in files_and_directories if os.path.isfile(os.path.join(directory_path, f))]\n",
    "mp3_files = [f for f in files_and_directories \n",
    "             if os.path.isfile(os.path.join(directory_path, f)) and f.lower().endswith('.mp3')]\n",
    "\n",
    "print(\"Arquivos no diretório:\", mp3_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f37a7fab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\otimi\\anaconda3\\envs\\python\\Lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 406, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\otimi\\anaconda3\\envs\\python\\Lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 70, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\otimi\\anaconda3\\envs\\python\\Lib\\site-packages\\fastapi\\applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"C:\\Users\\otimi\\anaconda3\\envs\\python\\Lib\\site-packages\\starlette\\applications.py\", line 113, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\otimi\\anaconda3\\envs\\python\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 187, in __call__\n",
      "    raise exc\n",
      "  File \"C:\\Users\\otimi\\anaconda3\\envs\\python\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 165, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"C:\\Users\\otimi\\anaconda3\\envs\\python\\Lib\\site-packages\\gradio\\route_utils.py\", line 790, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\otimi\\anaconda3\\envs\\python\\Lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"C:\\Users\\otimi\\anaconda3\\envs\\python\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\otimi\\anaconda3\\envs\\python\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\otimi\\anaconda3\\envs\\python\\Lib\\site-packages\\starlette\\routing.py\", line 715, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\otimi\\anaconda3\\envs\\python\\Lib\\site-packages\\starlette\\routing.py\", line 735, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"C:\\Users\\otimi\\anaconda3\\envs\\python\\Lib\\site-packages\\starlette\\routing.py\", line 288, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\otimi\\anaconda3\\envs\\python\\Lib\\site-packages\\starlette\\routing.py\", line 76, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"C:\\Users\\otimi\\anaconda3\\envs\\python\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\otimi\\anaconda3\\envs\\python\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\otimi\\anaconda3\\envs\\python\\Lib\\site-packages\\starlette\\routing.py\", line 74, in app\n",
      "    await response(scope, receive, send)\n",
      "  File \"C:\\Users\\otimi\\anaconda3\\envs\\python\\Lib\\site-packages\\starlette\\responses.py\", line 348, in __call__\n",
      "    await self._handle_simple(send, send_header_only)\n",
      "  File \"C:\\Users\\otimi\\anaconda3\\envs\\python\\Lib\\site-packages\\starlette\\responses.py\", line 377, in _handle_simple\n",
      "    await send({\"type\": \"http.response.body\", \"body\": chunk, \"more_body\": more_body})\n",
      "  File \"C:\\Users\\otimi\\anaconda3\\envs\\python\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 39, in sender\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\otimi\\anaconda3\\envs\\python\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 39, in sender\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\otimi\\anaconda3\\envs\\python\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 162, in _send\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\otimi\\anaconda3\\envs\\python\\Lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 510, in send\n",
      "    output = self.conn.send(event=h11.EndOfMessage())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\otimi\\anaconda3\\envs\\python\\Lib\\site-packages\\h11\\_connection.py\", line 512, in send\n",
      "    data_list = self.send_with_data_passthrough(event)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\otimi\\anaconda3\\envs\\python\\Lib\\site-packages\\h11\\_connection.py\", line 545, in send_with_data_passthrough\n",
      "    writer(event, data_list.append)\n",
      "  File \"C:\\Users\\otimi\\anaconda3\\envs\\python\\Lib\\site-packages\\h11\\_writers.py\", line 67, in __call__\n",
      "    self.send_eom(event.headers, write)\n",
      "  File \"C:\\Users\\otimi\\anaconda3\\envs\\python\\Lib\\site-packages\\h11\\_writers.py\", line 96, in send_eom\n",
      "    raise LocalProtocolError(\"Too little data for declared Content-Length\")\n",
      "h11._util.LocalProtocolError: Too little data for declared Content-Length\n",
      "C:\\Users\\otimi\\anaconda3\\envs\\python\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "Exception in callback _ProactorBasePipeTransport._call_connection_lost(None)\n",
      "handle: <Handle _ProactorBasePipeTransport._call_connection_lost(None)>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\otimi\\anaconda3\\envs\\python\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\otimi\\anaconda3\\envs\\python\\Lib\\asyncio\\proactor_events.py\", line 165, in _call_connection_lost\n",
      "    self._sock.shutdown(socket.SHUT_RDWR)\n",
      "ConnectionResetError: [WinError 10054] Foi forçado o cancelamento de uma conexão existente pelo host remoto\n"
     ]
    }
   ],
   "source": [
    "starting_prompt = \"\"\"Você é um assistente.\n",
    "Você pode discutir com o usuário.\n",
    "Você receberá instruções começando com [Instruction] ou entrada do usuário começando com [User]. Siga as instruções.\n",
    "\"\"\"\n",
    "\n",
    "prompts = {\n",
    "'START': '[Instrução] Escreva WRITE_EMAIL se o usuário quiser escrever um e-mail, \"QUESTION\" se o usuário tiver uma pergunta precisa, \"OTHER\" em qualquer outro caso. Escreva apenas uma palavra.',\n",
    "'QUESTION': '[Instrução] Se você puder responder à pergunta, escreva \"ANSWER\", se precisar de mais informações, escreva MORE, se não puder responder, escreva \"OTHER\". Escreva apenas uma palavra.',\n",
    "'ANSWER': '[Instrução] Responda à pergunta do usuário',\n",
    "'MORE': '[Instrução] Peça mais informações ao usuário, conforme especificado nas instruções anteriores',\n",
    "'OTHER': '[Instrução] Dê uma resposta educada ou saudações se o usuário estiver conversando educadamente. Caso contrário, responda ao usuário que você não pode responder à pergunta ou realizar a ação',\n",
    "}\n",
    "\n",
    "class Discussion:\n",
    "    \"\"\"\n",
    "    A class representing a discussion with a voice assistant.\n",
    "\n",
    "    Attributes:\n",
    "        state (str): The current state of the discussion.\n",
    "        messages_history (list): A list of dictionaries representing the history of messages in the discussion.\n",
    "        client: An instance of the OpenAI client.\n",
    "        stt_model: The speech-to-text model used for transcribing audio.\n",
    "\n",
    "    Methods:\n",
    "        generate_answer: Generates an answer based on the given messages.\n",
    "        reset: Resets the discussion to the initial state.\n",
    "        do_action: Performs the specified action.\n",
    "        transcribe: Transcribes the given audio file.\n",
    "        discuss_from_audio: Starts a discussion based on the transcribed audio file.\n",
    "        discuss: Continues the discussion based on the given input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state='START', messages_history=None) -> None:\n",
    "        if messages_history is None:\n",
    "            messages_history = [{'role': 'user', 'content': f'{starting_prompt}'}]\n",
    "        self.state = state\n",
    "        self.messages_history = messages_history\n",
    "        self.client = OpenAI()\n",
    "        self.stt_model = whisper.load_model(\"base\")\n",
    "\n",
    "    def generate_answer(self, messages):\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    def reset(self, start_state='START'):\n",
    "        self.messages_history = [{'role': 'user', 'content': f'{starting_prompt}'}]\n",
    "        self.state = start_state\n",
    "        self.previous_state = None\n",
    "\n",
    "    def reset_to_previous_state(self):\n",
    "        self.state = self.previous_state\n",
    "        self.previous_state = None\n",
    "\n",
    "    def to_state(self, state):\n",
    "        self.previous_state = self.state\n",
    "        self.state = state\n",
    "\n",
    "    def do_action(self, action):\n",
    "        \"\"\"\n",
    "        Performs the specified action.\n",
    "\n",
    "        Args:\n",
    "            action (str): The action to perform.\n",
    "        \"\"\"\n",
    "        print(f'DEBUG perform action={action}')\n",
    "        pass\n",
    "\n",
    "    def transcribe(self, file):\n",
    "        transcription = self.stt_model.transcribe(file)\n",
    "        return transcription['text']\n",
    "\n",
    "    def discuss_from_audio(self, file):\n",
    "        if file:\n",
    "            # Transcribe the audio file and use the input to start the discussion\n",
    "            return self.discuss(f'[User] {self.transcribe(file)}')\n",
    "        # Empty output if there is no file\n",
    "        return ''\n",
    "\n",
    "    def discuss(self, input=None):\n",
    "        if input is not None:\n",
    "            self.messages_history.append({\"role\": \"user\", \"content\": input})\n",
    "\n",
    "        # Generate a completion\n",
    "        completion = self.generate_answer(\n",
    "            self.messages_history +\n",
    "            [{\"role\": \"user\", \"content\": prompts[self.state]}]\n",
    "        )\n",
    "\n",
    "        # Is the completion an action?\n",
    "        actions = [\"action1\", \"action2\"]\n",
    "        if completion.split(\"|\")[0].strip() in actions:\n",
    "            action = completion.split(\"|\")[0].strip()\n",
    "            self.to_state(action)\n",
    "            self.do_action(completion)\n",
    "            # Continue discussion\n",
    "            return self.discuss()\n",
    "        # Is the completion a new state?\n",
    "        elif completion in prompts:\n",
    "            self.to_state(completion)\n",
    "            # Continue discussion\n",
    "            return self.discuss()\n",
    "        # Is the completion an output for the user?\n",
    "        else:\n",
    "            self.messages_history.append({\"role\": \"assistant\", \"content\": completion})\n",
    "            if self.state != 'MORE':\n",
    "                # Get back to start\n",
    "                self.reset()\n",
    "            else:\n",
    "                # Get back to previous state\n",
    "                self.reset_to_previous_state()\n",
    "            return completion\n",
    "\n",
    "\n",
    "# Ensure the class is defined before creating an instance\n",
    "discussion = Discussion()\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "gr.Interface(\n",
    "    theme=gr.themes.Soft(),\n",
    "    fn=discussion.discuss_from_audio,\n",
    "    live=True,\n",
    "    inputs=gr.Audio(sources=\"microphone\", type=\"filepath\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Transcrição de Áudio\",\n",
    "    description=\"Grave um áudio pelo microfone e obtenha a transcrição.\"\n",
    ").launch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c60c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use command line instead of Gradio, remove above code and use this instead:  Qual  acapital do Brasil ?\n",
    "#while True:\n",
    "message = input('User: ')\n",
    "print(f'Assistant: {discussion.discuss(message)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "602ba57e-d174-44b8-9ad0-774bb2c636fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Função que retorna o caminho do arquivo de áudio\n",
    "def play_audio():\n",
    "    return \"./apresentacao.mp3\"\n",
    "\n",
    "# Criação da interface Gradio\n",
    "interface = gr.Interface(\n",
    "    fn=play_audio,\n",
    "    inputs=[],\n",
    "    outputs=gr.Audio(type=\"filepath\", label=\"Tocar Áudio\"),\n",
    "    title=\"Tocar Áudio MP3\",\n",
    "    description=\"Clique no botão Generate para tocar o áudio.\"\n",
    ")\n",
    "\n",
    "# Inicia a interface\n",
    "interface.launch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c885248-5d63-4f23-8f4f-490d8941ad1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\otimi\\anaconda3\\envs\\python\\Lib\\site-packages\\whisper\\__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\otimi\\anaconda3\\envs\\python\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import whisper\n",
    "\n",
    "# Carregar o modelo de transcrição do Whisper\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# Função que retorna o caminho do arquivo de áudio e sua transcrição\n",
    "def play_and_transcribe_audio():\n",
    "    audio_path = \"./apresentacao.mp3\"\n",
    "    transcription = model.transcribe(audio_path)\n",
    "    return audio_path, transcription[\"text\"]\n",
    "\n",
    "# Criação da interface Gradio\n",
    "interface = gr.Interface(\n",
    "    fn=play_and_transcribe_audio,\n",
    "    inputs=[],\n",
    "    outputs=[gr.Audio(type=\"filepath\", label=\"Tocar Áudio\"), gr.Textbox(label=\"Transcrição\")],\n",
    "    title=\"Tocar Áudio MP3 e Transcrição\",\n",
    "    description=\"Clique no botão Generate para tocar o áudio e ver a transcrição.\"\n",
    ")\n",
    "\n",
    "# Inicia a interface\n",
    "interface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a67cd3-e419-4b6a-9957-af8e20795423",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
